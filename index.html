<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                SRarxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-28T00:00:00Z">2024-06-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Image and Video Processing <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Malaria Cell Detection Using Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Sawant, Anurag Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malaria remains one of the most pressing public health concerns globally,
causing significant morbidity and mortality, especially in sub-Saharan Africa.
Rapid and accurate diagnosis is crucial for effective treatment and disease
management. Traditional diagnostic methods, such as microscopic examination of
blood smears, are labor-intensive and require significant expertise, which may
not be readily available in resource-limited settings. This project aims to
automate the detection of malaria-infected cells using a deep learning
approach. We employed a convolutional neural network (CNN) based on the
ResNet50 architecture, leveraging transfer learning to enhance performance. The
Malaria Cell Images Dataset from Kaggle, containing 27,558 images categorized
into infected and uninfected cells, was used for training and evaluation. Our
model demonstrated high accuracy, precision, and recall, indicating its
potential as a reliable tool for assisting in malaria diagnosis. Additionally,
a web application was developed using Streamlit to allow users to upload cell
images and receive predictions about malaria infection, making the technology
accessible and user-friendly. This paper provides a comprehensive overview of
the methodology, experiments, and results, highlighting the effectiveness of
deep learning in medical image analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Initialization on Intra-subject Pediatric Brain MR Image
  Registration: A Comparative Analysis between SyN ANTs and Deep Learning-Based
  Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andjela Dimitrijevic, Vincent Noblet, Benjamin De Leener
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study evaluates the performance of conventional SyN ANTs and
learning-based registration methods in the context of pediatric neuroimaging,
specifically focusing on intrasubject deformable registration. The comparison
involves three approaches: without (NR), with rigid (RR), and with rigid and
affine (RAR) initializations. In addition to initialization, performances are
evaluated in terms of accuracy, speed, and the impact of age intervals and sex
per pair. Data consists of the publicly available MRI scans from the Calgary
Preschool dataset, which includes 63 children aged 2-7 years, allowing for 431
registration pairs. We implemented the unsupervised DL framework with a U-Net
architecture using DeepReg and it was 5-fold cross-validated. Evaluation
includes Dice scores for tissue segmentation from 18 smaller regions obtained
by SynthSeg, analysis of log Jacobian determinants, and registration pro-rated
training and inference times. Learning-based approaches, with or without linear
initializations, exhibit slight superiority over SyN ANTs in terms of Dice
scores. Indeed, DL-based implementations with RR and RAR initializations
significantly outperform SyN ANTs. Both SyN ANTs and DL-based registration
involve parameter optimization, but the choice between these methods depends on
the scale of registration: network-based for broader coverage or SyN ANTs for
specific structures. Both methods face challenges with larger age intervals due
to greater growth changes. The main takeaway is that while DL-based methods
show promise with faster and more accurate registrations, SyN ANTs remains
robust and generalizable without the need for extensive training, highlighting
the importance of method selection based on specific registration needs in the
pediatric context. Our code is available at
https://github.com/neuropoly/pediatric-DL-registration
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024:013</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Unfolding-Aided Parameter Tuning for Plug-and-Play Based Video
  Snapshot Compressive Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Matsuda, Ryo Hayakawa, Youji Iiguni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Snapshot compressive imaging (SCI) captures high-dimensional data efficiently
by compressing it into two-dimensional observations and reconstructing
high-dimensional data from two-dimensional observations with various
algorithms. Plug-and-play (PnP) is a promising approach for the video SCI
reconstruction because it can leverage both the observation model and denoising
methods for videos. This paper proposes a deep unfolding-based method for
tuning noise level parameters in PnP-based video SCI, which significantly
affects the reconstruction accuracy. For the training of the parameters, we
prepare training data from the densely annotated video segmentation (DAVIS)
dataset, reparametrize the noise level parameters, and apply the checkpointing
technique to reduce the required memory. Simulation results show that the
trained noise level parameters significantly improve the reconstruction
accuracy and exhibit a non-monotonic pattern, which is different from the
assumptions in the conventional convergence analyses of PnP-based algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work will be submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Generative Replay for Task-Incremental Segmentation with
  Concurrent Appearance and Semantic Forgetting <span class="chip">MICCAI24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Li, Jingyang Zhang, Pheng-Ann Heng, Lixu Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalist segmentation models are increasingly favored for diverse tasks
involving various objects from different image sources. Task-Incremental
Learning (TIL) offers a privacy-preserving training paradigm using tasks
arriving sequentially, instead of gathering them due to strict data sharing
policies. However, the task evolution can span a wide scope that involves
shifts in both image appearance and segmentation semantics with intricate
correlation, causing concurrent appearance and semantic forgetting. To solve
this issue, we propose a Comprehensive Generative Replay (CGR) framework that
restores appearance and semantic knowledge by synthesizing image-mask pairs to
mimic past task data, which focuses on two aspects: modeling image-mask
correspondence and promoting scalability for diverse tasks. Specifically, we
introduce a novel Bayesian Joint Diffusion (BJD) model for high-quality
synthesis of image-mask pairs with their correspondence explicitly preserved by
conditional denoising. Furthermore, we develop a Task-Oriented Adapter (TOA)
that recalibrates prompt embeddings to modulate the diffusion model, making the
data synthesis compatible with different tasks. Experiments on incremental
tasks (cardiac, fundus and prostate segmentation) show its clear advantage for
alleviating concurrent appearance and semantic forgetting. Code is available at
https://github.com/jingyzhang/CGR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MICCAI24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPIRONet: Spatial-Frequency Learning and Topological Channel Interaction
  Network for Vessel Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        De-Xing Huang, Xiao-Hu Zhou, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Zhen-Qiu Feng, Mei-Jiang Gui, Hao Li, Tian-Yu Xiang, Bo-Xian Yao, Zeng-Guang Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic vessel segmentation is paramount for developing next-generation
interventional navigation systems. However, current approaches suffer from
suboptimal segmentation performances due to significant challenges in
intraoperative images (i.e., low signal-to-noise ratio, small or slender
vessels, and strong interference). In this paper, a novel spatial-frequency
learning and topological channel interaction network (SPIRONet) is proposed to
address the above issues. Specifically, dual encoders are utilized to
comprehensively capture local spatial and global frequency vessel features.
Then, a cross-attention fusion module is introduced to effectively fuse spatial
and frequency features, thereby enhancing feature discriminability.
Furthermore, a topological channel interaction module is designed to filter out
task-irrelevant responses based on graph neural networks. Extensive
experimental results on several challenging datasets (CADSA, CAXF, DCA1, and
XCAD) demonstrate state-of-the-art performances of our method. Moreover, the
inference speed of SPIRONet is 21 FPS with a 512x512 input size, surpassing
clinical real-time requirements (6~12FPS). These promising outcomes indicate
SPIRONet's potential for integration into vascular interventional navigation
systems. Code is available at https://github.com/Dxhuang-CASIA/SPIRONet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Radiological Diagnosis: A Collaborative Approach Integrating
  AI and Human Expertise for Visual Miss Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Awasthi, Ngan Le, Zhigang Deng, Carol C. Wu, Hien Van Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-AI collaboration to identify and correct perceptual errors in chest
radiographs has not been previously explored. This study aimed to develop a
collaborative AI system, CoRaX, which integrates eye gaze data and radiology
reports to enhance diagnostic accuracy in chest radiology by pinpointing
perceptual errors and refining the decision-making process. Using public
datasets REFLACX and EGD-CXR, the study retrospectively developed CoRaX,
employing a large multimodal model to analyze image embeddings, eye gaze data,
and radiology reports. The system's effectiveness was evaluated based on its
referral-making process, the quality of referrals, and performance in
collaborative diagnostic settings. CoRaX was tested on a simulated error
dataset of 271 samples with 28% (93 of 332) missed abnormalities. The system
corrected 21% (71 of 332) of these errors, leaving 7% (22 of 312) unresolved.
The Referral-Usefulness score, indicating the accuracy of predicted regions for
all true referrals, was 0.63 (95% CI 0.59, 0.68). The Total-Usefulness score,
reflecting the diagnostic accuracy of CoRaX's interactions with radiologists,
showed that 84% (237 of 280) of these interactions had a score above 0.40. In
conclusion, CoRaX efficiently collaborates with radiologists to address
perceptual errors across various abnormalities, with potential applications in
the education and training of novice radiologists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review in Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSAKD: Knowledge <span class="highlight-title">Distillation</span> with Cross Self-<span class="highlight-title">Attention</span> for
  Hyperspectral and Multispectral Image Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Chung Hsu, Chih-Chien Ni, Chia-Ming Lee, Li-Wei Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral imaging, capturing detailed spectral information for each
pixel, is pivotal in diverse scientific and industrial applications. Yet, the
acquisition of high-resolution (HR) hyperspectral images (HSIs) often needs to
be addressed due to the hardware limitations of existing imaging systems. A
prevalent workaround involves capturing both a high-resolution multispectral
image (HR-MSI) and a low-resolution (LR) HSI, subsequently fusing them to yield
the desired HR-HSI. Although deep learning-based methods have shown promising
in HR-MSI/LR-HSI fusion and LR-HSI super-resolution (SR), their substantial
model complexities hinder deployment on resource-constrained imaging devices.
This paper introduces a novel knowledge distillation (KD) framework for
HR-MSI/LR-HSI fusion to achieve SR of LR-HSI. Our KD framework integrates the
proposed Cross-Layer Residual Aggregation (CLRA) block to enhance efficiency
for constructing Dual Two-Streamed (DTS) network structure, designed to extract
joint and distinct features from LR-HSI and HR-MSI simultaneously. To fully
exploit the spatial and spectral feature representations of LR-HSI and HR-MSI,
we propose a novel Cross Self-Attention (CSA) fusion module to adaptively fuse
those features to improve the spatial and spectral quality of the reconstructed
HR-HSI. Finally, the proposed KD-based joint loss function is employed to
co-train the teacher and student networks. Our experimental results demonstrate
that the student model not only achieves comparable or superior LR-HSI SR
performance but also significantly reduces the model-size and computational
requirements. This marks a substantial advancement over existing
state-of-the-art methods. The source code is available at
https://github.com/ming053l/CSAKD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AstMatch: Adversarial Self-training Consistency Framework for
  Semi-Supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanghao Zhu, Jing Zhang, Juanxiu Liu, Xiaohui Du, Ruqian Hao, Yong Liu, Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) has shown considerable potential in medical
image segmentation, primarily leveraging consistency regularization and
pseudo-labeling. However, many SSL approaches only pay attention to low-level
consistency and overlook the significance of pseudo-label reliability.
Therefore, in this work, we propose an adversarial self-training consistency
framework (AstMatch). Firstly, we design an adversarial consistency
regularization (ACR) approach to enhance knowledge transfer and strengthen
prediction consistency under varying perturbation intensities. Second, we apply
a feature matching loss for adversarial training to incorporate high-level
consistency regularization. Additionally, we present the pyramid channel
attention (PCA) and efficient channel and spatial attention (ECSA) modules to
improve the discriminator's performance. Finally, we propose an adaptive
self-training (AST) approach to ensure the pseudo-labels' quality. The proposed
AstMatch has been extensively evaluated with cutting-edge SSL methods on three
public-available datasets. The experimental results under different labeled
ratios indicate that AstMatch outperforms other existing methods, achieving new
state-of-the-art performance. Our code will be available at
https://github.com/GuanghaoZhu663/AstMatch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deformable MRI Sequence Registration for AI-based Prostate Cancer
  Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessa Hering, Sarah de Boer, Anindo Saha, Jasper J. Twilt, Mattias P. Heinrich, Derya Yakar, Maarten de Rooij, Henkjan Huisman, Joeran S. Bosma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The PI-CAI (Prostate Imaging: Cancer AI) challenge led to expert-level
diagnostic algorithms for clinically significant prostate cancer detection. The
algorithms receive biparametric MRI scans as input, which consist of
T2-weighted and diffusion-weighted scans. These scans can be misaligned due to
multiple factors in the scanning process. Image registration can alleviate this
issue by predicting the deformation between the sequences. We investigate the
effect of image registration on the diagnostic performance of AI-based prostate
cancer diagnosis. First, the image registration algorithm, developed in
MeVisLab, is analyzed using a dataset with paired lesion annotations. Second,
the effect on diagnosis is evaluated by comparing case-level cancer diagnosis
performance between using the original dataset, rigidly aligned
diffusion-weighted scans, or deformably aligned diffusion-weighted scans. Rigid
registration showed no improvement. Deformable registration demonstrated a
substantial improvement in lesion overlap (+10% median Dice score) and a
positive yet non-significant improvement in diagnostic performance (+0.3%
AUROC, p=0.18). Our investigation shows that a substantial improvement in
lesion alignment does not directly lead to a significant improvement in
diagnostic performance. Qualitative analysis indicated that jointly developing
image registration methods and diagnostic AI algorithms could enhance
diagnostic accuracy and patient outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver
  with a Few Partial Ultrasound Scans <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaushalya Sivayogaraj, Sahan T. Guruge, Udari Liyanage, Jeevani Udupihille, Saroj Jayasinghe, Gerard Fernando, Ranga Rodrigo, M. Rukshani Liyanaarachchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction of the liver for volumetry is important for qualitative
analysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,
although advantageous due to less acquisition time and safety, is challenging
due to the inherent noisiness in US scans, blurry boundaries, and partial liver
visibility. We address these challenges by using the segmentation masks of a
few incomplete sagittal-plane US scans of the liver in conjunction with a
statistical shape model (SSM) built using a set of CT scans of the liver. We
compute the shape parameters needed to warp this canonical SSM to fit the US
scans through a parametric regression network. The resulting 3D liver
reconstruction is accurate and leads to automatic liver volume calculation. We
evaluate the accuracy of the estimated liver volumes with respect to CT
segmentation volumes using RMSE. Our volume computation is statistically much
closer to the volume estimated using CT scans than the volume computed using
Childs' method by radiologists: p-value of 0.094 (>0.05) says that there is no
significant difference between CT segmentation volumes and ours in contrast to
Childs' method. We validate our method using investigations (ablation studies)
on the US image resolution, the number of CT scans used for SSM, the number of
principal components, and the number of input US scans. To the best of our
knowledge, this is the first automatic liver volumetry system using a few
incomplete US scans given a set of CT scans of livers for SSM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, Accepted to MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-domain Denoising for Low-dose Multi-frame Spiral Computed
  Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10839v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10839v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Lu, Zhixin Xu, Moon Hyung Choi, Jimin Kim, Seung-Won Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed tomography (CT) has been used worldwide as a non-invasive test to
assist in diagnosis. However, the ionizing nature of X-ray exposure raises
concerns about potential health risks such as cancer. The desire for lower
radiation doses has driven researchers to improve reconstruction quality.
Although previous studies on low-dose computed tomography (LDCT) denoising have
demonstrated the effectiveness of learning-based methods, most were developed
on the simulated data. However, the real-world scenario differs significantly
from the simulation domain, especially when using the multi-slice spiral
scanner geometry. This paper proposes a two-stage method for the commercially
available multi-slice spiral CT scanners that better exploits the complete
reconstruction pipeline for LDCT denoising across different domains. Our
approach makes good use of the high redundancy of multi-slice projections and
the volumetric reconstructions while leveraging the over-smoothing problem in
conventional cascaded frameworks caused by aggressive denoising. The dedicated
design also provides a more explicit interpretation of the data flow. Extensive
experiments on various datasets showed that the proposed method could remove up
to 70\% of noise without compromised spatial resolution, and subjective
evaluations by two experienced radiologists further supported its superior
performance against state-of-the-art methods in clinical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Knowledge <span class="highlight-title">Distillation</span> for <span class="highlight-title">Lightweight</span> Skin Cancer
  Classification: Balancing Accuracy and Computational Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niful Islam, Khan Md Hasib, Fahmida Akter Joti, Asif Karim, Sami Azam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer is a major concern to public health, accounting for one-third of
the reported cancers. If not detected early, the cancer has the potential for
severe consequences. Recognizing the critical need for effective skin cancer
classification, we address the limitations of existing models, which are often
too large to deploy in areas with limited computational resources. In response,
we present a knowledge distillation based approach for creating a lightweight
yet high-performing classifier. The proposed solution involves fusing three
models, namely ResNet152V2, ConvNeXtBase, and ViT Base, to create an effective
teacher model. The teacher model is then employed to guide a lightweight
student model of size 2.03 MB. This student model is further compressed to
469.77 KB using 16-bit quantization, enabling smooth incorporation into edge
devices. With six-stage image preprocessing, data augmentation, and a rigorous
ablation study, the model achieves an impressive accuracy of 98.75% on the
HAM10000 dataset and 98.94% on the Kaggle dataset in classifying benign and
malignant skin cancers. With its high accuracy and compact size, our model
appears to be a potential choice for accurate skin cancer classification,
particularly in resource-constrained settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Epicardium Prompt-guided Real-time Cardiac Ultrasound Frame-to-volume
  Registration <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Lei, Jun Zhou, Jialun Pei, Baoliang Zhao, Yueming Jin, Yuen-Chun Jeremy Teoh, Jing Qin, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A comprehensive guidance view for cardiac interventional surgery can be
provided by the real-time fusion of the intraoperative 2D images and
preoperative 3D volume based on the ultrasound frame-to-volume registration.
However, cardiac ultrasound images are characterized by a low signal-to-noise
ratio and small differences between adjacent frames, coupled with significant
dimension variations between 2D frames and 3D volumes to be registered,
resulting in real-time and accurate cardiac ultrasound frame-to-volume
registration being a very challenging task. This paper introduces a lightweight
end-to-end Cardiac Ultrasound frame-to-volume Registration network, termed
CU-Reg. Specifically, the proposed model leverages epicardium prompt-guided
anatomical clues to reinforce the interaction of 2D sparse and 3D dense
features, followed by a voxel-wise local-global aggregation of enhanced
features, thereby boosting the cross-dimensional matching effectiveness of
low-quality ultrasound modalities. We further embed an inter-frame
discriminative regularization term within the hybrid supervised learning to
increase the distinction between adjacent slices in the same ultrasound volume
to ensure registration stability. Experimental results on the reprocessed CAMUS
dataset demonstrate that our CU-Reg surpasses existing methods in terms of
registration accuracy and efficiency, meeting the guidance requirements of
clinical cardiac interventional surgery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">90</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Odd-One-Out: Anomaly Detection by Comparing with Neighbors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankan Bhunia, Changjian Li, Hakan Bilen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel anomaly detection (AD) problem that focuses on
identifying `odd-looking' objects relative to the other instances within a
scene. Unlike the traditional AD benchmarks, in our setting, anomalies in this
context are scene-specific, defined by the regular instances that make up the
majority. Since object instances are often partly visible from a single
viewpoint, our setting provides multiple views of each scene as input. To
provide a testbed for future research in this task, we introduce two
benchmarks, ToysAD-8K and PartsAD-15K. We propose a novel method that generates
3D object-centric representations for each instance and detects the anomalous
ones through a cross-examination between the instances. We rigorously analyze
our method quantitatively and qualitatively in the presented benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes & Dataset at https://github.com/VICO-UoE/OddOneOutAD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework
  for Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukmin Yun, Haokun Lin, Rusiru Thushara, Mohammad Qazim Bhat, Yongxin Wang, Zutao Jiang, Mingkai Deng, Jinhong Wang, Tianhua Tao, Junbo Li, Haonan Li, Preslav Nakov, Timothy Baldwin, Zhengzhong Liu, Eric P. Xing, Xiaodan Liang, Zhiqiang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have shown impressive success across
modalities such as image, video, and audio in a variety of understanding and
generation tasks. However, current MLLMs are surprisingly poor at understanding
webpage screenshots and generating their corresponding HTML code. To address
this problem, we propose Web2Code, a benchmark consisting of a new large-scale
webpage-to-code dataset for instruction tuning and an evaluation framework for
the webpage understanding and HTML code translation abilities of MLLMs. For
dataset construction, we leverage pretrained LLMs to enhance existing
webpage-to-code datasets as well as generate a diverse pool of new webpages
rendered into images. Specifically, the inputs are webpage images and
instructions, while the responses are the webpage's HTML code. We further
include diverse natural language QA pairs about the webpage content in the
responses to enable a more comprehensive understanding of the web content. To
evaluate model performance in these tasks, we develop an evaluation framework
for testing MLLMs' abilities in webpage understanding and web-to-code
generation. Extensive experiments show that our proposed dataset is beneficial
not only to our proposed tasks but also in the general visual domain, while
previous datasets result in worse performance. We hope our work will contribute
to the development of general MLLMs suitable for web-based content generation
and task automation. Our data and code will be available at
https://github.com/MBZUAI-LLM/web2code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website at https://mbzuai-llm.github.io/webpage2code/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaRA: Supercharging Robot Learning Data for Vision-Language Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) equipped with extensive world knowledge and
strong reasoning skills can tackle diverse tasks across domains, often by
posing them as conversation-style instruction-response pairs. In this paper, we
propose LLaRA: Large Language and Robotics Assistant, a framework which
formulates robot action policy as conversations, and provides improved
responses when trained with auxiliary data that complements policy learning.
LLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity
to process state information as visual-textual prompts and generate optimal
policy decisions in text. To train such action policy VLMs, we first introduce
an automated pipeline to generate diverse high-quality robotics instruction
data from existing behavior cloning data. A VLM finetuned with the resulting
collection of datasets based on a conversation-style formulation tailored for
robotics tasks, can generate meaningful robot action policy decisions. Our
experiments across multiple simulated and real-world environments demonstrate
the state-of-the-art performance of the proposed LLaRA framework. The code,
datasets, and pretrained models are available at
https://github.com/LostXine/LLaRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVolta: <span class="highlight-title">Efficient</span> Multi-modal Models via Stage-wise Visual Context
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieneng Chen, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, Alan Yuille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While significant advancements have been made in compressed representations
for text embeddings in large language models (LLMs), the compression of visual
tokens in large multi-modal models (LMMs) has remained a largely overlooked
area. In this work, we present the study on the analysis of redundancy
concerning visual tokens and efficient training within these models. Our
initial experiments show that eliminating up to 70% of visual tokens at the
testing stage by simply average pooling only leads to a minimal 3% reduction in
visual question answering accuracy on the GQA benchmark, indicating significant
redundancy in visual context. Addressing this, we introduce Visual Context
Compressor, which reduces the number of visual tokens during training to
enhance training efficiency without sacrificing performance. To minimize
information loss caused by the compression on visual tokens while maintaining
training efficiency, we develop LLaVolta as a lite training scheme. LLaVolta
incorporates stage-wise visual context compression to progressively compress
the visual tokens from heavily to lightly, and finally no compression at the
end of training, yielding no loss of information when testing. Extensive
experiments demonstrate that our approach enhances the performance of MLLMs in
both image-language and video-language understanding, while also significantly
cutting training costs. Code is available at
https://github.com/Beckschen/LLaVolta
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/Beckschen/LLaVolta</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auto Cherry-Picker: Learning from High-quality Generative Data Driven by
  Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Chen, Xiangtai Li, Yining Li, Yanhong Zeng, Jianzong Wu, Xiangyu Zhao, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based models have shown great potential in generating high-quality
images with various layouts, which can benefit downstream perception tasks.
However, a fully automatic layout generation driven only by language and a
suitable metric for measuring multiple generated instances has not been well
explored. In this work, we present Auto Cherry-Picker (ACP), a novel framework
that generates high-quality multi-modal training examples to augment perception
and multi-modal training. Starting with a simple list of natural language
concepts, we prompt large language models (LLMs) to generate a detailed
description and design reasonable layouts. Next, we use an off-the-shelf
text-to-image model to generate multiple images. Then, the generated data are
refined using a comprehensively designed metric to ensure quality. In
particular, we present a new metric, Composite Layout and Image Score (CLIS),
to evaluate the generated images fairly. Our synthetic high-quality examples
boost performance in various scenarios by customizing the initial concept list,
especially in addressing challenges associated with long-tailed distribution
and imbalanced datasets. Experiment results on downstream tasks demonstrate
that Auto Cherry-Picker can significantly improve the performance of existing
models. In addition, we have thoroughly investigated the correlation between
CLIS and performance gains in downstream tasks, and we find that a better CLIS
score results in better performance. This finding shows the potential for
evaluation metrics as the role for various visual perception and MLLM tasks.
Code will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoliFormer: Scaling On-Policy RL with <span class="highlight-title">Transformer</span>s Results in Masterful
  Navigators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuo-Hao Zeng, Zichen Zhang, Kiana Ehsani, Rose Hendrix, Jordi Salvador, Alvaro Herrasti, Ross Girshick, Aniruddha Kembhavi, Luca Weihs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PoliFormer (Policy Transformer), an RGB-only indoor navigation
agent trained end-to-end with reinforcement learning at scale that generalizes
to the real-world without adaptation despite being trained purely in
simulation. PoliFormer uses a foundational vision transformer encoder with a
causal transformer decoder enabling long-term memory and reasoning. It is
trained for hundreds of millions of interactions across diverse environments,
leveraging parallelized, multi-machine rollouts for efficient training with
high throughput. PoliFormer is a masterful navigator, producing
state-of-the-art results across two distinct embodiments, the LoCoBot and
Stretch RE-1 robots, and four navigation benchmarks. It breaks through the
plateaus of previous work, achieving an unprecedented 85.5% success rate in
object goal navigation on the CHORES-S benchmark, a 28.5% absolute improvement.
PoliFormer can also be trivially extended to a variety of downstream
applications such as object tracking, multi-object navigation, and
open-vocabulary navigation with no finetuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segment Anything without Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        XuDong Wang, Jingfeng Yang, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segmentation Anything Model (SAM) requires labor-intensive data labeling.
We present Unsupervised SAM (UnSAM) for promptable and automatic whole-image
segmentation that does not require human annotations. UnSAM utilizes a
divide-and-conquer strategy to "discover" the hierarchical structure of visual
scenes. We first leverage top-down clustering methods to partition an unlabeled
image into instance/semantic level segments. For all pixels within a segment, a
bottom-up clustering method is employed to iteratively merge them into larger
groups, thereby forming a hierarchical structure. These unsupervised
multi-granular masks are then utilized to supervise model training. Evaluated
across seven popular datasets, UnSAM achieves competitive results with the
supervised counterpart SAM, and surpasses the previous state-of-the-art in
unsupervised segmentation by 11% in terms of AR. Moreover, we show that
supervised SAM can also benefit from our self-supervised labels. By integrating
our unsupervised pseudo masks into SA-1B's ground-truth masks and training
UnSAM with only 1% of SA-1B, a lightly semi-supervised UnSAM can often segment
entities overlooked by supervised SAM, exceeding SAM's AR by over 6.7% and AP
by 3.9% on SA-1B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/frank-xwang/UnSAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GM-DF: Generalized Multi-Scenario Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxin Lai, Zitong Yu, Jing Yang, Bin Li, Xiangui Kang, Linlin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing face forgery detection usually follows the paradigm of training
models in a single domain, which leads to limited generalization capacity when
unseen scenarios and unknown attacks occur. In this paper, we elaborately
investigate the generalization capacity of deepfake detection models when
jointly trained on multiple face forgery detection datasets. We first find a
rapid degradation of detection accuracy when models are directly trained on
combined datasets due to the discrepancy across collection scenarios and
generation methods. To address the above issue, a Generalized Multi-Scenario
Deepfake Detection framework (GM-DF) is proposed to serve multiple real-world
scenarios by a unified model. First, we propose a hybrid expert modeling
approach for domain-specific real/forgery feature extraction. Besides, as for
the commonality representation, we use CLIP to extract the common features for
better aligning visual and textual features across domains. Meanwhile, we
introduce a masked image reconstruction mechanism to force models to capture
rich forged details. Finally, we supervise the models via a domain-aware
meta-learning strategy to further enhance their generalization capacities.
Specifically, we design a novel domain alignment loss to strongly align the
distributions of the meta-test domains and meta-train domains. Thus, the
updated models are able to represent both specific and common real/forgery
features across multiple datasets. In consideration of the lack of study of
multi-dataset training, we establish a new benchmark leveraging multi-source
data to fairly evaluate the models' generalization capacity on unseen
scenarios. Both qualitative and quantitative experiments on five datasets
conducted on traditional protocols as well as the proposed benchmark
demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HouseCrafter: Lifting Floorplans to 3D Scenes with 2D <span class="highlight-title">Diffusion</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hieu T. Nguyen, Yiwen Chen, Vikram Voleti, Varun Jampani, Huaizu Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HouseCrafter, a novel approach that can lift a floorplan into a
complete large 3D indoor scene (e.g., a house). Our key insight is to adapt a
2D diffusion model, which is trained on web-scale images, to generate
consistent multi-view color (RGB) and depth (D) images across different
locations of the scene. Specifically, the RGB-D images are generated
autoregressively in a batch-wise manner along sampled locations based on the
floorplan, where previously generated images are used as condition to the
diffusion model to produce images at nearby locations. The global floorplan and
attention design in the diffusion model ensures the consistency of the
generated images, from which a 3D scene can be reconstructed. Through extensive
evaluation on the 3D-Front dataset, we demonstrate that HouseCraft can generate
high-quality house-scale 3D scenes. Ablation studies also validate the
effectiveness of different design choices. We will release our code and model
weights. Project page: https://neu-vi.github.io/houseCrafter/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang, Tianheng Cheng, Rui Hu, ei Liu, Heng Liu, Longjin Ran, Xiaoxin Chen, Wenyu Liu, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segment Anything Model (SAM) has attracted widespread attention for its
superior interactive segmentation capabilities with visual prompts while
lacking further exploration of text prompts. In this paper, we empirically
investigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting
SAM for referring expression segmentation and introduce the Early
Vision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective
referring segmentation method which exploits multimodal prompts (i.e., image
and text) and comprises a pre-trained vision-language model to generate
referring prompts and a SAM model for segmentation. Surprisingly, we observe
that: (1) multimodal prompts and (2) vision-language models with early fusion
(e.g., BEIT-3) are beneficial for prompting SAM for accurate referring
segmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3
can obtain state-of-the-art performance on RefCOCO/+/g for referring expression
segmentation and demonstrate the superiority of prompting SAM with early
vision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters
achieves remarkably higher performance while reducing nearly 82% of parameters
compared to previous SAM methods based on large multimodal models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASSR-NeRF: Arbitrary-Scale <span class="highlight-title">Super-Resolution</span> on Voxel Grid for
  High-Quality Radiance Fields Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ding-Jiun Huang, Zi-Ting Chou, Yu-Chiang Frank Wang, Cheng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  NeRF-based methods reconstruct 3D scenes by building a radiance field with
implicit or explicit representations. While NeRF-based methods can perform
novel view synthesis (NVS) at arbitrary scale, the performance in
high-resolution novel view synthesis (HRNVS) with low-resolution (LR)
optimization often results in oversmoothing. On the other hand, single-image
super-resolution (SR) aims to enhance LR images to HR counterparts but lacks
multi-view consistency. To address these challenges, we propose Arbitrary-Scale
Super-Resolution NeRF (ASSR-NeRF), a novel framework for super-resolution novel
view synthesis (SRNVS). We propose an attention-based VoxelGridSR model to
directly perform 3D super-resolution (SR) on the optimized volume. Our model is
trained on diverse scenes to ensure generalizability. For unseen scenes trained
with LR views, we then can directly apply our VoxelGridSR to further refine the
volume and achieve multi-view consistent SR. We demonstrate quantitative and
qualitatively that the proposed method achieves significant performance in
SRNVS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Sabour, Lily Goli, George Kopanas, Mark Matthews, Dmitry Lagun, Leonidas Guibas, Alec Jacobson, David J. Fleet, Andrea Tagliasacchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) is a promising technique for 3D reconstruction,
offering efficient training and rendering speeds, making it suitable for
real-time applications.However, current methods require highly controlled
environments (no moving people or wind-blown elements, and consistent lighting)
to meet the inter-view consistency assumption of 3DGS. This makes
reconstruction of real-world captures problematic. We present SpotlessSplats,
an approach that leverages pre-trained and general-purpose features coupled
with robust optimization to effectively ignore transient distractors. Our
method achieves state-of-the-art reconstruction quality both visually and
quantitatively, on casual captures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HAITCH: A Framework for Distortion and Motion Correction in Fetal
  Multi-Shell <span class="highlight-title">Diffusion</span>-Weighted MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haykel Snoussi, Davood Karimi, Onur Afacan, Mustafa Utkur, Ali Gholipour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion magnetic resonance imaging (dMRI) is pivotal for probing the
microstructure of the rapidly-developing fetal brain. However, fetal motion
during scans and its interaction with magnetic field inhomogeneities result in
artifacts and data scattering across spatial and angular domains. The effects
of those artifacts are more pronounced in high-angular resolution fetal dMRI,
where signal-to-noise ratio is very low. Those effects lead to biased estimates
and compromise the consistency and reliability of dMRI analysis. This work
presents HAITCH, the first and the only publicly available tool to correct and
reconstruct multi-shell high-angular resolution fetal dMRI data. HAITCH offers
several technical advances that include a blip-reversed dual-echo acquisition
for dynamic distortion correction, advanced motion correction for model-free
and robust reconstruction, optimized multi-shell design for enhanced
information capture and increased tolerance to motion, and outlier detection
for improved reconstruction fidelity. The framework is open-source, flexible,
and can be used to process any type of fetal dMRI data including single-echo or
single-shell acquisitions, but is most effective when used with multi-shell
multi-echo fetal dMRI data that cannot be processed with any of the existing
tools. Validation experiments on real fetal dMRI scans demonstrate significant
improvements and accurate correction across diverse fetal ages and motion
levels. HAITCH successfully removes artifacts and reconstructs high-fidelity
fetal dMRI data suitable for advanced diffusion modeling, including fiber
orientation distribution function estimation. These advancements pave the way
for more reliable analysis of the fetal brain microstructure and tractography
under challenging imaging conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ eMoE-Tracker: Environmental MoE-based <span class="highlight-title">Transformer</span> for Robust
  Event-guided Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Chen, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unique complementarity of frame-based and event cameras for high frame
rate object tracking has recently inspired some research attempts to develop
multi-modal fusion approaches. However, these methods directly fuse both
modalities and thus ignore the environmental attributes, e.g., motion blur,
illumination variance, occlusion, scale variation, etc. Meanwhile, no
interaction between search and template features makes distinguishing target
objects and backgrounds difficult. As a result, performance degradation is
induced especially in challenging conditions. This paper proposes a novel and
effective Transformer-based event-guided tracking framework, called
eMoE-Tracker, which achieves new SOTA performance under various conditions. Our
key idea is to disentangle the environment into several learnable attributes to
dynamically learn the attribute-specific features for better interaction and
discriminability between the target information and background. To achieve the
goal, we first propose an environmental Mix-of-Experts (eMoE) module that is
built upon the environmental Attributes Disentanglement to learn
attribute-specific features and environmental Attributes Gating to assemble the
attribute-specific features by the learnable attribute scores dynamically. The
eMoE module is a subtle router that fine-tunes the transformer backbone more
efficiently. We then introduce a contrastive relation modeling (CRM) module to
improve interaction and discriminability between the target information and
background. Extensive experiments on diverse event-based benchmark datasets
showcase the superior performance of our eMoE-Tracker compared to the prior
arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RGB-event single object tracking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Malaria Cell Detection Using Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Sawant, Anurag Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malaria remains one of the most pressing public health concerns globally,
causing significant morbidity and mortality, especially in sub-Saharan Africa.
Rapid and accurate diagnosis is crucial for effective treatment and disease
management. Traditional diagnostic methods, such as microscopic examination of
blood smears, are labor-intensive and require significant expertise, which may
not be readily available in resource-limited settings. This project aims to
automate the detection of malaria-infected cells using a deep learning
approach. We employed a convolutional neural network (CNN) based on the
ResNet50 architecture, leveraging transfer learning to enhance performance. The
Malaria Cell Images Dataset from Kaggle, containing 27,558 images categorized
into infected and uninfected cells, was used for training and evaluation. Our
model demonstrated high accuracy, precision, and recall, indicating its
potential as a reliable tool for assisting in malaria diagnosis. Additionally,
a web application was developed using Streamlit to allow users to upload cell
images and receive predictions about malaria infection, making the technology
accessible and user-friendly. This paper provides a comprehensive overview of
the methodology, experiments, and results, highlighting the effectiveness of
deep learning in medical image analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wavelets Are All You Need for Autoregressive Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wael Mattar, Idan Levy, Nir Sharon, Shai Dekel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we take a new approach to autoregressive image generation that
is based on two main ingredients. The first is wavelet image coding, which
allows to tokenize the visual details of an image from coarse to fine details
by ordering the information starting with the most significant bits of the most
significant wavelet coefficients. The second is a variant of a language
transformer whose architecture is re-designed and optimized for token sequences
in this 'wavelet language'. The transformer learns the significant statistical
correlations within a token sequence, which are the manifestations of
well-known correlations between the wavelet subbands at various resolutions. We
show experimental results with conditioning on the generation process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STLLaVA-Med: Self-Training Large Language and Vision Assistant for
  Medical 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guohao Sun, Can Qin, Huazhu Fu, Linwei Wang, Zhiqiang Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have shown significant potential in
assisting medical diagnosis by leveraging extensive biomedical datasets.
However, the advancement of medical image understanding and reasoning
critically depends on building high-quality visual instruction data, which is
costly and labor-intensive to obtain, particularly in the medical domain. To
mitigate this data-starving issue, we introduce Self-Training Large Language
and Vision Assistant for Medical (STLLaVA-Med). The proposed method is designed
to train a policy model (an LVLM) capable of auto-generating medical visual
instruction data to improve data efficiency, guided through Direct Preference
Optimization (DPO). Specifically, a more powerful and larger LVLM (e.g.,
GPT-4o) is involved as a biomedical expert to oversee the DPO fine-tuning
process on the auto-generated data, encouraging the policy model to align
efficiently with human preferences. We validate the efficacy and data
efficiency of STLLaVA-Med across three major medical Visual Question Answering
(VQA) benchmarks, demonstrating competitive zero-shot performance with the
utilization of only 9% of the medical data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Initialization on Intra-subject Pediatric Brain MR Image
  Registration: A Comparative Analysis between SyN ANTs and Deep Learning-Based
  Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andjela Dimitrijevic, Vincent Noblet, Benjamin De Leener
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study evaluates the performance of conventional SyN ANTs and
learning-based registration methods in the context of pediatric neuroimaging,
specifically focusing on intrasubject deformable registration. The comparison
involves three approaches: without (NR), with rigid (RR), and with rigid and
affine (RAR) initializations. In addition to initialization, performances are
evaluated in terms of accuracy, speed, and the impact of age intervals and sex
per pair. Data consists of the publicly available MRI scans from the Calgary
Preschool dataset, which includes 63 children aged 2-7 years, allowing for 431
registration pairs. We implemented the unsupervised DL framework with a U-Net
architecture using DeepReg and it was 5-fold cross-validated. Evaluation
includes Dice scores for tissue segmentation from 18 smaller regions obtained
by SynthSeg, analysis of log Jacobian determinants, and registration pro-rated
training and inference times. Learning-based approaches, with or without linear
initializations, exhibit slight superiority over SyN ANTs in terms of Dice
scores. Indeed, DL-based implementations with RR and RAR initializations
significantly outperform SyN ANTs. Both SyN ANTs and DL-based registration
involve parameter optimization, but the choice between these methods depends on
the scale of registration: network-based for broader coverage or SyN ANTs for
specific structures. Both methods face challenges with larger age intervals due
to greater growth changes. The main takeaway is that while DL-based methods
show promise with faster and more accurate registrations, SyN ANTs remains
robust and generalizable without the need for extensive training, highlighting
the importance of method selection based on specific registration needs in the
pediatric context. Our code is available at
https://github.com/neuropoly/pediatric-DL-registration
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024:013</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRACE: Graph-Regularized Attentive Convolutional Entanglement with
  Laplacian Smoothing for Robust DeepFake Video Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Chung Hsu, Shao-Ning Chen, Mei-Hsuan Wu, Yi-Fang Wang, Chia-Ming Lee, Yi-Shiuan Chou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As DeepFake video manipulation techniques escalate, posing profound threats,
the urgent need to develop efficient detection strategies is underscored.
However, one particular issue lies with facial images being mis-detected, often
originating from degraded videos or adversarial attacks, leading to unexpected
temporal artifacts that can undermine the efficacy of DeepFake video detection
techniques. This paper introduces a novel method for robust DeepFake video
detection, harnessing the power of the proposed Graph-Regularized Attentive
Convolutional Entanglement (GRACE) based on the graph convolutional network
with graph Laplacian to address the aforementioned challenges. First,
conventional Convolution Neural Networks are deployed to perform spatiotemporal
features for the entire video. Then, the spatial and temporal features are
mutually entangled by constructing a graph with sparse constraint, enforcing
essential features of valid face images in the noisy face sequences remaining,
thus augmenting stability and performance for DeepFake video detection.
Furthermore, the Graph Laplacian prior is proposed in the graph convolutional
network to remove the noise pattern in the feature space to further improve the
performance. Comprehensive experiments are conducted to illustrate that our
proposed method delivers state-of-the-art performance in DeepFake video
detection under noisy face sequences. The source code is available at
https://github.com/ming053l/GRACE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TPAMI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallax-tolerant Image Stitching via Segmentation-guided
  Multi-homography Warping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianli Liao, Ce Wang, Lei Li, Guangen Liu, Nan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large parallax between images is an intractable issue in image stitching.
Various warping-based methods are proposed to address it, yet the results are
unsatisfactory. In this paper, we propose a novel image stitching method using
multi-homography warping guided by image segmentation. Specifically, we
leverage the Segment Anything Model to segment the target image into numerous
contents and partition the feature points into multiple subsets via the
energy-based multi-homography fitting algorithm. The multiple subsets of
feature points are used to calculate the corresponding multiple homographies.
For each segmented content in the overlapping region, we select its
best-fitting homography with the lowest photometric error. For each segmented
content in the non-overlapping region, we calculate a weighted combination of
the linearized homographies. Finally, the target image is warped via the
best-fitting homographies to align with the reference image, and the final
panorama is generated via linear blending. Comprehensive experimental results
on the public datasets demonstrate that our method provides the best alignment
accuracy by a large margin, compared with the state-of-the-art methods. The
source code is available at https://github.com/tlliao/multi-homo-warp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving Token Gradient Conflict in Mixture-of-Experts for Large
  Vision-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longrong Yang, Dong Sheng, Chaoxiang Cai, Fan Yang, Size Li, Di Zhang, Xi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Mixture-of-Experts (MoE) has gained increasing attention in the study of
Large Vision-Language Models (LVLMs). It uses a sparse model to replace the
dense model, achieving comparable performance while activating fewer parameters
during inference, thus significantly reducing the inference cost. Existing MoE
methods in LVLMs encourage different experts to handle different tokens, and
thus they employ a router to predict the routing for each token. However, the
predictions are based solely on sample features and do not truly reveal the
optimization direction of tokens. This can lead to severe optimization
conflicts between different tokens within an expert. To address this problem,
this paper proposes a novel method based on token-level gradient analysis.
Specifically, we first use token-level gradients to identify conflicting tokens
in experts. Then, we add a specialized loss tailored to eliminate conflicts
among tokens within each expert. Our method can serve as a plug-in for diverse
Large Vision-Language Models, and extensive experimental results demonstrate
the effectiveness of our method. The code will be publicly available at
https://github.com/longrongyang/STGC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Value of PHH3 for Mitotic Figure Detection on H&E-stained Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Ganz, Christian Marzahl, Jonas Ammeling, Barbara Richter, Chloé Puget, Daniela Denk, Elena A. Demeter, Flaviu A. Tabaran, Gabriel Wasinger, Karoline Lipnik, Marco Tecilla, Matthew J. Valentine, Michael J. Dark, Niklas Abele, Pompei Bolfa, Ramona Erber, Robert Klopfleisch, Sophie Merz, Taryn A. Donovan, Samir Jabari, Christof A. Bertram, Katharina Breininger, Marc Aubreville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The count of mitotic figures (MFs) observed in hematoxylin and eosin
(H&E)-stained slides is an important prognostic marker as it is a measure for
tumor cell proliferation. However, the identification of MFs has a known low
inter-rater agreement. Deep learning algorithms can standardize this task, but
they require large amounts of annotated data for training and validation.
Furthermore, label noise introduced during the annotation process may impede
the algorithm's performance. Unlike H&E, the mitosis-specific antibody
phospho-histone H3 (PHH3) specifically highlights MFs. Counting MFs on slides
stained against PHH3 leads to higher agreement among raters and has therefore
recently been used as a ground truth for the annotation of MFs in H&E. However,
as PHH3 facilitates the recognition of cells indistinguishable from H&E stain
alone, the use of this ground truth could potentially introduce noise into the
H&E-related dataset, impacting model performance. This study analyzes the
impact of PHH3-assisted MF annotation on inter-rater reliability and object
level agreement through an extensive multi-rater experiment. We found that the
annotators' object-level agreement increased when using PHH3-assisted labeling.
Subsequently, MF detectors were evaluated on the resulting datasets to
investigate the influence of PHH3-assisted labeling on the models' performance.
Additionally, a novel dual-stain MF detector was developed to investigate the
interpretation-shift of PHH3-assisted labels used in H&E, which clearly
outperformed single-stain detectors. However, the PHH3-assisted labels did not
have a positive effect on solely H&E-based models. The high performance of our
dual-input detector reveals an information mismatch between the H&E and
PHH3-stained images as the cause of this effect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfiniBench: A Comprehensive Benchmark for Large Multimodal Models in
  Very Long Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirolos Ataallah, Chenhui Gou, Eslam Abdelrahman, Khushbu Pahwa, Jian Ding, Mohamed Elhoseiny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding long videos, ranging from tens of minutes to several hours,
presents unique challenges in video comprehension. Despite the increasing
importance of long-form video content, existing benchmarks primarily focus on
shorter clips. To address this gap, we introduce InfiniBench a comprehensive
benchmark for very long video understanding which presents 1)The longest video
duration, averaging 76.34 minutes; 2) The largest number of question-answer
pairs, 108.2K; 3) Diversity in questions that examine nine different skills and
include both multiple-choice questions and open-ended questions; 4)
Humancentric, as the video sources come from movies and daily TV shows, with
specific human-level question designs such as Movie Spoiler Questions that
require critical thinking and comprehensive understanding. Using InfiniBench,
we comprehensively evaluate existing Large MultiModality Models (LMMs) on each
skill, including the commercial model Gemini 1.5 Flash and the open-source
models. The evaluation shows significant challenges in our benchmark.Our
results show that the best AI models such Gemini struggles to perform well with
42.72% average accuracy and 2.71 out of 5 average score. We hope this benchmark
will stimulate the LMMs community towards long video and human-level
understanding. Our benchmark can be accessed at
https://vision-cair.github.io/InfiniBench/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 page ,17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FootBots: A <span class="highlight-title">Transformer</span>-based Architecture for Motion Prediction in
  Soccer <span class="chip">ICIP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillem Capellera, Luis Ferraz, Antonio Rubio, Antonio Agudo, Francesc Moreno-Noguer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion prediction in soccer involves capturing complex dynamics from player
and ball interactions. We present FootBots, an encoder-decoder
transformer-based architecture addressing motion prediction and conditioned
motion prediction through equivariance properties. FootBots captures temporal
and social dynamics using set attention blocks and multi-attention block
decoder. Our evaluation utilizes two datasets: a real soccer dataset and a
tailored synthetic one. Insights from the synthetic dataset highlight the
effectiveness of FootBots' social attention mechanism and the significance of
conditioned motion prediction. Empirical results on real soccer data
demonstrate that FootBots outperforms baselines in motion prediction and excels
in conditioned tasks, such as predicting the players based on the ball
position, predicting the offensive (defensive) team based on the ball and the
defensive (offensive) team, and predicting the ball position based on all
players. Our evaluation connects quantitative and qualitative findings.
https://youtu.be/9kaEkfzG3L8
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at IEEE ICIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StreamMOTP: Streaming and Unified Framework for Joint 3D Multi-Object
  Tracking and Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaheng Zhuang, Guoan Wang, Siyu Zhang, Xiyang Wang, Hangning Zhou, Ziyao Xu, Chi Zhang, Zhiheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D multi-object tracking and trajectory prediction are two crucial modules in
autonomous driving systems. Generally, the two tasks are handled separately in
traditional paradigms and a few methods have started to explore modeling these
two tasks in a joint manner recently. However, these approaches suffer from the
limitations of single-frame training and inconsistent coordinate
representations between tracking and prediction tasks. In this paper, we
propose a streaming and unified framework for joint 3D Multi-Object Tracking
and trajectory Prediction (StreamMOTP) to address the above challenges.
Firstly, we construct the model in a streaming manner and exploit a memory bank
to preserve and leverage the long-term latent features for tracked objects more
effectively. Secondly, a relative spatio-temporal positional encoding strategy
is introduced to bridge the gap of coordinate representations between the two
tasks and maintain the pose-invariance for trajectory prediction. Thirdly, we
further improve the quality and consistency of predicted trajectories with a
dual-stream predictor. We conduct extensive experiments on popular nuSences
dataset and the experimental results demonstrate the effectiveness and
superiority of StreamMOTP, which outperforms previous methods significantly on
both tasks. Furthermore, we also prove that the proposed framework has great
potential and advantages in actual applications of autonomous driving.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LightStereo: Channel Boost Is All Your Need for <span class="highlight-title">Efficient</span> 2D Cost
  Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianda Guo, Chenming Zhang, Dujun Nie, Wenzhao Zheng, Youmin Zhang, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LightStereo, a cutting-edge stereo-matching network crafted to
accelerate the matching process. Departing from conventional methodologies that
rely on aggregating computationally intensive 4D costs, LightStereo adopts the
3D cost volume as a lightweight alternative. While similar approaches have been
explored previously, our breakthrough lies in enhancing performance through a
dedicated focus on the channel dimension of the 3D cost volume, where the
distribution of matching costs is encapsulated. Our exhaustive exploration has
yielded plenty of strategies to amplify the capacity of the pivotal dimension,
ensuring both precision and efficiency. We compare the proposed LightStereo
with existing state-of-the-art methods across various benchmarks, which
demonstrate its superior performance in speed, accuracy, and resource
utilization. LightStereo achieves a competitive EPE metric in the SceneFlow
datasets while demanding a minimum of only 22 GFLOPs, with an inference time of
just 17 ms. Our comprehensive analysis reveals the effect of 2D cost
aggregation for stereo matching, paving the way for real-world applications of
efficient stereo systems. Code will be available at
\url{https://github.com/XiandaGuo/OpenStereo}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code will be available at
  \url{https://github.com/XiandaGuo/OpenStereo}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion Loss Attacking: Adversarial Attack Perception for Skeleton based
  on Multi-dimensional Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Liu, Qing Xu, Qijian Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attack on skeletal motion is a hot topic. However, existing
researches only consider part of dynamic features when measuring distance
between skeleton graph sequences, which results in poor imperceptibility. To
this end, we propose a novel adversarial attack method to attack action
recognizers for skeletal motions. Firstly, our method systematically proposes a
dynamic distance function to measure the difference between skeletal motions.
Meanwhile, we innovatively introduce emotional features for complementary
information. In addition, we use Alternating Direction Method of
Multipliers(ADMM) to solve the constrained optimization problem, which
generates adversarial samples with better imperceptibility to deceive the
classifiers. Experiments show that our method is effective on multiple action
classifiers and datasets. When the perturbation magnitude measured by l norms
is the same, the dynamic perturbations generated by our method are much lower
than that of other methods. What's more, we are the first to prove the
effectiveness of emotional features, and provide a new idea for measuring the
distance between skeletal motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extract More from Less: <span class="highlight-title">Efficient</span> Fine-Grained Visual Recognition in
  Low-Data Regimes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitry Demidov, Abduragim Shtanchaev, Mihail Mihaylov, Mohammad Almansoori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emerging task of fine-grained image classification in low-data regimes
assumes the presence of low inter-class variance and large intra-class
variation along with a highly limited amount of training samples per class.
However, traditional ways of separately dealing with fine-grained
categorisation and extremely scarce data may be inefficient under both these
harsh conditions presented together. In this paper, we present a novel
framework, called AD-Net, aiming to enhance deep neural network performance on
this challenge by leveraging the power of Augmentation and Distillation
techniques. Specifically, our approach is designed to refine learned features
through self-distillation on augmented samples, mitigating harmful overfitting.
We conduct comprehensive experiments on popular fine-grained image
classification benchmarks where our AD-Net demonstrates consistent improvement
over traditional fine-tuning and state-of-the-art low-data techniques.
Remarkably, with the smallest data available, our framework shows an
outstanding relative accuracy increase of up to 45 % compared to standard
ResNet-50 and up to 27 % compared to the closest SOTA runner-up. We emphasise
that our approach is practically architecture-independent and adds zero extra
cost at inference time. Additionally, we provide an extensive study on the
impact of every framework's component, highlighting the importance of each in
achieving optimal performance. Source code and trained models are publicly
available at github.com/demidovd98/fgic_lowd.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper and Appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daiwei Zhang, Gengyan Li, Jiajie Li, Mickaël Bressieux, Otmar Hilliges, Marc Pollefeys, Luc Van Gool, Xi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human activities are inherently complex, and even simple household tasks
involve numerous object interactions. To better understand these activities and
behaviors, it is crucial to model their dynamic interactions with the
environment. The recent availability of affordable head-mounted cameras and
egocentric data offers a more accessible and efficient means to understand
dynamic human-object interactions in 3D environments. However, most existing
methods for human activity modeling either focus on reconstructing 3D models of
hand-object or human-scene interactions or on mapping 3D scenes, neglecting
dynamic interactions with objects. The few existing solutions often require
inputs from multiple sources, including multi-camera setups, depth-sensing
cameras, or kinesthetic sensors. To this end, we introduce EgoGaussian, the
first method capable of simultaneously reconstructing 3D scenes and dynamically
tracking 3D object motion from RGB egocentric input alone. We leverage the
uniquely discrete nature of Gaussian Splatting and segment dynamic interactions
from the background. Our approach employs a clip-level online learning pipeline
that leverages the dynamic nature of human activities, allowing us to
reconstruct the temporal evolution of the scene in chronological order and
track rigid object motion. Additionally, our method automatically segments
object and background Gaussians, providing 3D representations for both static
scenes and dynamic objects. EgoGaussian outperforms previous NeRF and Dynamic
Gaussian methods in challenging in-the-wild videos and we also qualitatively
demonstrate the high quality of the reconstructed models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Generative Replay for Task-Incremental Segmentation with
  Concurrent Appearance and Semantic Forgetting <span class="chip">MICCAI24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Li, Jingyang Zhang, Pheng-Ann Heng, Lixu Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalist segmentation models are increasingly favored for diverse tasks
involving various objects from different image sources. Task-Incremental
Learning (TIL) offers a privacy-preserving training paradigm using tasks
arriving sequentially, instead of gathering them due to strict data sharing
policies. However, the task evolution can span a wide scope that involves
shifts in both image appearance and segmentation semantics with intricate
correlation, causing concurrent appearance and semantic forgetting. To solve
this issue, we propose a Comprehensive Generative Replay (CGR) framework that
restores appearance and semantic knowledge by synthesizing image-mask pairs to
mimic past task data, which focuses on two aspects: modeling image-mask
correspondence and promoting scalability for diverse tasks. Specifically, we
introduce a novel Bayesian Joint Diffusion (BJD) model for high-quality
synthesis of image-mask pairs with their correspondence explicitly preserved by
conditional denoising. Furthermore, we develop a Task-Oriented Adapter (TOA)
that recalibrates prompt embeddings to modulate the diffusion model, making the
data synthesis compatible with different tasks. Experiments on incremental
tasks (cardiac, fundus and prostate segmentation) show its clear advantage for
alleviating concurrent appearance and semantic forgetting. Code is available at
https://github.com/jingyzhang/CGR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MICCAI24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure-aware World Model for Probe Guidance via Large-scale
  Self-supervised Pre-train 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojun Jiang, Meng Li, Zhenguo Sun, Ning Jia, Yu Sun, Shaqi Luo, Shiji Song, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complex structure of the heart leads to significant challenges in
echocardiography, especially in acquisition cardiac ultrasound images.
Successful echocardiography requires a thorough understanding of the structures
on the two-dimensional plane and the spatial relationships between planes in
three-dimensional space. In this paper, we innovatively propose a large-scale
self-supervised pre-training method to acquire a cardiac structure-aware world
model. The core innovation lies in constructing a self-supervised task that
requires structural inference by predicting masked structures on a 2D plane and
imagining another plane based on pose transformation in 3D space. To support
large-scale pre-training, we collected over 1.36 million echocardiograms from
ten standard views, along with their 3D spatial poses. In the downstream probe
guidance task, we demonstrate that our pre-trained model consistently reduces
guidance errors across the ten most common standard views on the test set with
0.29 million samples from 74 routine clinical scans, indicating that
structure-aware pre-training benefits the scanning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPIRONet: Spatial-Frequency Learning and Topological Channel Interaction
  Network for Vessel Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        De-Xing Huang, Xiao-Hu Zhou, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Zhen-Qiu Feng, Mei-Jiang Gui, Hao Li, Tian-Yu Xiang, Bo-Xian Yao, Zeng-Guang Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic vessel segmentation is paramount for developing next-generation
interventional navigation systems. However, current approaches suffer from
suboptimal segmentation performances due to significant challenges in
intraoperative images (i.e., low signal-to-noise ratio, small or slender
vessels, and strong interference). In this paper, a novel spatial-frequency
learning and topological channel interaction network (SPIRONet) is proposed to
address the above issues. Specifically, dual encoders are utilized to
comprehensively capture local spatial and global frequency vessel features.
Then, a cross-attention fusion module is introduced to effectively fuse spatial
and frequency features, thereby enhancing feature discriminability.
Furthermore, a topological channel interaction module is designed to filter out
task-irrelevant responses based on graph neural networks. Extensive
experimental results on several challenging datasets (CADSA, CAXF, DCA1, and
XCAD) demonstrate state-of-the-art performances of our method. Moreover, the
inference speed of SPIRONet is 21 FPS with a 512x512 input size, surpassing
clinical real-time requirements (6~12FPS). These promising outcomes indicate
SPIRONet's potential for integration into vascular interventional navigation
systems. Code is available at https://github.com/Dxhuang-CASIA/SPIRONet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-Instruct: Generated Visual Instructions for Large Multimodal Model
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Liu, Xin Huang, Jinliang Zheng, Boxiao Liu, Jia Wang, Osamu Yoshie, Yu Liu, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces MM-Instruct, a large-scale dataset of diverse and
high-quality visual instruction data designed to enhance the
instruction-following capabilities of large multimodal models (LMMs). While
existing visual instruction datasets often focus on question-answering, they
struggle to generalize to broader application scenarios such as creative
writing, summarization, or image analysis. To address these limitations, we
propose a novel approach to constructing MM-Instruct that leverages the strong
instruction-following capabilities of existing LLMs to generate novel visual
instruction data from large-scale but conventional image captioning datasets.
MM-Instruct first leverages ChatGPT to automatically generate diverse
instructions from a small set of seed instructions through augmenting and
summarization. It then matches these instructions with images and uses an
open-sourced large language model (LLM) to generate coherent answers to the
instruction-image pairs. The LLM is grounded by the detailed text descriptions
of images in the whole answer generation process to guarantee the alignment of
the instruction data. Moreover, we introduce a benchmark based on the generated
instruction data to evaluate the instruction-following capabilities of existing
LMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5
model on the generated data, denoted as LLaVA-Instruct, which exhibits
significant improvements in instruction-following capabilities compared to
LLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models
are available at https://github.com/jihaonew/MM-Instruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset and models are available at
  https://github.com/jihaonew/MM-Instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EPOCH: Jointly Estimating the 3D Pose of Cameras and Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Garau, Giulia Martinelli, Niccolò Bisagno, Denis Tomè, Carsten Stoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular Human Pose Estimation (HPE) aims at determining the 3D positions of
human joints from a single 2D image captured by a camera. However, a single 2D
point in the image may correspond to multiple points in 3D space. Typically,
the uniqueness of the 2D-3D relationship is approximated using an orthographic
or weak-perspective camera model. In this study, instead of relying on
approximations, we advocate for utilizing the full perspective camera model.
This involves estimating camera parameters and establishing a precise,
unambiguous 2D-3D relationship. To do so, we introduce the EPOCH framework,
comprising two main components: the pose lifter network (LiftNet) and the pose
regressor network (RegNet). LiftNet utilizes the full perspective camera model
to precisely estimate the 3D pose in an unsupervised manner. It takes a 2D pose
and camera parameters as inputs and produces the corresponding 3D pose
estimation. These inputs are obtained from RegNet, which starts from a single
image and provides estimates for the 2D pose and camera parameters. RegNet
utilizes only 2D pose data as weak supervision. Internally, RegNet predicts a
3D pose, which is then projected to 2D using the estimated camera parameters.
This process enables RegNet to establish the unambiguous 2D-3D relationship.
Our experiments show that modeling the lifting as an unsupervised task with a
camera in-the-loop results in better generalization to unseen data. We obtain
state-of-the-art results for the 3D HPE on the Human3.6M and MPI-INF-3DHP
datasets. Our code is available at: [Github link upon acceptance, see
supplementary materials].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision <span class="highlight-title">Transformer</span> with Key-select Routing <span class="highlight-title">Attention</span> for Single Image
  Dehazing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihan Tong, Weijia Li, Qingxia Yang, Liyuan Chen, Peng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Ksformer, utilizing Multi-scale Key-select Routing Attention
(MKRA) for intelligent selection of key areas through multi-channel,
multi-scale windows with a top-k operator, and Lightweight Frequency Processing
Module (LFPM) to enhance high-frequency features, outperforming other dehazing
methods in tests.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages,4 figures,IEICE Trans. Information and Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMRo: Are Multimodal LLMs Eligible as the Brain for In-Home Robotics? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinming Li, Yichen Zhu, Zhiyuan Xu, Jindong Gu, Minjie Zhu, Xin Liu, Ning Liu, Yaxin Peng, Feifei Feng, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is fundamentally challenging for robots to serve as useful assistants in
human environments because this requires addressing a spectrum of sub-problems
across robotics, including perception, language understanding, reasoning, and
planning. The recent advancements in Multimodal Large Language Models (MLLMs)
have demonstrated their exceptional abilities in solving complex mathematical
problems, mastering commonsense and abstract reasoning. This has led to the
recent utilization of MLLMs as the brain in robotic systems, enabling these
models to conduct high-level planning prior to triggering low-level control
actions for task execution. However, it remains uncertain whether existing
MLLMs are reliable in serving the brain role of robots. In this study, we
introduce the first benchmark for evaluating Multimodal LLM for Robotic (MMRo)
benchmark, which tests the capability of MLLMs for robot applications.
Specifically, we identify four essential capabilities perception, task
planning, visual reasoning, and safety measurement that MLLMs must possess to
qualify as the robot's central processing unit. We have developed several
scenarios for each capability, resulting in a total of 14 metrics for
evaluation. We present experimental results for various MLLMs, including both
commercial and open-source models, to assess the performance of existing
systems. Our findings indicate that no single model excels in all areas,
suggesting that current MLLMs are not yet trustworthy enough to serve as the
cognitive core for robots. Our data can be found in
https://mm-robobench.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Fusion Model for Brain Tumor Classification Using Fine-Grained
  Gradient Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niful Islam, Mohaiminul Islam Bhuiyan, Jarin Tasnim Raya, Nur Shazwani Kamarudin, Khan Md Hasib, M. F. Mridha, Dewan Md. Farid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumors are one of the most common diseases that lead to early death if
not diagnosed at an early stage. Traditional diagnostic approaches are
extremely time-consuming and prone to errors. In this context, computer
vision-based approaches have emerged as an effective tool for accurate brain
tumor classification. While some of the existing solutions demonstrate
noteworthy accuracy, the models become infeasible to deploy in areas where
computational resources are limited. This research addresses the need for
accurate and fast classification of brain tumors with a priority of deploying
the model in technologically underdeveloped regions. The research presents a
novel architecture for precise brain tumor classification fusing pretrained
ResNet152V2 and modified VGG16 models. The proposed architecture undergoes a
diligent fine-tuning process that ensures fine gradients are preserved in deep
neural networks, which are essential for effective brain tumor classification.
The proposed solution incorporates various image processing techniques to
improve image quality and achieves an astounding accuracy of 98.36% and 98.04%
in Figshare and Kaggle datasets respectively. This architecture stands out for
having a streamlined profile, with only 2.8 million trainable parameters. We
have leveraged 8-bit quantization to produce a model of size 73.881 MB,
significantly reducing it from the previous size of 289.45 MB, ensuring smooth
deployment in edge devices even in resource-constrained areas. Additionally,
the use of Grad-CAM improves the interpretability of the model, offering
insightful information regarding its decision-making process. Owing to its high
discriminative ability, this model can be a reliable option for accurate brain
tumor classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Radiological Diagnosis: A Collaborative Approach Integrating
  AI and Human Expertise for Visual Miss Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Awasthi, Ngan Le, Zhigang Deng, Carol C. Wu, Hien Van Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-AI collaboration to identify and correct perceptual errors in chest
radiographs has not been previously explored. This study aimed to develop a
collaborative AI system, CoRaX, which integrates eye gaze data and radiology
reports to enhance diagnostic accuracy in chest radiology by pinpointing
perceptual errors and refining the decision-making process. Using public
datasets REFLACX and EGD-CXR, the study retrospectively developed CoRaX,
employing a large multimodal model to analyze image embeddings, eye gaze data,
and radiology reports. The system's effectiveness was evaluated based on its
referral-making process, the quality of referrals, and performance in
collaborative diagnostic settings. CoRaX was tested on a simulated error
dataset of 271 samples with 28% (93 of 332) missed abnormalities. The system
corrected 21% (71 of 332) of these errors, leaving 7% (22 of 312) unresolved.
The Referral-Usefulness score, indicating the accuracy of predicted regions for
all true referrals, was 0.63 (95% CI 0.59, 0.68). The Total-Usefulness score,
reflecting the diagnostic accuracy of CoRaX's interactions with radiologists,
showed that 84% (237 of 280) of these interactions had a score above 0.40. In
conclusion, CoRaX efficiently collaborates with radiologists to address
perceptual errors across various abnormalities, with potential applications in
the education and training of novice radiologists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review in Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MimicMotion: High-Quality Human Motion Video Generation with
  Confidence-aware Pose Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, Fangyuan Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, generative artificial intelligence has achieved significant
advancements in the field of image generation, spawning a variety of
applications. However, video generation still faces considerable challenges in
various aspects, such as controllability, video length, and richness of
details, which hinder the application and popularization of this technology. In
this work, we propose a controllable video generation framework, dubbed
MimicMotion, which can generate high-quality videos of arbitrary length
mimicking specific motion guidance. Compared with previous methods, our
approach has several highlights. Firstly, we introduce confidence-aware pose
guidance that ensures high frame quality and temporal smoothness. Secondly, we
introduce regional loss amplification based on pose confidence, which
significantly reduces image distortion. Lastly, for generating long and smooth
videos, we propose a progressive latent fusion strategy. By this means, we can
produce videos of arbitrary length with acceptable resource consumption. With
extensive experiments and user studies, MimicMotion demonstrates significant
improvements over previous approaches in various aspects. Detailed results and
comparisons are available on our project page:
https://tencent.github.io/MimicMotion .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-based Depth Estimation Methods from Monocular Image and
  Videos: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uchitha Rajapaksha, Ferdous Sohel, Hamid Laga, Dean Diepeveen, Mohammed Bennamoun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating depth from single RGB images and videos is of widespread interest
due to its applications in many areas, including autonomous driving, 3D
reconstruction, digital entertainment, and robotics. More than 500 deep
learning-based papers have been published in the past 10 years, which indicates
the growing interest in the task. This paper presents a comprehensive survey of
the existing deep learning-based methods, the challenges they address, and how
they have evolved in their architecture and supervision methods. It provides a
taxonomy for classifying the current work based on their input and output
modalities, network architectures, and learning methods. It also discusses the
major milestones in the history of monocular depth estimation, and different
pipelines, datasets, and evaluation metrics used in existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 10 figures, The paper has been accepted for publication in
  ACM Computing Surveys 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond First-Order: A Multi-Scale Approach to Finger Knuckle Print
  Biometrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengrui Gao, Ziyuan Yang, Andrew Beng Jin Teoh, Min Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, finger knuckle prints (FKPs) have gained attention due to their
rich textural patterns, positioning them as a promising biometric for identity
recognition. Prior FKP recognition methods predominantly leverage first-order
feature descriptors, which capture intricate texture details but fail to
account for structural information. Emerging research, however, indicates that
second-order textures, which describe the curves and arcs of the textures,
encompass this overlooked structural information. This paper introduces a novel
FKP recognition approach, the Dual-Order Texture Competition Network (DOTCNet),
designed to capture texture information in FKP images comprehensively. DOTCNet
incorporates three dual-order texture competitive modules (DTCMs), each
targeting textures at different scales. Each DTCM employs a learnable texture
descriptor, specifically a learnable Gabor filter (LGF), to extract texture
features. By leveraging LGFs, the network extracts first and second order
textures to describe fine textures and structural features thoroughly.
Furthermore, an attention mechanism enhances relevant features in the
first-order features, thereby highlighting significant texture details. For
second-order features, a competitive mechanism emphasizes structural
information while reducing noise from higher-order features. Extensive
experimental results reveal that DOTCNet significantly outperforms several
standard algorithms on the publicly available PolyU-FKP dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PopAlign: Population-Level Alignment for Fair Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shufan Li, Harkanwar Singh, Aditya Grover
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) models achieve high-fidelity generation through extensive
training on large datasets. However, these models may unintentionally pick up
undesirable biases of their training data, such as over-representation of
particular identities in gender or ethnicity neutral prompts. Existing
alignment methods such as Reinforcement Learning from Human Feedback (RLHF) and
Direct Preference Optimization (DPO) fail to address this problem effectively
because they operate on pairwise preferences consisting of individual samples,
while the aforementioned biases can only be measured at a population level. For
example, a single sample for the prompt "doctor" could be male or female, but a
model generating predominantly male doctors even with repeated sampling
reflects a gender bias. To address this limitation, we introduce PopAlign, a
novel approach for population-level preference optimization, while standard
optimization would prefer entire sets of samples over others. We further derive
a stochastic lower bound that directly optimizes for individual samples from
preferred populations over others for scalable training. Using human evaluation
and standard image quality and bias metrics, we show that PopAlign
significantly mitigates the bias of pretrained T2I models while largely
preserving the generation quality. Code is available at
https://github.com/jacklishufan/PopAlignSDXL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSAKD: Knowledge <span class="highlight-title">Distillation</span> with Cross Self-<span class="highlight-title">Attention</span> for
  Hyperspectral and Multispectral Image Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Chung Hsu, Chih-Chien Ni, Chia-Ming Lee, Li-Wei Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral imaging, capturing detailed spectral information for each
pixel, is pivotal in diverse scientific and industrial applications. Yet, the
acquisition of high-resolution (HR) hyperspectral images (HSIs) often needs to
be addressed due to the hardware limitations of existing imaging systems. A
prevalent workaround involves capturing both a high-resolution multispectral
image (HR-MSI) and a low-resolution (LR) HSI, subsequently fusing them to yield
the desired HR-HSI. Although deep learning-based methods have shown promising
in HR-MSI/LR-HSI fusion and LR-HSI super-resolution (SR), their substantial
model complexities hinder deployment on resource-constrained imaging devices.
This paper introduces a novel knowledge distillation (KD) framework for
HR-MSI/LR-HSI fusion to achieve SR of LR-HSI. Our KD framework integrates the
proposed Cross-Layer Residual Aggregation (CLRA) block to enhance efficiency
for constructing Dual Two-Streamed (DTS) network structure, designed to extract
joint and distinct features from LR-HSI and HR-MSI simultaneously. To fully
exploit the spatial and spectral feature representations of LR-HSI and HR-MSI,
we propose a novel Cross Self-Attention (CSA) fusion module to adaptively fuse
those features to improve the spatial and spectral quality of the reconstructed
HR-HSI. Finally, the proposed KD-based joint loss function is employed to
co-train the teacher and student networks. Our experimental results demonstrate
that the student model not only achieves comparable or superior LR-HSI SR
performance but also significantly reduces the model-size and computational
requirements. This marks a substantial advancement over existing
state-of-the-art methods. The source code is available at
https://github.com/ming053l/CSAKD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PM-VIS+: High-Performance Video Instance Segmentation without Video
  Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangjing Yang, Dun Liu, Xin Wang, Zhe Li, Barathwaj Anandan, Yi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video instance segmentation requires detecting, segmenting, and tracking
objects in videos, typically relying on costly video annotations. This paper
introduces a method that eliminates video annotations by utilizing image
datasets. The PM-VIS algorithm is adapted to handle both bounding box and
instance-level pixel annotations dynamically. We introduce ImageNet-bbox to
supplement missing categories in video datasets and propose the PM-VIS+
algorithm to adjust supervision based on annotation types. To enhance accuracy,
we use pseudo masks and semi-supervised optimization techniques on unannotated
video data. This method achieves high video instance segmentation performance
without manual video annotations, offering a cost-effective solution and new
perspectives for video instance segmentation applications. The code will be
available in https://github.com/ldknight/PM-VIS-plus
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MIPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Basketball-SORT: An Association Method for Complex Multi-object
  Occlusion Problems in Basketball Multi-object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingrui Hu, Atom Scott, Calvin Yeung, Keisuke Fujii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent deep learning-based object detection approaches have led to
significant progress in multi-object tracking (MOT) algorithms. The current MOT
methods mainly focus on pedestrian or vehicle scenes, but basketball sports
scenes are usually accompanied by three or more object occlusion problems with
similar appearances and high-intensity complex motions, which we call complex
multi-object occlusion (CMOO). Here, we propose an online and robust MOT
approach, named Basketball-SORT, which focuses on the CMOO problems in
basketball videos. To overcome the CMOO problem, instead of using the
intersection-over-union-based (IoU-based) approach, we use the trajectories of
neighboring frames based on the projected positions of the players. Our method
designs the basketball game restriction (BGR) and reacquiring Long-Lost IDs
(RLLI) based on the characteristics of basketball scenes, and we also solve the
occlusion problem based on the player trajectories and appearance features.
Experimental results show that our method achieves a Higher Order Tracking
Accuracy (HOTA) score of 63.48$\%$ on the basketball fixed video dataset and
outperforms other recent popular approaches. Overall, our approach solved the
CMOO problem more effectively than recent MOT algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AstMatch: Adversarial Self-training Consistency Framework for
  Semi-Supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanghao Zhu, Jing Zhang, Juanxiu Liu, Xiaohui Du, Ruqian Hao, Yong Liu, Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) has shown considerable potential in medical
image segmentation, primarily leveraging consistency regularization and
pseudo-labeling. However, many SSL approaches only pay attention to low-level
consistency and overlook the significance of pseudo-label reliability.
Therefore, in this work, we propose an adversarial self-training consistency
framework (AstMatch). Firstly, we design an adversarial consistency
regularization (ACR) approach to enhance knowledge transfer and strengthen
prediction consistency under varying perturbation intensities. Second, we apply
a feature matching loss for adversarial training to incorporate high-level
consistency regularization. Additionally, we present the pyramid channel
attention (PCA) and efficient channel and spatial attention (ECSA) modules to
improve the discriminator's performance. Finally, we propose an adaptive
self-training (AST) approach to ensure the pseudo-labels' quality. The proposed
AstMatch has been extensively evaluated with cutting-edge SSL methods on three
public-available datasets. The experimental results under different labeled
ratios indicate that AstMatch outperforms other existing methods, achieving new
state-of-the-art performance. Our code will be available at
https://github.com/GuanghaoZhu663/AstMatch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Efficient</span> Event Stream <span class="highlight-title">Super-Resolution</span> with Recursive Multi-Branch
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanmin Liang, Zhilin Huang, Xiawu Zheng, Feidiao Yang, Jun Peng, Kai Huang, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Event Stream Super-Resolution (ESR) methods overlook the redundant
and complementary information present in positive and negative events within
the event stream, employing a direct mixing approach for super-resolution,
which may lead to detail loss and inefficiency. To address these issues, we
propose an efficient Recursive Multi-Branch Information Fusion Network (RMFNet)
that separates positive and negative events for complementary information
extraction, followed by mutual supplementation and refinement. Particularly, we
introduce Feature Fusion Modules (FFM) and Feature Exchange Modules (FEM). FFM
is designed for the fusion of contextual information within neighboring event
streams, leveraging the coupling relationship between positive and negative
events to alleviate the misleading of noises in the respective branches. FEM
efficiently promotes the fusion and exchange of information between positive
and negative branches, enabling superior local information enhancement and
global information complementation. Experimental results demonstrate that our
approach achieves over 17% and 31% improvement on synthetic and real datasets,
accompanied by a 2.3X acceleration. Furthermore, we evaluate our method on two
downstream event-driven applications, \emph{i.e.}, object recognition and video
reconstruction, achieving remarkable results that outperform existing methods.
Our code and Supplementary Material are available at
https://github.com/Lqm26/RMFNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precision matters: Precision-aware ensemble for weakly supervised
  semantic segmentation <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsung Park, Hyunjung Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly Supervised Semantic Segmentation (WSSS) employs weak supervision, such
as image-level labels, to train the segmentation model. Despite the impressive
achievement in recent WSSS methods, we identify that introducing weak labels
with high mean Intersection of Union (mIoU) does not guarantee high
segmentation performance. Existing studies have emphasized the importance of
prioritizing precision and reducing noise to improve overall performance. In
the same vein, we propose ORANDNet, an advanced ensemble approach tailored for
WSSS. ORANDNet combines Class Activation Maps (CAMs) from two different
classifiers to increase the precision of pseudo-masks (PMs). To further
mitigate small noise in the PMs, we incorporate curriculum learning. This
involves training the segmentation model initially with pairs of smaller-sized
images and corresponding PMs, gradually transitioning to the original-sized
pairs. By combining the original CAMs of ResNet-50 and ViT, we significantly
improve the segmentation performance over the single-best model and the naive
ensemble model, respectively. We further extend our ensemble method to CAMs
from AMN (ResNet-like) and MCTformer (ViT-like) models, achieving performance
benefits in advanced WSSS models. It highlights the potential of our ORANDNet
as a final add-on module for WSSS models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, accepted in AAAI 2024 Edge Intelligence Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Predictive Simulation Using Structured Graphical Models and
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghua Lou, Meet Dave, Shrinu Kushagra, Miguel Lazaro-Gredilla, Kevin Murphy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an approach to simulating trajectories of multiple interacting
agents (road users) based on transformers and probabilistic graphical models
(PGMs), and apply it to the Waymo SimAgents challenge. The transformer baseline
is based on the MTR model, which predicts multiple future trajectories
conditioned on the past trajectories and static road layout features. We then
improve upon these generated trajectories using a PGM, which contains factors
which encode prior knowledge, such as a preference for smooth trajectories, and
avoidance of collisions with static obstacles and other moving agents. We
perform (approximate) MAP inference in this PGM using the Gauss-Newton method.
Finally we sample $K=32$ trajectories for each of the $N \sim 100$ agents for
the next $T=8 \Delta$ time steps, where $\Delta=10$ is the sampling rate per
second. Following the Model Predictive Control (MPC) paradigm, we only return
the first element of our forecasted trajectories at each step, and then we
replan, so that the simulation can constantly adapt to its changing
environment. We therefore call our approach "Model Predictive Simulation" or
MPS. We show that MPS improves upon the MTR baseline, especially in safety
critical metrics such as collision rate. Furthermore, our approach is
compatible with any underlying forecasting model, and does not require extra
training, so we believe it is a valuable contribution to the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Special Mention at the Waymo Sim Agents Challenge 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PPTFormer: Pseudo Multi-Perspective <span class="highlight-title">Transformer</span> for UAV Segmentation <span class="chip">IJCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deyi Ji, Wenwei Jin, Hongtao Lu, Feng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ascension of Unmanned Aerial Vehicles (UAVs) in various fields
necessitates effective UAV image segmentation, which faces challenges due to
the dynamic perspectives of UAV-captured images. Traditional segmentation
algorithms falter as they cannot accurately mimic the complexity of UAV
perspectives, and the cost of obtaining multi-perspective labeled datasets is
prohibitive. To address these issues, we introduce the PPTFormer, a novel
\textbf{P}seudo Multi-\textbf{P}erspective \textbf{T}rans\textbf{former}
network that revolutionizes UAV image segmentation. Our approach circumvents
the need for actual multi-perspective data by creating pseudo perspectives for
enhanced multi-perspective learning. The PPTFormer network boasts Perspective
Decomposition, novel Perspective Prototypes, and a specialized encoder and
decoder that together achieve superior segmentation results through Pseudo
Multi-Perspective Attention (PMP Attention) and fusion. Our experiments
demonstrate that PPTFormer achieves state-of-the-art performance across five
UAV segmentation datasets, confirming its capability to effectively simulate
UAV flight perspectives and significantly advance segmentation precision. This
work presents a pioneering leap in UAV scene understanding and sets a new
benchmark for future developments in semantic segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Video Compression using Pixel Shift Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hitesh Saai Mananchery Panneerselvam, Smit Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Video comprises approximately ~85\% of all internet traffic, but video
encoding/compression is being historically done with hard coded rules, which
has worked well but only to a certain limit. We have seen a surge in video
compression algorithms using ML-based models in the last few years and many of
them have outperformed several legacy codecs. The models range from encoding
video end to end using an ML approach or replacing some intermediate steps in
legacy codecs using ML models to increase the efficiency of those steps.
  Optimizing video storage is an essential aspect of video processing, so we
are proposing one of the possible approaches to achieve it is by avoiding
redundant data at each frame. In this paper, we want to introduce the approach
of redundancies removal in subsequent frames for a given video as a main
approach for video compression. We call this method Redundancy Removal using
Shift (R\textsuperscript2S). This method can be utilized across various Machine
Learning model algorithms, and make the compression more accessible and
adaptable. In this study, we have utilized a computer vision-based pixel point
tracking method to identify redundant pixels to encode video for optimal
storage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Deep Clustering: From the Prior Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiding Lu, Haobin Li, Yunfan Li, Yijie Lin, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facilitated by the powerful feature extraction ability of neural networks,
deep clustering has achieved great success in analyzing high-dimensional and
complex real-world data. The performance of deep clustering methods is affected
by various factors such as network structures and learning objectives. However,
as pointed out in this survey, the essence of deep clustering lies in the
incorporation and utilization of prior knowledge, which is largely ignored by
existing works. From pioneering deep clustering methods based on data structure
assumptions to recent contrastive clustering methods based on data augmentation
invariances, the development of deep clustering intrinsically corresponds to
the evolution of prior knowledge. In this survey, we provide a comprehensive
review of deep clustering methods by categorizing them into six types of prior
knowledge. We find that in general the prior innovation follows two trends,
namely, i) from mining to constructing, and ii) from internal to external.
Besides, we provide a benchmark on five widely-used datasets and analyze the
performance of methods with diverse priors. By providing a novel prior
knowledge perspective, we hope this survey could provide some novel insights
and inspire future research in the deep clustering community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SK-VQA: Synthetic Knowledge Generation at Scale for Training
  Context-Augmented Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Su, Man Luo, Kris W Pan, Tien Pei Chou, Vasudev Lal, Phillip Howard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data generation has gained significant attention recently for its
utility in training large vision and language models. However, the application
of synthetic data to the training of multimodal context-augmented generation
systems has been relatively unexplored. This gap in existing work is important
because existing vision and language models (VLMs) are not trained specifically
for context-augmented generation. Resources for adapting such models are
therefore crucial for enabling their use in retrieval-augmented generation
(RAG) settings, where a retriever is used to gather relevant information that
is then subsequently provided to a generative model via context augmentation.
To address this challenging problem, we generate SK-VQA: a large synthetic
multimodal dataset containing over 2 million question-answer pairs which
require external knowledge to determine the final answer. Our dataset is both
larger and significantly more diverse than existing resources of its kind,
possessing over 11x more unique questions and containing images from a greater
variety of sources than previously-proposed datasets. Through extensive
experiments, we demonstrate that our synthetic dataset can not only serve as a
challenging benchmark, but is also highly effective for adapting existing
generative multimodal models for context-augmented generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting <span class="highlight-title">Diffusion</span> Prior for <span class="highlight-title">Real-World</span> Image <span class="highlight-title">Super-Resolution</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07015v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07015v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C. K. Chan, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to leverage prior knowledge encapsulated in
pre-trained text-to-image diffusion models for blind super-resolution (SR).
Specifically, by employing our time-aware encoder, we can achieve promising
restoration results without altering the pre-trained synthesis model, thereby
preserving the generative prior and minimizing training cost. To remedy the
loss of fidelity caused by the inherent stochasticity of diffusion models, we
employ a controllable feature wrapping module that allows users to balance
quality and fidelity by simply adjusting a scalar value during the inference
process. Moreover, we develop a progressive aggregation sampling strategy to
overcome the fixed-size constraints of pre-trained diffusion models, enabling
adaptation to resolutions of any size. A comprehensive evaluation of our method
using both synthetic and real-world benchmarks demonstrates its superiority
over current state-of-the-art approaches. Code and models are available at
https://github.com/IceClear/StableSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCV'2024. Some Figs are compressed due to size limits.
  Uncompressed ver.:
  https://github.com/IceClear/StableSR/releases/download/UncompressedPDF/StableSR_IJCV_Uncompressed.pdf.
  Project page: https://iceclear.github.io/projects/stablesr/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EnSolver: Uncertainty-Aware Ensemble CAPTCHA Solvers with Theoretical
  Guarantees <span class="chip">UAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duc C. Hoang, Behzad Ousat, Amin Kharraz, Cuong V. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popularity of text-based CAPTCHA as a security mechanism to protect
websites from automated bots has prompted researches in CAPTCHA solvers, with
the aim of understanding its failure cases and subsequently making CAPTCHAs
more secure. Recently proposed solvers, built on advances in deep learning, are
able to crack even the very challenging CAPTCHAs with high accuracy. However,
these solvers often perform poorly on out-of-distribution samples that contain
visual features different from those in the training set. Furthermore, they
lack the ability to detect and avoid such samples, making them susceptible to
being locked out by defense systems after a certain number of failed attempts.
In this paper, we propose EnSolver, a family of CAPTCHA solvers that use deep
ensemble uncertainty to detect and skip out-of-distribution CAPTCHAs, making it
harder to be detected. We prove novel theoretical bounds on the effectiveness
of our solvers and demonstrate their use with state-of-the-art CAPTCHA solvers.
Our experiments show that the proposed approaches perform well when cracking
CAPTCHA datasets that contain both in-distribution and out-of-distribution
samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A previous version of this paper was presented at the Epistemic
  Uncertainty - E-pi UAI 2023 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness Assessment of a Runway Object Classifier for Safe Aircraft
  Taxiing <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00035v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00035v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhak Elboher, Raya Elsaleh, Omri Isac, Mélanie Ducoffe, Audrey Galametz, Guillaume Povéda, Ryma Boumazouza, Noémie Cohen, Guy Katz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As deep neural networks (DNNs) are becoming the prominent solution for many
computational problems, the aviation industry seeks to explore their potential
in alleviating pilot workload and in improving operational safety. However, the
use of DNNs in this type of safety-critical applications requires a thorough
certification process. This need can be addressed through formal verification,
which provides rigorous assurances -- e.g.,~by proving the absence of certain
mispredictions. In this case-study paper, we demonstrate this process using an
image-classifier DNN currently under development at Airbus and intended for use
during the aircraft taxiing phase. We use formal methods to assess this DNN's
robustness to three common image perturbation types: noise, brightness and
contrast, and some of their combinations. This process entails multiple
invocations of the underlying verifier, which might be computationally
expensive; and we therefore propose a method that leverages the monotonicity of
these robustness properties, as well as the results of past verification
queries, in order to reduce the overall number of verification queries required
by nearly 60%. Our results provide an indication of the level of robustness
achieved by the DNN classifier under study, and indicate that it is
considerably more vulnerable to noise than to brightness or contrast
perturbations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint version of the paper in the proceedings of 43rd
  Digital Avionics Systems Conference (DASC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to utilize image second-order derivative information for crisp
  edge detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05779v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05779v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changsong Liu, Wei Zhang, Yanyan Liu, Yimeng Fan, Mingyang Li, Wenlin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge detection is a fundamental task in computer vision. It has made great
progress under the development of deep convolutional neural networks (DCNNs),
some of which have achieved a beyond human-level performance. However, recent
top-performing edge detection methods tend to generate thick and noisy edge
lines. In this work, we solve this problem from two aspects: (1) the lack of
prior knowledge regarding image edges, and (2) the issue of imbalanced pixel
distribution. We propose a second-order derivative-based multi-scale contextual
enhancement module (SDMCM) to help the model locate true edge pixels accurately
by introducing the edge prior knowledge. We also construct a hybrid focal loss
function (HFL) to alleviate the imbalanced distribution issue. In addition, we
employ the conditionally parameterized convolution (CondConv) to develop a
novel boundary refinement module (BRM), which can further refine the final
output edge maps. In the end, we propose a U-shape network named LUS-Net which
is based on the SDMCM and BRM for crisp edge detection. We perform extensive
experiments on three standard benchmarks, and the experiment results illustrate
that our method can predict crisp and clean edge maps and achieves
state-of-the-art performance on the BSDS500 dataset (ODS=0.829), NYUD-V2
dataset (ODS=0.768), and BIPED dataset (ODS=0.903).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DWARF: Disease-weighted network for <span class="highlight-title">attention</span> map refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Luo, Aurélie Pahud de Mortanges, Oana Inel, Abraham Bernstein, Mauricio Reyes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interpretability of deep learning is crucial for evaluating the
reliability of medical imaging models and reducing the risks of inaccurate
patient recommendations. This study addresses the "human out of the loop" and
"trustworthiness" issues in medical image analysis by integrating medical
professionals into the interpretability process. We propose a disease-weighted
attention map refinement network (DWARF) that leverages expert feedback to
enhance model relevance and accuracy. Our method employs cyclic training to
iteratively improve diagnostic performance, generating precise and
interpretable feature maps. Experimental results demonstrate significant
improvements in interpretability and diagnostic accuracy across multiple
medical imaging datasets. This approach fosters effective collaboration between
AI systems and healthcare professionals, ultimately aiming to improve patient
outcomes
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling State Shifting via Local-Global <span class="highlight-title">Distillation</span> for Event-Frame
  Gaze Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiading Li, Zhiyu Zhu, Jinhui Hou, Junhui Hou, Jinjian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the problem of passive gaze estimation using both event
and frame data. Considering the inherently different physiological structures,
it is intractable to accurately estimate gaze purely based on a given state.
Thus, we reformulate gaze estimation as the quantification of the state
shifting from the current state to several prior registered anchor states.
Specifically, we propose a two-stage learning-based gaze estimation framework
that divides the whole gaze estimation process into a coarse-to-fine approach
involving anchor state selection and final gaze location. Moreover, to improve
the generalization ability, instead of learning a large gaze estimation network
directly, we align a group of local experts with a student network, where a
novel denoising distillation algorithm is introduced to utilize denoising
diffusion techniques to iteratively remove inherent noise in event data.
Extensive experiments demonstrate the effectiveness of the proposed method,
which surpasses state-of-the-art methods by a large margin of 15$\%$. The code
will be publicly available at
https://github.com/jdjdli/Denoise_distill_EF_gazetracker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tracking Object Positions in Reinforcement Learning: A Metric for
  Keypoint Detection (extended version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma Cramer, Jonas Reiher, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) for robot control typically requires a detailed
representation of the environment state, including information about
task-relevant objects not directly measurable. Keypoint detectors, such as
spatial autoencoders (SAEs), are a common approach to extracting a
low-dimensional representation from high-dimensional image data. SAEs aim at
spatial features such as object positions, which are often useful
representations in robotic RL. However, whether an SAE is actually able to
track objects in the scene and thus yields a spatial state representation well
suited for RL tasks has rarely been examined due to a lack of established
metrics. In this paper, we propose to assess the performance of an SAE instance
by measuring how well keypoints track ground truth objects in images. We
present a computationally lightweight metric and use it to evaluate common
baseline SAE architectures on image data from a simulated robot task. We find
that common SAEs differ substantially in their spatial extraction capability.
Furthermore, we validate that SAEs that perform well in our metric achieve
superior performance when used in downstream RL. Thus, our metric is an
effective and lightweight indicator of RL performance before executing
expensive RL training. Building on these insights, we identify three key
modifications of SAE architectures to improve tracking performance. We make our
code available at anonymous.4open.science/r/sae-rl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LatentExplainer: Explaining Latent Representations in Deep Generative
  Models with Multi-modal Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14862v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14862v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengdan Zhu, Raasikh Kanjiani, Jiahui Lu, Andrew Choi, Qirui Ye, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models like VAEs and diffusion models have advanced various
generation tasks by leveraging latent variables to learn data distributions and
generate high-quality samples. Despite the field of explainable AI making
strides in interpreting machine learning models, understanding latent variables
in generative models remains challenging. This paper introduces
LatentExplainer, a framework for automatically generating semantically
meaningful explanations of latent variables in deep generative models.
LatentExplainer tackles three main challenges: inferring the meaning of latent
variables, aligning explanations with inductive biases, and handling varying
degrees of explainability. By perturbing latent variables and interpreting
changes in generated data, the framework provides a systematic approach to
understanding and controlling the data generation process, enhancing the
transparency and interpretability of deep generative models. We evaluate our
proposed method on several real-world and synthetic datasets, and the results
demonstrate superior performance in generating high-quality explanations of
latent variables.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mining Open Semantics from CLIP: A Relation Transition Perspective for
  Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cilin Yan, Haochen Wang, Xiaolong Jiang, Yao Hu, Xu Tang, Guoliang Kang, Efstratios Gavves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Vision-Language Pre-training(CLIP) demonstrates impressive
zero-shot capability. The key to improve the adaptation of CLIP to downstream
task with few exemplars lies in how to effectively model and transfer the
useful knowledge embedded in CLIP. Previous work mines the knowledge typically
based on the limited visual samples and close-set semantics (i.e., within
target category set of downstream task). However, the aligned CLIP image/text
encoders contain abundant relationships between visual features and almost
infinite open semantics, which may benefit the few-shot learning but remains
unexplored. In this paper, we propose to mine open semantics as anchors to
perform a relation transition from image-anchor relationship to image-target
relationship to make predictions. Specifically, we adopt a transformer module
which takes the visual feature as "Query", the text features of the anchors as
"Key" and the similarity matrix between the text features of anchor and target
classes as "Value". In this way, the output of such a transformer module
represents the relationship between the image and target categories, i.e., the
classification predictions. To avoid manually selecting the open semantics, we
make the [CLASS] token of input text embedding learnable. We conduct extensive
experiments on eleven representative classification datasets. The results show
that our method performs favorably against previous state-of-the-arts
considering few-shot classification settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kandinsky 3.0 Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03511v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03511v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Arkhipkin, Andrei Filatov, Viacheslav Vasilev, Anastasia Maltseva, Said Azizov, Igor Pavlov, Julia Agafonova, Andrey Kuznetsov, Denis Dimitrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Kandinsky 3.0, a large-scale text-to-image generation model based
on latent diffusion, continuing the series of text-to-image Kandinsky models
and reflecting our progress to achieve higher quality and realism of image
generation. In this report we describe the architecture of the model, the data
collection procedure, the training technique, and the production system for
user interaction. We focus on the key components that, as we have identified as
a result of a large number of experiments, had the most significant impact on
improving the quality of our model compared to the others. We also describe
extensions and applications of our model, including super resolution,
inpainting, image editing, image-to-video generation, and a distilled version
of Kandinsky 3.0 - Kandinsky 3.1, which does inference in 4 steps of the
reverse process and 20 times faster without visual quality decrease. By
side-by-side human preferences comparison, Kandinsky becomes better in text
understanding and works better on specific domains. The code is available at
https://github.com/ai-forever/Kandinsky-3
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ai-forever.github.io/Kandinsky-3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deformable MRI Sequence Registration for AI-based Prostate Cancer
  Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessa Hering, Sarah de Boer, Anindo Saha, Jasper J. Twilt, Mattias P. Heinrich, Derya Yakar, Maarten de Rooij, Henkjan Huisman, Joeran S. Bosma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The PI-CAI (Prostate Imaging: Cancer AI) challenge led to expert-level
diagnostic algorithms for clinically significant prostate cancer detection. The
algorithms receive biparametric MRI scans as input, which consist of
T2-weighted and diffusion-weighted scans. These scans can be misaligned due to
multiple factors in the scanning process. Image registration can alleviate this
issue by predicting the deformation between the sequences. We investigate the
effect of image registration on the diagnostic performance of AI-based prostate
cancer diagnosis. First, the image registration algorithm, developed in
MeVisLab, is analyzed using a dataset with paired lesion annotations. Second,
the effect on diagnosis is evaluated by comparing case-level cancer diagnosis
performance between using the original dataset, rigidly aligned
diffusion-weighted scans, or deformably aligned diffusion-weighted scans. Rigid
registration showed no improvement. Deformable registration demonstrated a
substantial improvement in lesion overlap (+10% median Dice score) and a
positive yet non-significant improvement in diagnostic performance (+0.3%
AUROC, p=0.18). Our investigation shows that a substantial improvement in
lesion alignment does not directly lead to a significant improvement in
diagnostic performance. Qualitative analysis indicated that jointly developing
image registration methods and diagnostic AI algorithms could enhance
diagnostic accuracy and patient outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Data Curation for Self-Supervised Learning: A Clustering-Based
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy V. Vo, Vasil Khalidov, Timothée Darcet, Théo Moutakanni, Nikita Smetanin, Marc Szafraniec, Hugo Touvron, Camille Couprie, Maxime Oquab, Armand Joulin, Hervé Jégou, Patrick Labatut, Piotr Bojanowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised features are the cornerstone of modern machine learning
systems. They are typically pre-trained on data collections whose construction
and curation typically require extensive human effort. This manual process has
some limitations similar to those encountered in supervised learning, e.g., the
crowd-sourced selection of data is costly and time-consuming, preventing
scaling the dataset size. In this work, we consider the problem of automatic
curation of high-quality datasets for self-supervised pre-training. We posit
that such datasets should be large, diverse and balanced, and propose a
clustering-based approach for building ones satisfying all these criteria. Our
method involves successive and hierarchical applications of $k$-means on a
large and diverse data repository to obtain clusters that distribute uniformly
among data concepts, followed by a hierarchical, balanced sampling step from
these clusters. Extensive experiments on three different data domains including
web-based images, satellite images and text show that features trained on our
automatically curated datasets outperform those trained on uncurated data while
being on par or better than ones trained on manually curated data. Code is
available at https://github.com/facebookresearch/ssl-data-curation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver
  with a Few Partial Ultrasound Scans <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaushalya Sivayogaraj, Sahan T. Guruge, Udari Liyanage, Jeevani Udupihille, Saroj Jayasinghe, Gerard Fernando, Ranga Rodrigo, M. Rukshani Liyanaarachchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction of the liver for volumetry is important for qualitative
analysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,
although advantageous due to less acquisition time and safety, is challenging
due to the inherent noisiness in US scans, blurry boundaries, and partial liver
visibility. We address these challenges by using the segmentation masks of a
few incomplete sagittal-plane US scans of the liver in conjunction with a
statistical shape model (SSM) built using a set of CT scans of the liver. We
compute the shape parameters needed to warp this canonical SSM to fit the US
scans through a parametric regression network. The resulting 3D liver
reconstruction is accurate and leads to automatic liver volume calculation. We
evaluate the accuracy of the estimated liver volumes with respect to CT
segmentation volumes using RMSE. Our volume computation is statistically much
closer to the volume estimated using CT scans than the volume computed using
Childs' method by radiologists: p-value of 0.094 (>0.05) says that there is no
significant difference between CT segmentation volumes and ours in contrast to
Childs' method. We validate our method using investigations (ablation studies)
on the US image resolution, the number of CT scans used for SSM, the number of
principal components, and the number of input US scans. To the best of our
knowledge, this is the first automatic liver volumetry system using a few
incomplete US scans given a set of CT scans of livers for SSM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, Accepted to MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-domain Denoising for Low-dose Multi-frame Spiral Computed
  Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10839v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10839v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Lu, Zhixin Xu, Moon Hyung Choi, Jimin Kim, Seung-Won Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed tomography (CT) has been used worldwide as a non-invasive test to
assist in diagnosis. However, the ionizing nature of X-ray exposure raises
concerns about potential health risks such as cancer. The desire for lower
radiation doses has driven researchers to improve reconstruction quality.
Although previous studies on low-dose computed tomography (LDCT) denoising have
demonstrated the effectiveness of learning-based methods, most were developed
on the simulated data. However, the real-world scenario differs significantly
from the simulation domain, especially when using the multi-slice spiral
scanner geometry. This paper proposes a two-stage method for the commercially
available multi-slice spiral CT scanners that better exploits the complete
reconstruction pipeline for LDCT denoising across different domains. Our
approach makes good use of the high redundancy of multi-slice projections and
the volumetric reconstructions while leveraging the over-smoothing problem in
conventional cascaded frameworks caused by aggressive denoising. The dedicated
design also provides a more explicit interpretation of the data flow. Extensive
experiments on various datasets showed that the proposed method could remove up
to 70\% of noise without compromised spatial resolution, and subjective
evaluations by two experienced radiologists further supported its superior
performance against state-of-the-art methods in clinical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defect Detection in Synthetic Fibre Ropes using Detectron2 Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anju Rani, Daniel O. Arroyo, Petar Durdevic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fibre ropes with the latest technology have emerged as an appealing
alternative to steel ropes for offshore industries due to their lightweight and
high tensile strength. At the same time, frequent inspection of these ropes is
essential to ensure the proper functioning and safety of the entire system. The
development of deep learning (DL) models in condition monitoring (CM)
applications offers a simpler and more effective approach for defect detection
in synthetic fibre ropes (SFRs). The present paper investigates the performance
of Detectron2, a state-of-the-art library for defect detection and instance
segmentation. Detectron2 with Mask R-CNN architecture is used for segmenting
defects in SFRs. Mask R-CNN with various backbone configurations has been
trained and tested on an experimentally obtained dataset comprising 1,803
high-dimensional images containing seven damage classes (placking high,
placking medium, placking low, compression, core out, chafing, and normal
respectively) for SFRs. By leveraging the capabilities of Detectron2, this
study aims to develop an automated and efficient method for detecting defects
in SFRs, enhancing the inspection process, and ensuring the safety of the fibre
ropes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessment of Sentinel-2 spatial and temporal coverage based on the
  scene classification layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristhian Sanchez, Francisco Mena, Marcela Charfuelan, Marlon Nuske, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the launch of the Sentinel-2 (S2) satellites, many ML models have used
the data for diverse applications. The scene classification layer (SCL) inside
the S2 product provides rich information for training, such as filtering images
with high cloud coverage. However, there is more potential in this. We propose
a technique to assess the clean optical coverage of a region, expressed by a
SITS and calculated with the S2-based SCL data. With a manual threshold and
specific labels in the SCL, the proposed technique assigns a percentage of
spatial and temporal coverage across the time series and a high/low assessment.
By evaluating the AI4EO challenge for Enhanced Agriculture, we show that the
assessment is correlated to the predictive results of ML models. The
classification results in a region with low spatial and temporal coverage is
worse than in a region with high coverage. Finally, we applied the technique
across all continents of the global dataset LandCoverNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE International Geoscience and Remote Sensing
  Symposium 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logical Closed Loop: Uncovering Object Hallucinations in Large
  Vision-Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object hallucination has been an Achilles' heel which hinders the broader
applications of large vision-language models (LVLMs). Object hallucination
refers to the phenomenon that the LVLMs claim non-existent objects in the
image. To mitigate the object hallucinations, instruction tuning and external
model-based detection methods have been proposed, which either require
large-scare computational resources or depend on the detection result of
external models. However, there remains an under-explored field to utilize the
LVLM itself to alleviate object hallucinations. In this work, we adopt the
intuition that the LVLM tends to respond logically consistently for existent
objects but inconsistently for hallucinated objects. Therefore, we propose a
Logical Closed Loop-based framework for Object Hallucination Detection and
Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency
probing to raise questions with logical correlations, inquiring about
attributes from objects and vice versa. Whether their responses can form a
logical closed loop serves as an indicator of object hallucination. As a
plug-and-play method, it can be seamlessly applied to all existing LVLMs.
Comprehensive experiments conducted on three benchmarks across four LVLMs have
demonstrated significant improvements brought by our method, indicating its
effectiveness and generality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ACL 2024; 19 Pages, 15 Figures, 6 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Viewport Prediction for Volumetric Video Streaming by Exploring Video
  Saliency and Trajectory Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Li, Zhixin Li, Zhi Liu, Pengyuan Zhou, Richang Hong, Qiyue Li, Han Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric video, also known as hologram video, is a novel medium that
portrays natural content in Virtual Reality (VR), Augmented Reality (AR), and
Mixed Reality (MR). It is expected to be the next-gen video technology and a
prevalent use case for 5G and beyond wireless communication. Considering that
each user typically only watches a section of the volumetric video, known as
the viewport, it is essential to have precise viewport prediction for optimal
performance. However, research on this topic is still in its infancy. In the
end, this paper presents and proposes a novel approach, named Saliency and
Trajectory Viewport Prediction (STVP), which aims to improve the precision of
viewport prediction in volumetric video streaming. The STVP extensively
utilizes video saliency information and viewport trajectory. To our knowledge,
this is the first comprehensive study of viewport prediction in volumetric
video streaming. In particular, we introduce a novel sampling method, Uniform
Random Sampling (URS), to reduce computational complexity while still
preserving video features in an efficient manner. Then we present a saliency
detection technique that incorporates both spatial and temporal information for
detecting static, dynamic geometric, and color salient regions. Finally, we
intelligently fuse saliency and trajectory information to achieve more accurate
viewport prediction. We conduct extensive simulations to evaluate the
effectiveness of our proposed viewport prediction methods using
state-of-the-art volumetric video sequences. The experimental results show the
superiority of the proposed method over existing schemes. The dataset and
source code will be publicly accessible after acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAGhead: Fully Animate Gaussian Head from Monocular Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Xuan, Xinyang Li, Gongxin Yao, Shiwei Zhou, Donghui Sun, Xiaoxin Chen, Yu Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-fidelity reconstruction of 3D human avatars has a wild application in
visual reality. In this paper, we introduce FAGhead, a method that enables
fully controllable human portraits from monocular videos. We explicit the
traditional 3D morphable meshes (3DMM) and optimize the neutral 3D Gaussians to
reconstruct with complex expressions. Furthermore, we employ a novel
Point-based Learnable Representation Field (PLRF) with learnable Gaussian point
positions to enhance reconstruction performance. Meanwhile, to effectively
manage the edges of avatars, we introduced the alpha rendering to supervise the
alpha value of each pixel. Extensive experimental results on the open-source
datasets and our capturing datasets demonstrate that our approach is able to
generate high-fidelity 3D head avatars and fully control the expression and
pose of the virtual avatars, which is outperforming than existing works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Refer-and-Ground Multimodal Large Language Model for Biomedicine <span class="chip">MICCAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoshuang Huang, Haifeng Huang, Lingdong Shen, Yehui Yang, Fangxin Shang, Junwei Liu, Jia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of multimodal large language models (MLLMs),
especially their capabilities in visual chat through refer and ground
functionalities, their significance is increasingly recognized. However, the
biomedical field currently exhibits a substantial gap in this area, primarily
due to the absence of a dedicated refer and ground dataset for biomedical
images. To address this challenge, we devised the Med-GRIT-270k dataset. It
comprises 270k question-and-answer pairs and spans eight distinct medical
imaging modalities. Most importantly, it is the first dedicated to the
biomedical domain and integrating refer and ground conversations. The key idea
is to sample large-scale biomedical image-mask pairs from medical segmentation
datasets and generate instruction datasets from text using chatGPT.
Additionally, we introduce a Refer-and-Ground Multimodal Large Language Model
for Biomedicine (BiRD) by using this dataset and multi-task instruction
learning. Extensive experiments have corroborated the efficacy of the
Med-GRIT-270k dataset and the multi-modal, fine-grained interactive
capabilities of the BiRD model. This holds significant reference value for the
exploration and development of intelligent biomedical assistants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MICCAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProbRadarM3F: mmWave Radar based Human Skeletal Pose Estimation with
  Probability Map Guided Multi-Format Feature Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bing Zhu, Zixin He, Weiyi Xiong, Guanhua Ding, Jianan Liu, Tao Huang, Wei Chen, Wei Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Millimeter wave (mmWave) radar is a non-intrusive privacy and relatively
convenient and inexpensive device, which has been demonstrated to be applicable
in place of RGB cameras in human indoor pose estimation tasks. However, mmWave
radar relies on the collection of reflected signals from the target, and the
radar signals containing information is difficult to be fully applied. This has
been a long-standing hindrance to the improvement of pose estimation accuracy.
To address this major challenge, this paper introduces a probability map guided
multi-format feature fusion model, ProbRadarM3F. This is a novel radar feature
extraction framework using a traditional FFT method in parallel with a
probability map based positional encoding method. ProbRadarM3F fuses the
traditional heatmap features and the positional features, then effectively
achieves the estimation of 14 keypoints of the human body. Experimental
evaluation on the HuPR dataset proves the effectiveness of the model proposed
in this paper, outperforming other methods experimented on this dataset with an
AP of 69.9 %. The emphasis of our study is focusing on the position information
that is not exploited before in radar singal. This provides direction to
investigate other potential non-redundant information from mmWave rader.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text
  Cues <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Xie, Tao Zhou, Yi Zhou, Geng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly-supervised medical image segmentation is a challenging task that aims
to reduce the annotation cost while keep the segmentation performance. In this
paper, we present a novel framework, SimTxtSeg, that leverages simple text cues
to generate high-quality pseudo-labels and study the cross-modal fusion in
training segmentation models, simultaneously. Our contribution consists of two
key components: an effective Textual-to-Visual Cue Converter that produces
visual prompts from text prompts on medical images, and a text-guided
segmentation model with Text-Vision Hybrid Attention that fuses text and image
features. We evaluate our framework on two medical image segmentation tasks:
colonic polyp segmentation and MRI brain tumor segmentation, and achieve
consistent state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Knowledge <span class="highlight-title">Distillation</span> for <span class="highlight-title">Lightweight</span> Skin Cancer
  Classification: Balancing Accuracy and Computational Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niful Islam, Khan Md Hasib, Fahmida Akter Joti, Asif Karim, Sami Azam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer is a major concern to public health, accounting for one-third of
the reported cancers. If not detected early, the cancer has the potential for
severe consequences. Recognizing the critical need for effective skin cancer
classification, we address the limitations of existing models, which are often
too large to deploy in areas with limited computational resources. In response,
we present a knowledge distillation based approach for creating a lightweight
yet high-performing classifier. The proposed solution involves fusing three
models, namely ResNet152V2, ConvNeXtBase, and ViT Base, to create an effective
teacher model. The teacher model is then employed to guide a lightweight
student model of size 2.03 MB. This student model is further compressed to
469.77 KB using 16-bit quantization, enabling smooth incorporation into edge
devices. With six-stage image preprocessing, data augmentation, and a rigorous
ablation study, the model achieves an impressive accuracy of 98.75% on the
HAM10000 dataset and 98.94% on the Kaggle dataset in classifying benign and
malignant skin cancers. With its high accuracy and compact size, our model
appears to be a potential choice for accurate skin cancer classification,
particularly in resource-constrained settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlowVQA: Mapping Multimodal Logic in Visual Question Answering with
  Flowcharts <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal Gupta, Vivek Gupta, Dan Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for visual question answering lack in visual grounding
and complexity, particularly in evaluating spatial reasoning skills. We
introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of
visual question-answering multimodal language models in reasoning with
flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and
human-verified flowchart images from three distinct content sources, along with
22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,
including information localization, decision-making, and logical progression.
We conduct a thorough baseline evaluation on a suite of both open-source and
proprietary multimodal language models using various strategies, followed by an
analysis of directional bias. The results underscore the benchmark's potential
as a vital tool for advancing the field of multimodal modeling, providing a
focused and challenging environment for enhancing model performance in visual
and logical reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL 2024 (Findings), 21 pages, 7 figures, 9 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CSI4Free: GAN-Augmented mmWave CSI for Improved Pose Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nabeel Nisar Bhat, Rafael Berkvens, Jeroen Famaey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Joint Communication and Sensing (JC&S), has demonstrated
significant success, particularly in utilizing sub-6 GHz frequencies with
commercial-off-the-shelf (COTS) Wi-Fi devices for applications such as
localization, gesture recognition, and pose classification. Deep learning and
the existence of large public datasets has been pivotal in achieving such
results. However, at mmWave frequencies (30-300 GHz), which has shown potential
for more accurate sensing performance, there is a noticeable lack of research
in the domain of COTS Wi-Fi sensing. Challenges such as limited research
hardware, the absence of large datasets, limited functionality in COTS
hardware, and the complexities of data collection present obstacles to a
comprehensive exploration of this field. In this work, we aim to address these
challenges by developing a method that can generate synthetic mmWave channel
state information (CSI) samples. In particular, we use a generative adversarial
network (GAN) on an existing dataset, to generate 30,000 additional CSI
samples. The augmented samples exhibit a remarkable degree of consistency with
the original data, as indicated by the notably high GAN-train and GAN-test
scores. Furthermore, we integrate the augmented samples in training a pose
classification model. We observe that the augmented samples complement the real
data and improve the generalization of the classification model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ All-In-One Medical <span class="highlight-title">Image Restoration</span> via Task-Adaptive Routing <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwen Yang, Haowei Chen, Ziniu Qian, Yang Yi, Hui Zhang, Dan Zhao, Bingzheng Wei, Yan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although single-task medical image restoration (MedIR) has witnessed
remarkable success, the limited generalizability of these methods poses a
substantial obstacle to wider application. In this paper, we focus on the task
of all-in-one medical image restoration, aiming to address multiple distinct
MedIR tasks with a single universal model. Nonetheless, due to significant
differences between different MedIR tasks, training a universal model often
encounters task interference issues, where different tasks with shared
parameters may conflict with each other in the gradient update direction. This
task interference leads to deviation of the model update direction from the
optimal path, thereby affecting the model's performance. To tackle this issue,
we propose a task-adaptive routing strategy, allowing conflicting tasks to
select different network paths in spatial and channel dimensions, thereby
mitigating task interference. Experimental results demonstrate that our
proposed \textbf{A}ll-in-one \textbf{M}edical \textbf{I}mage
\textbf{R}estoration (\textbf{AMIR}) network achieves state-of-the-art
performance in three MedIR tasks: MRI super-resolution, CT denoising, and PET
synthesis, both in single-task and all-in-one settings. The code and data will
be available at
\href{https://github.com/Yaziwel/All-In-One-Medical-Image-Restoration-via-Task-Adaptive-Routing.git}{https://github.com/Yaziwel/AMIR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been early accepted by MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Backdoor Attacks against Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Liang, Jiawei Liang, Tianyu Pang, Chao Du, Aishan Liu, Ee-Chien Chang, Xiaochun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning enhances large vision-language models (LVLMs) but raises
security risks through potential backdoor attacks due to their openness.
Previous backdoor studies focus on enclosed scenarios with consistent training
and testing instructions, neglecting the practical domain gaps that could
affect attack effectiveness. This paper empirically examines the
generalizability of backdoor attacks during the instruction tuning of LVLMs for
the first time, revealing certain limitations of most backdoor strategies in
practical scenarios. We quantitatively evaluate the generalizability of six
typical backdoor attacks on image caption benchmarks across multiple LVLMs,
considering both visual and textual domain offsets. Our findings indicate that
attack generalizability is positively correlated with the backdoor trigger's
irrelevance to specific images/models and the preferential correlation of the
trigger pattern. Additionally, we modify existing backdoor attacks based on the
above key observations, demonstrating significant improvements in cross-domain
scenario generalizability (+86% attack success rate). Notably, even without
access to the instruction datasets, a multimodal instruction set can be
successfully poisoned with a very low poisoning rate (0.2%), achieving an
attack success rate of over 97%. This paper underscores that even simple
traditional backdoor strategies pose a serious threat to LVLMs, necessitating
more attention and in-depth research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Autoencoding of Dropout Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunta Maeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a generative model termed Deciphering Autoencoders. In this model,
we assign a unique random dropout pattern to each data point in the training
dataset and then train an autoencoder to reconstruct the corresponding data
point using this pattern as information to be encoded. Even if a completely
random dropout pattern is assigned to each data point regardless of their
similarities, a sufficiently large encoder can smoothly map them to a
low-dimensional latent space to reconstruct individual training data points.
During inference, using a dropout pattern different from those used during
training allows the model to function as a generator. Since the training of
Deciphering Autoencoders relies solely on reconstruction error, it offers more
stable training compared to other generative models. Despite their simplicity,
Deciphering Autoencoders show sampling quality comparable to DCGAN on the
CIFAR-10 dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EgoVideo: Exploring Egocentric Foundation Model and Downstream
  Adaptation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18070v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18070v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baoqi Pei, Guo Chen, Jilan Xu, Yuping He, Yicheng Liu, Kanghua Pan, Yifei Huang, Yali Wang, Tong Lu, Limin Wang, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we present our solutions to the EgoVis Challenges in CVPR
2024, including five tracks in the Ego4D challenge and three tracks in the
EPIC-Kitchens challenge. Building upon the video-language two-tower model and
leveraging our meticulously organized egocentric video data, we introduce a
novel foundation model called EgoVideo. This model is specifically designed to
cater to the unique characteristics of egocentric videos and provides strong
support for our competition submissions. In the Ego4D challenges, we tackle
various tasks including Natural Language Queries, Step Grounding, Moment
Queries, Short-term Object Interaction Anticipation, and Long-term Action
Anticipation. In addition, we also participate in the EPIC-Kitchens challenge,
where we engage in the Action Recognition, Multiple Instance Retrieval, and
Domain Adaptation for Action Recognition tracks. By adapting EgoVideo to these
diverse tasks, we showcase its versatility and effectiveness in different
egocentric video analysis scenarios, demonstrating the powerful representation
ability of EgoVideo as an egocentric foundation model. Our codebase and
pretrained models are publicly available at
https://github.com/OpenGVLab/EgoVideo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Champion solutions in the EgoVis CVPR 2024 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AlignIT: Enhancing Prompt Alignment in Customization of Text-to-Image
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Agarwal, Srikrishna Karanam, Balaji Vasan Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of customizing text-to-image diffusion models with
user-supplied reference images. Given new prompts, the existing methods can
capture the key concept from the reference images but fail to align the
generated image with the prompt. In this work, we seek to address this key
issue by proposing new methods that can easily be used in conjunction with
existing customization methods that optimize the embeddings/weights at various
intermediate stages of the text encoding process.
  The first contribution of this paper is a dissection of the various stages of
the text encoding process leading up to the conditioning vector for
text-to-image models. We take a holistic view of existing customization methods
and notice that key and value outputs from this process differs substantially
from their corresponding baseline (non-customized) models (e.g., baseline
stable diffusion). While this difference does not impact the concept being
customized, it leads to other parts of the generated image not being aligned
with the prompt. Further, we also observe that these keys and values allow
independent control various aspects of the final generation, enabling semantic
manipulation of the output. Taken together, the features spanning these keys
and values, serve as the basis for our next contribution where we fix the
aforementioned issues with existing methods. We propose a new post-processing
algorithm, AlignIT, that infuses the keys and values for the concept of
interest while ensuring the keys and values for all other tokens in the input
prompt are unchanged.
  Our proposed method can be plugged in directly to existing customization
methods, leading to a substantial performance improvement in the alignment of
the final result with the input prompt while retaining the customization
quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Character-Adapter: Prompt-Guided Region Control for High-Fidelity
  Character Customization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Ma, Wenting Xu, Jiji Tang, Qinfeng Jin, Rongsheng Zhang, Zeng Zhao, Changjie Fan, Zhipeng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customized image generation, which seeks to synthesize images with consistent
characters, holds significant relevance for applications such as storytelling,
portrait generation, and character design. However, previous approaches have
encountered challenges in preserving characters with high-fidelity consistency
due to inadequate feature extraction and concept confusion of reference
characters. Therefore, we propose Character-Adapter, a plug-and-play framework
designed to generate images that preserve the details of reference characters,
ensuring high-fidelity consistency. Character-Adapter employs prompt-guided
segmentation to ensure fine-grained regional features of reference characters
and dynamic region-level adapters to mitigate concept confusion. Extensive
experiments are conducted to validate the effectiveness of Character-Adapter.
Both quantitative and qualitative results demonstrate that Character-Adapter
achieves the state-of-the-art performance of consistent character generation,
with an improvement of 24.8% compared with other methods. Our code will be
released at https://github.com/Character-Adapter/Character-Adapte
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MolX: Enhancing Large Language Models for Molecular Learning with A
  Multi-Modal Extension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06777v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06777v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khiem Le, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, Nitesh V. Chawla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) with their strong task-handling
capabilities have shown remarkable advancements across a spectrum of fields,
moving beyond natural language understanding. However, their proficiency within
the chemistry domain remains restricted, especially in solving professional
molecule-related tasks. This challenge is attributed to their inherent
limitations in comprehending molecules using only common textual
representations, i.e., SMILES strings. In this study, we seek to enhance the
ability of LLMs to comprehend molecules by designing and equipping them with a
multi-modal external module, namely MolX. In particular, instead of directly
using a SMILES string to represent a molecule, we utilize specific encoders to
extract fine-grained features from both SMILES string and 2D molecular graph
representations for feeding into an LLM. Moreover, a human-defined molecular
fingerprint is incorporated to leverage its embedded domain knowledge. Then, to
establish an alignment between MolX and the LLM's textual input space, the
whole model in which the LLM is frozen, is pre-trained with a versatile
strategy including a diverse set of tasks. Extensive experimental evaluations
demonstrate that our proposed method only introduces a small number of
trainable parameters while outperforming baselines on various downstream
molecule-related tasks ranging from molecule-to-text translation to
retrosynthesis, with and without fine-tuning the LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyControl: Create Your Artwork with Versatile Control on Text-to-Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18958v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18958v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Sun, Yanchen Liu, Yinhao Tang, Wenjie Pei, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of text-to-image (T2I) generation has made significant progress in
recent years, largely driven by advancements in diffusion models. Linguistic
control enables effective content creation, but struggles with fine-grained
control over image generation. This challenge has been explored, to a great
extent, by incorporating additional user-supplied spatial conditions, such as
depth maps and edge maps, into pre-trained T2I models through extra encoding.
However, multi-control image synthesis still faces several challenges.
Specifically, current approaches are limited in handling free combinations of
diverse input control signals, overlook the complex relationships among
multiple spatial conditions, and often fail to maintain semantic alignment with
provided textual prompts. This can lead to suboptimal user experiences. To
address these challenges, we propose AnyControl, a multi-control image
synthesis framework that supports arbitrary combinations of diverse control
signals. AnyControl develops a novel Multi-Control Encoder that extracts a
unified multi-modal embedding to guide the generation process. This approach
enables a holistic understanding of user inputs, and produces high-quality,
faithful results under versatile control signals, as demonstrated by extensive
quantitative and qualitative evaluations. Our project page is available in
https://any-control.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Manipulate-Anything: Automating <span class="highlight-title">Real-World</span> Robots using Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, Ranjay Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale endeavors like RT-1 and widespread community efforts such as
Open-X-Embodiment have contributed to growing the scale of robot demonstration
data. However, there is still an opportunity to improve the quality, quantity,
and diversity of robot demonstration data. Although vision-language models have
been shown to automatically generate demonstration data, their utility has been
limited to environments with privileged state information, they require
hand-designed skills, and are limited to interactions with few object
instances. We propose Manipulate-Anything, a scalable automated generation
method for real-world robotic manipulation. Unlike prior work, our method can
operate in real-world environments without any privileged state information,
hand-designed skills, and can manipulate any static object. We evaluate our
method using two setups. First, Manipulate-Anything successfully generates
trajectories for all 5 real-world and 12 simulation tasks, significantly
outperforming existing methods like VoxPoser. Second, Manipulate-Anything's
demonstrations can train more robust behavior cloning policies than training
with human demonstrations, or from data generated by VoxPoser and
Code-As-Policies. We believe Manipulate-Anything can be the scalable method for
both generating data for robotics and solving novel tasks in a zero-shot
setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://robot-ma.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Epicardium Prompt-guided Real-time Cardiac Ultrasound Frame-to-volume
  Registration <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Lei, Jun Zhou, Jialun Pei, Baoliang Zhao, Yueming Jin, Yuen-Chun Jeremy Teoh, Jing Qin, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A comprehensive guidance view for cardiac interventional surgery can be
provided by the real-time fusion of the intraoperative 2D images and
preoperative 3D volume based on the ultrasound frame-to-volume registration.
However, cardiac ultrasound images are characterized by a low signal-to-noise
ratio and small differences between adjacent frames, coupled with significant
dimension variations between 2D frames and 3D volumes to be registered,
resulting in real-time and accurate cardiac ultrasound frame-to-volume
registration being a very challenging task. This paper introduces a lightweight
end-to-end Cardiac Ultrasound frame-to-volume Registration network, termed
CU-Reg. Specifically, the proposed model leverages epicardium prompt-guided
anatomical clues to reinforce the interaction of 2D sparse and 3D dense
features, followed by a voxel-wise local-global aggregation of enhanced
features, thereby boosting the cross-dimensional matching effectiveness of
low-quality ultrasound modalities. We further embed an inter-frame
discriminative regularization term within the hybrid supervised learning to
increase the distinction between adjacent slices in the same ultrasound volume
to ensure registration stability. Experimental results on the reprocessed CAMUS
dataset demonstrate that our CU-Reg surpasses existing methods in terms of
registration accuracy and efficiency, meeting the guidance requirements of
clinical cardiac interventional surgery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving the Inverse Problem of Electrocardiography for Cardiac Digital
  Twins: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Julia Camps, Blanca Rodriguez, Vicente Grau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac digital twins are personalized virtual representations used to
understand complex heart mechanisms. Solving the ECG inverse problem is crucial
for accurate virtual heart modelling, enabling the derivation of internal
electrical activity information from recorded surface potentials. Despite
challenges from cardiac complexity, noisy ECG data, and computational
efficiency, recent advancements hold significant promise for enhancing virtual
heart modelling, ultimately advancing precision medicine in cardiology. This
paper aims to provide a comprehensive review of the methods of solving ECG
inverse problem, the validation strategies, the clinical applications, and
future perspectives. For the computing methodologies, we broadly classify
state-of-the-art approaches into two categories: deterministic and
probabilistic methods, including conventional and deep learning-based
techniques. Integrating physics laws with deep learning models holds promise,
but challenges such as capturing dynamic electrophysiology accurately,
accessing accurate domain knowledge, and quantifying prediction uncertainty
persist. Integrating models into clinical workflows while ensuring
interpretability and usability for healthcare professionals is essential.
Overcoming these challenges will drive further research in cardiac digital
twins.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing the Power of MLLMs for Transferable Text-to-Image Person ReID <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04940v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04940v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image person re-identification (ReID) retrieves pedestrian images
according to textual descriptions. Manually annotating textual descriptions is
time-consuming, restricting the scale of existing datasets and therefore the
generalization ability of ReID models. As a result, we study the transferable
text-to-image ReID problem, where we train a model on our proposed large-scale
database and directly deploy it to various datasets for evaluation. We obtain
substantial training data via Multi-modal Large Language Models (MLLMs).
Moreover, we identify and address two key challenges in utilizing the obtained
textual descriptions. First, an MLLM tends to generate descriptions with
similar structures, causing the model to overfit specific sentence patterns.
Thus, we propose a novel method that uses MLLMs to caption images according to
various templates. These templates are obtained using a multi-turn dialogue
with a Large Language Model (LLM). Therefore, we can build a large-scale
dataset with diverse textual descriptions. Second, an MLLM may produce
incorrect descriptions. Hence, we introduce a novel method that automatically
identifies words in a description that do not correspond with the image. This
method is based on the similarity between one text and all patch token
embeddings in the image. Then, we mask these words with a larger probability in
the subsequent training epoch, alleviating the impact of noisy textual
descriptions. The experimental results demonstrate that our methods
significantly boost the direct transfer text-to-image ReID performance.
Benefiting from the pre-trained model weights, we also achieve state-of-the-art
performance in the traditional evaluation settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">103</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework
  for Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukmin Yun, Haokun Lin, Rusiru Thushara, Mohammad Qazim Bhat, Yongxin Wang, Zutao Jiang, Mingkai Deng, Jinhong Wang, Tianhua Tao, Junbo Li, Haonan Li, Preslav Nakov, Timothy Baldwin, Zhengzhong Liu, Eric P. Xing, Xiaodan Liang, Zhiqiang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have shown impressive success across
modalities such as image, video, and audio in a variety of understanding and
generation tasks. However, current MLLMs are surprisingly poor at understanding
webpage screenshots and generating their corresponding HTML code. To address
this problem, we propose Web2Code, a benchmark consisting of a new large-scale
webpage-to-code dataset for instruction tuning and an evaluation framework for
the webpage understanding and HTML code translation abilities of MLLMs. For
dataset construction, we leverage pretrained LLMs to enhance existing
webpage-to-code datasets as well as generate a diverse pool of new webpages
rendered into images. Specifically, the inputs are webpage images and
instructions, while the responses are the webpage's HTML code. We further
include diverse natural language QA pairs about the webpage content in the
responses to enable a more comprehensive understanding of the web content. To
evaluate model performance in these tasks, we develop an evaluation framework
for testing MLLMs' abilities in webpage understanding and web-to-code
generation. Extensive experiments show that our proposed dataset is beneficial
not only to our proposed tasks but also in the general visual domain, while
previous datasets result in worse performance. We hope our work will contribute
to the development of general MLLMs suitable for web-based content generation
and task automation. Our data and code will be available at
https://github.com/MBZUAI-LLM/web2code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website at https://mbzuai-llm.github.io/webpage2code/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaRA: Supercharging Robot Learning Data for Vision-Language Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, Michael S. Ryoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) equipped with extensive world knowledge and
strong reasoning skills can tackle diverse tasks across domains, often by
posing them as conversation-style instruction-response pairs. In this paper, we
propose LLaRA: Large Language and Robotics Assistant, a framework which
formulates robot action policy as conversations, and provides improved
responses when trained with auxiliary data that complements policy learning.
LLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity
to process state information as visual-textual prompts and generate optimal
policy decisions in text. To train such action policy VLMs, we first introduce
an automated pipeline to generate diverse high-quality robotics instruction
data from existing behavior cloning data. A VLM finetuned with the resulting
collection of datasets based on a conversation-style formulation tailored for
robotics tasks, can generate meaningful robot action policy decisions. Our
experiments across multiple simulated and real-world environments demonstrate
the state-of-the-art performance of the proposed LLaRA framework. The code,
datasets, and pretrained models are available at
https://github.com/LostXine/LLaRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProgressGym: Alignment with a Millennium of Moral Progress 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Qiu, Yang Zhang, Xuchuan Huang, Jasmine Xinze Li, Jiaming Ji, Yaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frontier AI systems, including large language models (LLMs), hold increasing
influence over the epistemology of human users. Such influence can reinforce
prevailing societal values, potentially contributing to the lock-in of
misguided moral beliefs and, consequently, the perpetuation of problematic
moral practices on a broad scale. We introduce progress alignment as a
technical solution to mitigate this imminent risk. Progress alignment
algorithms learn to emulate the mechanics of human moral progress, thereby
addressing the susceptibility of existing alignment methods to contemporary
moral blindspots. To empower research in progress alignment, we introduce
ProgressGym, an experimental framework allowing the learning of moral progress
mechanics from history, in order to facilitate future progress in real-world
moral decisions. Leveraging 9 centuries of historical text and 18 historical
LLMs, ProgressGym enables codification of real-world progress alignment
challenges into concrete benchmarks. Specifically, we introduce three core
challenges: tracking evolving values (PG-Follow), preemptively anticipating
moral progress (PG-Predict), and regulating the feedback loop between human and
AI value shifts (PG-Coevolve). Alignment methods without a temporal dimension
are inapplicable to these tasks. In response, we present lifelong and
extrapolative algorithms as baseline methods of progress alignment, and build
an open leaderboard soliciting novel algorithms and challenges. The framework
and the leaderboard are available at
https://github.com/PKU-Alignment/ProgressGym and
https://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI for Extreme Event Modeling and Understanding: Methodologies and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustau Camps-Valls, Miguel-Ángel Fernández-Torres, Kai-Hendrik Cohrs, Adrian Höhl, Andrea Castelletti, Aytac Pacal, Claire Robin, Francesco Martinuzzi, Ioannis Papoutsis, Ioannis Prapas, Jorge Pérez-Aracil, Katja Weigel, Maria Gonzalez-Calabuig, Markus Reichstein, Martin Rabel, Matteo Giuliani, Miguel Mahecha, Oana-Iuliana Popescu, Oscar J. Pellicer-Valero, Said Ouala, Sancho Salcedo-Sanz, Sebastian Sippel, Spyros Kondylatos, Tamara Happé, Tristan Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, artificial intelligence (AI) has deeply impacted various
fields, including Earth system sciences. Here, AI improved weather forecasting,
model emulation, parameter estimation, and the prediction of extreme events.
However, the latter comes with specific challenges, such as developing accurate
predictors from noisy, heterogeneous and limited annotated data. This paper
reviews how AI is being used to analyze extreme events (like floods, droughts,
wildfires and heatwaves), highlighting the importance of creating accurate,
transparent, and reliable AI models. We discuss the hurdles of dealing with
limited data, integrating information in real-time, deploying models, and
making them understandable, all crucial for gaining the trust of stakeholders
and meeting regulatory needs. We provide an overview of how AI can help
identify and explain extreme events more effectively, improving disaster
response and communication. We emphasize the need for collaboration across
different fields to create AI solutions that are practical, understandable, and
trustworthy for analyzing and predicting extreme events. Such collaborative
efforts aim to enhance disaster readiness and disaster risk reduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Molecular Facts: Desiderata for Decontextualization in LLM Fact
  Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anisha Gunjal, Greg Durrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic factuality verification of large language model (LLM) generations
is becoming more and more widely used to combat hallucinations. A major point
of tension in the literature is the granularity of this fact-checking: larger
chunks of text are hard to fact-check, but more atomic facts like propositions
may lack context to interpret correctly. In this work, we assess the role of
context in these atomic facts. We argue that fully atomic facts are not the
right representation, and define two criteria for molecular facts:
decontextuality, or how well they can stand alone, and minimality, or how
little extra information is added to achieve decontexuality. We quantify the
impact of decontextualization on minimality, then present a baseline
methodology for generating molecular facts automatically, aiming to add the
right amount of information. We compare against various methods of
decontextualization and find that molecular facts balance minimality with fact
verification accuracy in ambiguous settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danny Halawi, Alexander Wei, Eric Wallace, Tony T. Wang, Nika Haghtalab, Jacob Steinhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Black-box finetuning is an emerging interface for adapting state-of-the-art
language models to user needs. However, such access may also let malicious
actors undermine model safety. To demonstrate the challenge of defending
finetuning interfaces, we introduce covert malicious finetuning, a method to
compromise model safety via finetuning while evading detection. Our method
constructs a malicious dataset where every individual datapoint appears
innocuous, but finetuning on the dataset teaches the model to respond to
encoded harmful requests with encoded harmful responses. Applied to GPT-4, our
method produces a finetuned model that acts on harmful instructions 99% of the
time and avoids detection by defense mechanisms such as dataset inspection,
safety evaluations, and input/output classifiers. Our findings question whether
black-box finetuning access can be secured against sophisticated adversaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Electrostatics-based particle sampling and approximate inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A new particle-based sampling and approximate inference method, based on
electrostatics and Newton mechanics principles, is introduced with theoretical
ground, algorithm design and experimental validation. This method simulates an
interacting particle system (IPS) where particles, i.e. the freely-moving
negative charges and spatially-fixed positive charges with magnitudes
proportional to the target distribution, interact with each other via
attraction and repulsion induced by the resulting electric fields described by
Poisson's equation. The IPS evolves towards a steady-state where the
distribution of negative charges conforms to the target distribution. This
physics-inspired method offers deterministic, gradient-free sampling and
inference, achieving comparable performance as other particle-based and MCMC
methods in benchmark tasks of inferring complex densities, Bayesian logistic
regression and dynamical system identification. A discrete-time, discrete-space
algorithmic design, readily extendable to continuous time and space, is
provided for usage in more general inference problems occurring in
probabilistic machine learning scenarios such as Bayesian inference, generative
modelling, and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BMW Agents -- A Framework For Task Automation Through Multi-agent
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noel Crawford, Edward B. Duffy, Iman Evazzade, Torsten Foehr, Gregory Robbins, Debbrata Kumar Saha, Jiya Varma, Marcin Ziolkowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents driven by Large Language Models (LLMs) offer enormous
potential for automation. Early proof of this technology can be found in
various demonstrations of agents solving complex tasks, interacting with
external systems to augment their knowledge, and triggering actions. In
particular, workflows involving multiple agents solving complex tasks in a
collaborative fashion exemplify their capacity to operate in less strict and
less well-defined environments. Thus, a multi-agent approach has great
potential for serving as a backbone in many industrial applications, ranging
from complex knowledge retrieval systems to next generation robotic process
automation. Given the reasoning abilities within the current generation of
LLMs, complex processes require a multi-step approach that includes a plan of
well-defined and modular tasks. Depending on the level of complexity, these
tasks can be executed either by a single agent or a group of agents. In this
work, we focus on designing a flexible agent engineering framework with careful
attention to planning and execution, capable of handling complex use case
applications across various domains. The proposed framework provides
reliability in industrial applications and presents techniques to ensure a
scalable, flexible, and collaborative workflow for multiple autonomous agents
working together towards solving tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages. 21 PDF images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pairwise Difference Learning for Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Karim Belaid, Maximilian Rabus, Eyke Hüllermeier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pairwise difference learning (PDL) has recently been introduced as a new
meta-learning technique for regression. Instead of learning a mapping from
instances to outcomes in the standard way, the key idea is to learn a function
that takes two instances as input and predicts the difference between the
respective outcomes. Given a function of this kind, predictions for a query
instance are derived from every training example and then averaged. This paper
extends PDL toward the task of classification and proposes a meta-learning
technique for inducing a PDL classifier by solving a suitably defined (binary)
classification problem on a paired version of the original training data. We
analyze the performance of the PDL classifier in a large-scale empirical study
and find that it outperforms state-of-the-art methods in terms of prediction
performance. Last but not least, we provide an easy-to-use and publicly
available implementation of PDL in a Python package.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for
  Tool-Augmented Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zhang, Jing Chen, Junjie Wang, Yaxin Liu, Cheng Yang, Chufan Shi, Xinyu Zhu, Zihao Lin, Hanwen Wan, Yujiu Yang, Tetsuya Sakai, Tian Feng, Hayato Yamana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tool-augmented large language models (LLMs) are rapidly being integrated into
real-world applications. Due to the lack of benchmarks, the community still
needs to fully understand the hallucination issues within these models. To
address this challenge, we introduce a comprehensive diagnostic benchmark,
ToolBH. Specifically, we assess the LLM's hallucinations through two
perspectives: depth and breadth. In terms of depth, we propose a multi-level
diagnostic process, including (1) solvability detection, (2) solution planning,
and (3) missing-tool analysis. For breadth, we consider three scenarios based
on the characteristics of the toolset: missing necessary tools, potential
tools, and limited functionality tools. Furthermore, we developed seven tasks
and collected 700 evaluation samples through multiple rounds of manual
annotation. The results show the significant challenges presented by the ToolBH
benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve a
total score of 45.3 and 37.0, respectively, on a scale of 100. In this
benchmark, larger model parameters do not guarantee better performance; the
training data and response strategies also play a crucial role in tool-enhanced
LLM scenarios. Our diagnostic analysis indicates that the primary reason for
model errors lies in assessing task solvability. Additionally, open-weight
models suffer from performance drops with verbose replies, whereas proprietary
models excel with longer reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wavelets Are All You Need for Autoregressive Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wael Mattar, Idan Levy, Nir Sharon, Shai Dekel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we take a new approach to autoregressive image generation that
is based on two main ingredients. The first is wavelet image coding, which
allows to tokenize the visual details of an image from coarse to fine details
by ordering the information starting with the most significant bits of the most
significant wavelet coefficients. The second is a variant of a language
transformer whose architecture is re-designed and optimized for token sequences
in this 'wavelet language'. The transformer learns the significant statistical
correlations within a token sequence, which are the manifestations of
well-known correlations between the wavelet subbands at various resolutions. We
show experimental results with conditioning on the generation process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single Parent Family: A Spectrum of Family Members from a Single
  Pre-Trained Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Habib Hajimolahoseini, Mohammad Hassanpour, Foozhan Ataiefard, Boxing Chen, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel method of Progressive Low Rank Decomposition
(PLRD) tailored for the compression of large language models. Our approach
leverages a pre-trained model, which is then incrementally decompressed to
smaller sizes using progressively lower ranks. This method allows for
significant reductions in computational overhead and energy consumption, as
subsequent models are derived from the original without the need for retraining
from scratch. We detail the implementation of PLRD, which strategically
decreases the tensor ranks, thus optimizing the trade-off between model
performance and resource usage. The efficacy of PLRD is demonstrated through
extensive experiments showing that models trained with PLRD method on only 1B
tokens maintain comparable performance with traditionally trained models while
using 0.1% of the tokens. The versatility of PLRD is highlighted by its ability
to generate multiple model sizes from a single foundational model, adapting
fluidly to varying computational and memory budgets. Our findings suggest that
PLRD could set a new standard for the efficient scaling of LLMs, making
advanced AI more feasible on diverse platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Into the Unknown: Generating Geospatial Descriptions for New
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tzuf Paz-Argaman, John Palowitch, Sayali Kulkarni, Reut Tsarfaty, Jason Baldridge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Similar to vision-and-language navigation (VLN) tasks that focus on bridging
the gap between vision and language for embodied navigation, the new Rendezvous
(RVS) task requires reasoning over allocentric spatial relationships
(independent of the observer's viewpoint) using non-sequential navigation
instructions and maps. However, performance substantially drops in new
environments with no training data. Using opensource descriptions paired with
coordinates (e.g., Wikipedia) provides training data but suffers from limited
spatially-oriented text resulting in low geolocation resolution. We propose a
large-scale augmentation method for generating high-quality synthetic data for
new environments using readily available geospatial data. Our method constructs
a grounded knowledge-graph, capturing entity relationships. Sampled entities
and relations (`shop north of school') generate navigation instructions via (i)
generating numerous templates using context-free grammar (CFG) to embed
specific entities and relations; (ii) feeding the entities and relation into a
large language model (LLM) for instruction generation. A comprehensive
evaluation on RVS, showed that our approach improves the 100-meter accuracy by
45.83% on unseen environments. Furthermore, we demonstrate that models trained
with CFG-based augmentation achieve superior performance compared with those
trained with LLM-based augmentation, both in unseen and seen environments.
These findings suggest that the potential advantages of explicitly structuring
spatial information for text-based geospatial reasoning in previously unknown,
can unlock data-scarce scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text2Robot: Evolutionary Robot Design from Text Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan P. Ringel, Zachary S. Charlick, Jiaxun Liu, Boxi Xia, Boyuan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot design has traditionally been costly and labor-intensive. Despite
advancements in automated processes, it remains challenging to navigate a vast
design space while producing physically manufacturable robots. We introduce
Text2Robot, a framework that converts user text specifications and performance
preferences into physical quadrupedal robots. Within minutes, Text2Robot can
use text-to-3D models to provide strong initializations of diverse
morphologies. Within a day, our geometric processing algorithms and
body-control co-optimization produce a walking robot by explicitly considering
real-world electronics and manufacturability. Text2Robot enables rapid
prototyping and opens new opportunities for robot design with generative
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our project website is at: https://generalroboticslab.com/Text2Robot</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering the hidden core-periphery structure in hyperbolic networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imran Ansari, Pawanesh Yadav, Niteesh Sahni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The hyperbolic network models exhibit very fundamental and essential
features, like small-worldness, scale-freeness, high-clustering coefficient,
and community structure. In this paper, we comprehensively explore the presence
of an important feature, the core-periphery structure, in the hyperbolic
network models, which is often exhibited by real-world networks. We focused on
well-known hyperbolic models such as popularity-similarity optimization model
(PSO) and S1/H2 models and studied core-periphery structures using a
well-established method that is based on standard random walk Markov chain
model. The observed core-periphery centralization values indicate that the
core-periphery structure can be very pronounced under certain conditions. We
also validate our findings by statistically testing for the significance of the
observed core-periphery structure in the network geometry. This study extends
network science and reveals core-periphery insights applicable to various
domains, enhancing network performance and resiliency in transportation and
information systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From the Least to the Most: Building a Plug-and-Play Visual Reasoner via
  Data Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore multi-step reasoning in vision-language models (VLMs). The problem
is challenging, as reasoning data consisting of multiple steps of visual and
language processing are barely available. To overcome the challenge, we first
introduce a least-to-most visual reasoning paradigm, which interleaves steps of
decomposing a question into sub-questions and invoking external tools for
resolving sub-questions. Based on the paradigm, we further propose a novel data
synthesis approach that can automatically create questions and multi-step
reasoning paths for an image in a bottom-up manner. Our approach divides the
complex synthesis task into a few simple sub-tasks, and (almost entirely)
relies on open-sourced models to accomplish the sub-tasks. Therefore, the
entire synthesis process is reproducible and cost-efficient, and the
synthesized data is quality guaranteed. With the approach, we construct $50$k
visual reasoning examples. Then, we develop a visual reasoner through
supervised fine-tuning, which is capable of generally enhancing the reasoning
abilities of a wide range of existing VLMs in a plug-and-play fashion.
Extensive experiments indicate that the visual reasoner can consistently and
significantly improve four VLMs on four VQA benchmarks. Our code and dataset
are available at https://github.com/steven-ccq/VisualReasoner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupling General and Personalized Knowledge in Federated Learning via
  Additive and Low-Rank Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Wu, Xuefeng Liu, Jianwei Niu, Haolin Wang, Shaojie Tang, Guogang Zhu, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address data heterogeneity, the key strategy of Personalized Federated
Learning (PFL) is to decouple general knowledge (shared among clients) and
client-specific knowledge, as the latter can have a negative impact on
collaboration if not removed. Existing PFL methods primarily adopt a parameter
partitioning approach, where the parameters of a model are designated as one of
two types: parameters shared with other clients to extract general knowledge
and parameters retained locally to learn client-specific knowledge. However, as
these two types of parameters are put together like a jigsaw puzzle into a
single model during the training process, each parameter may simultaneously
absorb both general and client-specific knowledge, thus struggling to separate
the two types of knowledge effectively. In this paper, we introduce FedDecomp,
a simple but effective PFL paradigm that employs parameter additive
decomposition to address this issue. Instead of assigning each parameter of a
model as either a shared or personalized one, FedDecomp decomposes each
parameter into the sum of two parameters: a shared one and a personalized one,
thus achieving a more thorough decoupling of shared and personalized knowledge
compared to the parameter partitioning method. In addition, as we find that
retaining local knowledge of specific clients requires much lower model
capacity compared with general knowledge across all clients, we let the matrix
containing personalized parameters be low rank during the training process.
Moreover, a new alternating training strategy is proposed to further improve
the performance. Experimental results across multiple datasets and varying
degrees of data heterogeneity demonstrate that FedDecomp outperforms
state-of-the-art methods up to 4.9\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AuthAttLyzer-V2: Unveiling Code Authorship Attribution using Enhanced
  Ensemble Learning Models & Generating Benchmark Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhaskar Joshi, Sepideh HajiHossein Khani, Arash HabibiLashkari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source Code Authorship Attribution (SCAA) is crucial for software
classification because it provides insights into the origin and behavior of
software. By accurately identifying the author or group behind a piece of code,
experts can better understand the motivations and techniques of developers. In
the cybersecurity era, this attribution helps trace the source of malicious
software, identify patterns in the code that may indicate specific threat
actors or groups, and ultimately enhance threat intelligence and mitigation
strategies. This paper presents AuthAttLyzer-V2, a new source code feature
extractor for SCAA, focusing on lexical, semantic, syntactic, and N-gram
features. Our research explores author identification in C++ by examining
24,000 source code samples from 3,000 authors. Our methodology integrates
Random Forest, Gradient Boosting, and XGBoost models, enhanced with SHAP for
interpretability. The study demonstrates how ensemble models can effectively
discern individual coding styles, offering insights into the unique attributes
of code authorship. This approach is pivotal in understanding and interpreting
complex patterns in authorship attribution, especially for malware
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-tuning of Geospatial Foundation Models for Aboveground Biomass
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Muszynski, Levente Klein, Ademir Ferreira da Silva, Anjani Prasad Atluri, Carlos Gomes, Daniela Szwarcman, Gurkanwar Singh, Kewen Gu, Maciel Zortea, Naomi Simumba, Paolo Fraccaro, Shraddha Singh, Steve Meliksetian, Campbell Watson, Daiki Kimura, Harini Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global vegetation structure mapping is critical for understanding the global
carbon cycle and maximizing the efficacy of nature-based carbon sequestration
initiatives. Moreover, vegetation structure mapping can help reduce the impacts
of climate change by, for example, guiding actions to improve water security,
increase biodiversity and reduce flood risk. Global satellite measurements
provide an important set of observations for monitoring and managing
deforestation and degradation of existing forests, natural forest regeneration,
reforestation, biodiversity restoration, and the implementation of sustainable
agricultural practices. In this paper, we explore the effectiveness of
fine-tuning of a geospatial foundation model to estimate above-ground biomass
(AGB) using space-borne data collected across different eco-regions in Brazil.
The fine-tuned model architecture consisted of a Swin-B transformer as the
encoder (i.e., backbone) and a single convolutional layer for the decoder head.
All results were compared to a U-Net which was trained as the baseline model
Experimental results of this sparse-label prediction task demonstrate that the
fine-tuned geospatial foundation model with a frozen encoder has comparable
performance to a U-Net trained from scratch. This is despite the fine-tuned
model having 13 times less parameters requiring optimization, which saves both
time and compute resources. Further, we explore the transfer-learning
capabilities of the geospatial foundation models by fine-tuning on satellite
imagery with sparse labels from different eco-regions in Brazil.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Subtle Differences between Human and Model Languages Using
  Spectrum of Relative Likelihood 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xu, Yu Wang, Hao An, Zhichen Liu, Yongyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human and model-generated texts can be distinguished by examining the
magnitude of likelihood in language. However, it is becoming increasingly
difficult as language model's capabilities of generating human-like texts keep
evolving. This study provides a new perspective by using the relative
likelihood values instead of absolute ones, and extracting useful features from
the spectrum-view of likelihood for the human-model text detection task. We
propose a detection procedure with two classification methods, supervised and
heuristic-based, respectively, which results in competitive performances with
previous zero-shot detection methods and a new state-of-the-art on short-text
detection. Our method can also reveal subtle differences between human and
model languages, which find theoretical roots in psycholinguistics studies. Our
code is available at https://github.com/CLCS-SUSTech/FourierGPT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaDesigner: Advancing Artistic Typography through AI-Driven,
  User-Centric, and Multilingual WordArt Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Qi He, Wangmeng Xiang, Hanyuan Chen, Jin-Peng Lan, Xianhui Lin, Kang Zhu, Bin Luo, Yifeng Geng, Xuansong Xie, Alexander G. Hauptmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MetaDesigner revolutionizes artistic typography synthesis by leveraging the
strengths of Large Language Models (LLMs) to drive a design paradigm centered
around user engagement. At the core of this framework lies a multi-agent system
comprising the Pipeline, Glyph, and Texture agents, which collectively enable
the creation of customized WordArt, ranging from semantic enhancements to the
imposition of complex textures. MetaDesigner incorporates a comprehensive
feedback mechanism that harnesses insights from multimodal models and user
evaluations to refine and enhance the design process iteratively. Through this
feedback loop, the system adeptly tunes hyperparameters to align with
user-defined stylistic and thematic preferences, generating WordArt that not
only meets but exceeds user expectations of visual appeal and contextual
relevance. Empirical validations highlight MetaDesigner's capability to
effectively serve diverse WordArt applications, consistently producing
aesthetically appealing and context-sensitive results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 16 figures, Project:
  https://modelscope.cn/studios/WordArt/WordArt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YuLan: An Open-source Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Zhu, Kun Zhou, Kelong Mao, Wentong Chen, Yiding Sun, Zhipeng Chen, Qian Cao, Yihan Wu, Yushuo Chen, Feng Wang, Lei Zhang, Junyi Li, Xiaolei Wang, Lei Wang, Beichen Zhang, Zican Dong, Xiaoxue Cheng, Yuhan Chen, Xinyu Tang, Yupeng Hou, Qiangqiang Ren, Xincheng Pang, Shufang Xie, Wayne Xin Zhao, Zhicheng Dou, Jiaxin Mao, Yankai Lin, Ruihua Song, Jun Xu, Xu Chen, Rui Yan, Zhewei Wei, Di Hu, Wenbing Huang, Ze-Feng Gao, Yueguo Chen, Weizheng Lu, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have become the foundation of many applications,
leveraging their extensive capabilities in processing and understanding natural
language. While many open-source LLMs have been released with technical
reports, the lack of training details hinders further research and development.
This paper presents the development of YuLan, a series of open-source LLMs with
$12$ billion parameters. The base model of YuLan is pre-trained on
approximately $1.7$T tokens derived from a diverse corpus, including massive
English, Chinese, and multilingual texts. We design a three-stage pre-training
method to enhance YuLan's overall capabilities. Subsequent phases of training
incorporate instruction-tuning and human alignment, employing a substantial
volume of high-quality synthesized data. To facilitate the learning of complex
and long-tail knowledge, we devise a curriculum-learning framework throughout
across these stages, which helps LLMs learn knowledge in an easy-to-hard
manner. YuLan's training is finished on Jan, 2024 and has achieved performance
on par with state-of-the-art LLMs across various English and Chinese
benchmarks. This paper outlines a comprehensive technical roadmap for
developing LLMs from scratch. Our model and codes are available at
https://github.com/RUC-GSAI/YuLan-Chat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnomaLLMy -- Detecting anomalous tokens in black-box LLMs through
  low-confidence single-token predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waligóra Witold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces AnomaLLMy, a novel technique for the automatic
detection of anomalous tokens in black-box Large Language Models (LLMs) with
API-only access. Utilizing low-confidence single-token predictions as a
cost-effective indicator, AnomaLLMy identifies irregularities in model
behavior, addressing the issue of anomalous tokens degrading the quality and
reliability of models. Validated on the cl100k_base dataset, the token set of
GPT-4, AnomaLLMy detected 413 major and 65 minor anomalies, demonstrating the
method's efficiency with just \$24.39 spent in API credits. The insights from
this research are expected to be beneficial for enhancing the robustness of and
accuracy of LLMs, particularly in the development and assessment of tokenizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for
  Multi-hop Question Answering <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chu, Jingchang Chen, Qianglong Chen, Haotian Wang, Kun Zhu, Xiyuan Du, Weijiang Yu, Ming Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated strong reasoning capabilities.
Nevertheless, they still suffer from factual errors when tackling
knowledge-intensive tasks. Retrieval-augmented reasoning represents a promising
approach. However, significant challenges still persist, including inaccurate
and insufficient retrieval for complex questions, as well as difficulty in
integrating multi-source knowledge. To address this, we propose Beam
Aggregation Reasoning, BeamAggR, a reasoning framework for knowledge-intensive
multi-hop QA. BeamAggR explores and prioritizes promising answers at each hop
of question. Concretely, we parse the complex questions into trees, which
include atom and composite questions, followed by bottom-up reasoning. For
atomic questions, the LLM conducts reasoning on multi-source knowledge to get
answer candidates. For composite questions, the LLM combines beam candidates,
explores multiple reasoning paths through probabilistic aggregation, and
prioritizes the most promising trajectory. Extensive experiments on four
open-domain multi-hop reasoning datasets show that our method significantly
outperforms SOTA methods by 8.5%. Furthermore, our analysis reveals that
BeamAggR elicits better knowledge collaboration and answer aggregation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion Loss Attacking: Adversarial Attack Perception for Skeleton based
  on Multi-dimensional Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Liu, Qing Xu, Qijian Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attack on skeletal motion is a hot topic. However, existing
researches only consider part of dynamic features when measuring distance
between skeleton graph sequences, which results in poor imperceptibility. To
this end, we propose a novel adversarial attack method to attack action
recognizers for skeletal motions. Firstly, our method systematically proposes a
dynamic distance function to measure the difference between skeletal motions.
Meanwhile, we innovatively introduce emotional features for complementary
information. In addition, we use Alternating Direction Method of
Multipliers(ADMM) to solve the constrained optimization problem, which
generates adversarial samples with better imperceptibility to deceive the
classifiers. Experiments show that our method is effective on multiple action
classifiers and datasets. When the perturbation magnitude measured by l norms
is the same, the dynamic perturbations generated by our method are much lower
than that of other methods. What's more, we are the first to prove the
effectiveness of emotional features, and provide a new idea for measuring the
distance between skeletal motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fuzzy Logic Guided Reward Function Variation: An Oracle for Testing
  Reinforcement Learning Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Zhang, Haoyang Song, Qixin Wang, Yu Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has gained significant attention across various
domains. However, the increasing complexity of RL programs presents testing
challenges, particularly the oracle problem: defining the correctness of the RL
program. Conventional human oracles struggle to cope with the complexity,
leading to inefficiencies and potential unreliability in RL testing. To
alleviate this problem, we propose an automated oracle approach that leverages
RL properties using fuzzy logic. Our oracle quantifies an agent's behavioral
compliance with reward policies and analyzes its trend over training episodes.
It labels an RL program as "Buggy" if the compliance trend violates
expectations derived from RL characteristics. We evaluate our oracle on RL
programs with varying complexities and compare it with human oracles. Results
show that while human oracles perform well in simpler testing scenarios, our
fuzzy oracle demonstrates superior performance in complex environments. The
proposed approach shows promise in addressing the oracle problem for RL
testing, particularly in complex cases where manual testing falls short. It
offers a potential solution to improve the efficiency, reliability, and
scalability of RL program testing. This research takes a step towards automated
testing of RL programs and highlights the potential of fuzzy logic-based
oracles in tackling the oracle problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deceptive <span class="highlight-title">Diffusion</span>: Generating Synthetic Adversarial Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Beerens, Catherine F. Higham, Desmond J. Higham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the concept of deceptive diffusion -- training a generative AI
model to produce adversarial images. Whereas a traditional adversarial attack
algorithm aims to perturb an existing image to induce a misclassificaton, the
deceptive diffusion model can create an arbitrary number of new, misclassified
images that are not directly associated with training or test images. Deceptive
diffusion offers the possibility of strengthening defence algorithms by
providing adversarial training data at scale, including types of
misclassification that are otherwise difficult to find. In our experiments, we
also investigate the effect of training on a partially attacked data set. This
highlights a new type of vulnerability for generative diffusion models: if an
attacker is able to stealthily poison a portion of the training data, then the
resulting diffusion model will generate a similar proportion of misleading
outputs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Supervised Spatial-Temporal Normality Learning for Time Series
  Anomaly Detection <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Chen, Hongzuo Xu, Guansong Pang, Hezhe Qiao, Yuan Zhou, Mingsheng Shang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time Series Anomaly Detection (TSAD) finds widespread applications across
various domains such as financial markets, industrial production, and
healthcare. Its primary objective is to learn the normal patterns of time
series data, thereby identifying deviations in test samples. Most existing TSAD
methods focus on modeling data from the temporal dimension, while ignoring the
semantic information in the spatial dimension. To address this issue, we
introduce a novel approach, called Spatial-Temporal Normality learning (STEN).
STEN is composed of a sequence Order prediction-based Temporal Normality
learning (OTN) module that captures the temporal correlations within sequences,
and a Distance prediction-based Spatial Normality learning (DSN) module that
learns the relative spatial relations between sequences in a feature space. By
synthesizing these two modules, STEN learns expressive spatial-temporal
representations for the normal patterns hidden in the time series data.
Extensive experiments on five popular TSAD benchmarks show that STEN
substantially outperforms state-of-the-art competing methods. Our code is
available at https://github.com/mala-lab/STEN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures, accepted in ECML PKDD2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ xSemAD: Explainable Semantic Anomaly Detection in Event Logs Using
  Sequence-to-Sequence Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiran Busch, Timotheus Kampik, Henrik Leopold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The identification of undesirable behavior in event logs is an important
aspect of process mining that is often addressed by anomaly detection methods.
Traditional anomaly detection methods tend to focus on statistically rare
behavior and neglect the subtle difference between rarity and undesirability.
The introduction of semantic anomaly detection has opened a promising avenue by
identifying semantically deviant behavior. This work addresses a gap in
semantic anomaly detection, which typically indicates the occurrence of an
anomaly without explaining the nature of the anomaly. We propose xSemAD, an
approach that uses a sequence-to-sequence model to go beyond pure
identification and provides extended explanations. In essence, our approach
learns constraints from a given process model repository and then checks
whether these constraints hold in the considered event log. This approach not
only helps understand the specifics of the undesired behavior, but also
facilitates targeted corrective actions. Our experiments demonstrate that our
approach outperforms existing state-of-the-art semantic anomaly detection
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at BPM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure-aware World Model for Probe Guidance via Large-scale
  Self-supervised Pre-train 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojun Jiang, Meng Li, Zhenguo Sun, Ning Jia, Yu Sun, Shaqi Luo, Shiji Song, Gao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complex structure of the heart leads to significant challenges in
echocardiography, especially in acquisition cardiac ultrasound images.
Successful echocardiography requires a thorough understanding of the structures
on the two-dimensional plane and the spatial relationships between planes in
three-dimensional space. In this paper, we innovatively propose a large-scale
self-supervised pre-training method to acquire a cardiac structure-aware world
model. The core innovation lies in constructing a self-supervised task that
requires structural inference by predicting masked structures on a 2D plane and
imagining another plane based on pose transformation in 3D space. To support
large-scale pre-training, we collected over 1.36 million echocardiograms from
ten standard views, along with their 3D spatial poses. In the downstream probe
guidance task, we demonstrate that our pre-trained model consistently reduces
guidance errors across the ten most common standard views on the test set with
0.29 million samples from 74 routine clinical scans, indicating that
structure-aware pre-training benefits the scanning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Protein Representation Learning with Sequence Information Embedding:
  Does it Always Lead to a Better Performance? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Tan, Lirong Zheng, Bozitao Zhong, Liang Hong, Bingxin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has become a crucial tool in studying proteins. While the
significance of modeling protein structure has been discussed extensively in
the literature, amino acid types are typically included in the input as a
default operation for many inference tasks. This study demonstrates with
structure alignment task that embedding amino acid types in some cases may not
help a deep learning model learn better representation. To this end, we propose
ProtLOCA, a local geometry alignment method based solely on amino acid
structure representation. The effectiveness of ProtLOCA is examined by a global
structure-matching task on protein pairs with an independent test dataset based
on CATH labels. Our method outperforms existing sequence- and structure-based
representation learning methods by more quickly and accurately matching
structurally consistent protein domains. Furthermore, in local structure
pairing tasks, ProtLOCA for the first time provides a valid solution to
highlight common local structures among proteins with different overall
structures but the same function. This suggests a new possibility for using
deep learning methods to analyze protein structure to infer function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROS-LLM: A ROS framework for embodied AI with task feedback and
  structured reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher E. Mower, Yuhui Wan, Hongzhan Yu, Antoine Grosnit, Jonas Gonzalez-Billandon, Matthieu Zimmer, Jinlong Wang, Xinyu Zhang, Yao Zhao, Anbang Zhai, Puze Liu, Davide Tateo, Cesar Cadena, Marco Hutter, Jan Peters, Guangjian Tian, Yuzheng Zhuang, Kun Shao, Xingyue Quan, Jianye Hao, Jun Wang, Haitham Bou-Ammar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for intuitive robot programming by non-experts,
leveraging natural language prompts and contextual information from the Robot
Operating System (ROS). Our system integrates large language models (LLMs),
enabling non-experts to articulate task requirements to the system through a
chat interface. Key features of the framework include: integration of ROS with
an AI agent connected to a plethora of open-source and commercial LLMs,
automatic extraction of a behavior from the LLM output and execution of ROS
actions/services, support for three behavior modes (sequence, behavior tree,
state machine), imitation learning for adding new robot actions to the library
of possible actions, and LLM reflection via human and environment feedback.
Extensive experiments validate the framework, showcasing robustness,
scalability, and versatility in diverse scenarios, including long-horizon
tasks, tabletop rearrangements, and remote supervisory control. To facilitate
the adoption of our framework and support the reproduction of our results, we
have made our code open-source. You can access it at:
https://github.com/huawei-noah/HEBO/tree/master/ROSLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This document contains 26 pages and 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classical Bandit Algorithms for Entanglement Detection in Parameterized
  Qubit States 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharati. K, Vikesh Siddhu, Krishna Jagannathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entanglement is a key resource for a wide range of tasks in quantum
information and computing. Thus, verifying availability of this quantum
resource is essential. Extensive research on entanglement detection has led to
no-go theorems (Lu et al. [Phys. Rev. Lett., 116, 230501 (2016)]) that
highlight the need for full state tomography (FST) in the absence of adaptive
or joint measurements. Recent advancements, as proposed by Zhu, Teo, and
Englert [Phys. Rev. A, 81, 052339, 2010], introduce a single-parameter family
of entanglement witness measurements which are capable of conclusively
detecting certain entangled states and only resort to FST when all witness
measurements are inconclusive. We find a variety of realistic noisy two-qubit
quantum states $\mathcal{F}$ that yield conclusive results under this witness
family. We solve the problem of detecting entanglement among $K$ quantum states
in $\mathcal{F}$, of which $m$ states are entangled, with $m$ potentially
unknown. We recognize a structural connection of this problem to the Bad Arm
Identification problem in stochastic Multi-Armed Bandits (MAB). In contrast to
existing quantum bandit frameworks, we establish a new correspondence tailored
for entanglement detection and term it the $(m,K)$-quantum Multi-Armed Bandit.
We implement two well-known MAB policies for arbitrary states derived from
$\mathcal{F}$, present theoretical guarantees on the measurement/sample
complexity and demonstrate the practicality of the policies through numerical
simulations. More broadly, this paper highlights the potential for employing
classical machine learning techniques for quantum entanglement detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-Instruct: Generated Visual Instructions for Large Multimodal Model
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Liu, Xin Huang, Jinliang Zheng, Boxiao Liu, Jia Wang, Osamu Yoshie, Yu Liu, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces MM-Instruct, a large-scale dataset of diverse and
high-quality visual instruction data designed to enhance the
instruction-following capabilities of large multimodal models (LMMs). While
existing visual instruction datasets often focus on question-answering, they
struggle to generalize to broader application scenarios such as creative
writing, summarization, or image analysis. To address these limitations, we
propose a novel approach to constructing MM-Instruct that leverages the strong
instruction-following capabilities of existing LLMs to generate novel visual
instruction data from large-scale but conventional image captioning datasets.
MM-Instruct first leverages ChatGPT to automatically generate diverse
instructions from a small set of seed instructions through augmenting and
summarization. It then matches these instructions with images and uses an
open-sourced large language model (LLM) to generate coherent answers to the
instruction-image pairs. The LLM is grounded by the detailed text descriptions
of images in the whole answer generation process to guarantee the alignment of
the instruction data. Moreover, we introduce a benchmark based on the generated
instruction data to evaluate the instruction-following capabilities of existing
LMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5
model on the generated data, denoted as LLaVA-Instruct, which exhibits
significant improvements in instruction-following capabilities compared to
LLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models
are available at https://github.com/jihaonew/MM-Instruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset and models are available at
  https://github.com/jihaonew/MM-Instruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CUPID: Improving Battle Fairness and Position Satisfaction in Online
  MOBA Games with a Re-matchmaking System <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Fan, Chaoyun Zhang, Kai Wang, Yingjie Li, Junyang Chen, Zenglin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multiplayer online battle arena (MOBA) genre has gained significant
popularity and economic success, attracting considerable research interest
within the Human-Computer Interaction community. Enhancing the gaming
experience requires a deep understanding of player behavior, and a crucial
aspect of MOBA games is matchmaking, which aims to assemble teams of comparable
skill levels. However, existing matchmaking systems often neglect important
factors such as players' position preferences and team assignment, resulting in
imbalanced matches and reduced player satisfaction. To address these
limitations, this paper proposes a novel framework called CUPID, which
introduces a novel process called ``re-matchmaking'' to optimize team and
position assignments to improve both fairness and player satisfaction. CUPID
incorporates a pre-filtering step to ensure a minimum level of matchmaking
quality, followed by a pre-match win-rate prediction model that evaluates the
fairness of potential assignments. By simultaneously considering players'
position satisfaction and game fairness, CUPID aims to provide an enhanced
matchmaking experience. Extensive experiments were conducted on two
large-scale, real-world MOBA datasets to validate the effectiveness of CUPID.
The results surpass all existing state-of-the-art baselines, with an average
relative improvement of 7.18% in terms of win prediction accuracy. Furthermore,
CUPID has been successfully deployed in a popular online mobile MOBA game. The
deployment resulted in significant improvements in match fairness and player
satisfaction, as evidenced by critical Human-Computer Interaction (HCI) metrics
covering usability, accessibility, and engagement, observed through A/B
testing. To the best of our knowledge, CUPID is the first re-matchmaking system
designed specifically for large-scale MOBA games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, accepted by CSCW 24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification in Large Language Models Through Convex Hull
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ferhat Ozgur Catak, Murat Kuzlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification approaches have been more critical in large
language models (LLMs), particularly high-risk applications requiring reliable
outputs. However, traditional methods for uncertainty quantification, such as
probabilistic models and ensemble techniques, face challenges when applied to
the complex and high-dimensional nature of LLM-generated outputs. This study
proposes a novel geometric approach to uncertainty quantification using convex
hull analysis. The proposed method leverages the spatial properties of response
embeddings to measure the dispersion and variability of model outputs. The
prompts are categorized into three types, i.e., `easy', `moderate', and
`confusing', to generate multiple responses using different LLMs at varying
temperature settings. The responses are transformed into high-dimensional
embeddings via a BERT model and subsequently projected into a two-dimensional
space using Principal Component Analysis (PCA). The Density-Based Spatial
Clustering of Applications with Noise (DBSCAN) algorithm is utilized to cluster
the embeddings and compute the convex hull for each selected cluster. The
experimental results indicate that the uncertainty of the model for LLMs
depends on the prompt complexity, the model, and the temperature setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Differentiable Approach to Multi-scale Brain Modeling <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoming Wang, Muyang Lyu, Tianqiu Zhang, Sichao He, Si Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a multi-scale differentiable brain modeling workflow utilizing
BrainPy, a unique differentiable brain simulator that combines accurate brain
simulation with powerful gradient-based optimization. We leverage this
capability of BrainPy across different brain scales. At the single-neuron
level, we implement differentiable neuron models and employ gradient methods to
optimize their fit to electrophysiological data. On the network level, we
incorporate connectomic data to construct biologically constrained network
models. Finally, to replicate animal behavior, we train these models on
cognitive tasks using gradient-based learning rules. Experiments demonstrate
that our approach achieves superior performance and speed in fitting
generalized leaky integrate-and-fire and Hodgkin-Huxley single neuron models.
Additionally, training a biologically-informed network of excitatory and
inhibitory spiking neurons on working memory tasks successfully replicates
observed neural activity and synaptic weight distributions. Overall, our
differentiable multi-scale simulation approach offers a promising tool to
bridge neuroscience data across electrophysiological, anatomical, and
behavioral scales.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2nd Differentiable Almost Everything Workshop at ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DISCO: <span class="highlight-title">Efficient</span> <span class="highlight-title">Diffusion</span> Solver for Large-Scale Combinatorial
  Optimization Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexiong Yu, Hang Zhao, Yuhang Huang, Renjiao Yi, Kai Xu, Chenyang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial Optimization (CO) problems are fundamentally crucial in
numerous practical applications across diverse industries, characterized by
entailing enormous solution space and demanding time-sensitive response.
Despite significant advancements made by recent neural solvers, their limited
expressiveness does not conform well to the multi-modal nature of CO
landscapes. While some research has pivoted towards diffusion models, they
require simulating a Markov chain with many steps to produce a sample, which is
time-consuming and does not meet the efficiency requirement of real
applications, especially at scale. We propose DISCO, an efficient DIffusion
Solver for Combinatorial Optimization problems that excels in both solution
quality and inference speed. DISCO's efficacy is two-pronged: Firstly, it
achieves rapid denoising of solutions through an analytically solvable form,
allowing for direct sampling from the solution space with very few reverse-time
steps, thereby drastically reducing inference time. Secondly, DISCO enhances
solution quality by restricting the sampling space to a more constrained,
meaningful domain guided by solution residues, while still preserving the
inherent multi-modality of the output probabilistic distributions. DISCO
achieves state-of-the-art results on very large Traveling Salesman Problems
with 10000 nodes and challenging Maximal Independent Set benchmarks, with its
per-instance denoising time up to 44.8 times faster. Through further combining
a divide-and-conquer strategy, DISCO can be generalized to solve
arbitrary-scale problem instances off the shelf, even outperforming models
trained specifically on corresponding scales.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Fusion Model for Brain Tumor Classification Using Fine-Grained
  Gradient Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niful Islam, Mohaiminul Islam Bhuiyan, Jarin Tasnim Raya, Nur Shazwani Kamarudin, Khan Md Hasib, M. F. Mridha, Dewan Md. Farid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumors are one of the most common diseases that lead to early death if
not diagnosed at an early stage. Traditional diagnostic approaches are
extremely time-consuming and prone to errors. In this context, computer
vision-based approaches have emerged as an effective tool for accurate brain
tumor classification. While some of the existing solutions demonstrate
noteworthy accuracy, the models become infeasible to deploy in areas where
computational resources are limited. This research addresses the need for
accurate and fast classification of brain tumors with a priority of deploying
the model in technologically underdeveloped regions. The research presents a
novel architecture for precise brain tumor classification fusing pretrained
ResNet152V2 and modified VGG16 models. The proposed architecture undergoes a
diligent fine-tuning process that ensures fine gradients are preserved in deep
neural networks, which are essential for effective brain tumor classification.
The proposed solution incorporates various image processing techniques to
improve image quality and achieves an astounding accuracy of 98.36% and 98.04%
in Figshare and Kaggle datasets respectively. This architecture stands out for
having a streamlined profile, with only 2.8 million trainable parameters. We
have leveraged 8-bit quantization to produce a model of size 73.881 MB,
significantly reducing it from the previous size of 289.45 MB, ensuring smooth
deployment in edge devices even in resource-constrained areas. Additionally,
the use of Grad-CAM improves the interpretability of the model, offering
insightful information regarding its decision-making process. Owing to its high
discriminative ability, this model can be a reliable option for accurate brain
tumor classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Radiological Diagnosis: A Collaborative Approach Integrating
  AI and Human Expertise for Visual Miss Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Awasthi, Ngan Le, Zhigang Deng, Carol C. Wu, Hien Van Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-AI collaboration to identify and correct perceptual errors in chest
radiographs has not been previously explored. This study aimed to develop a
collaborative AI system, CoRaX, which integrates eye gaze data and radiology
reports to enhance diagnostic accuracy in chest radiology by pinpointing
perceptual errors and refining the decision-making process. Using public
datasets REFLACX and EGD-CXR, the study retrospectively developed CoRaX,
employing a large multimodal model to analyze image embeddings, eye gaze data,
and radiology reports. The system's effectiveness was evaluated based on its
referral-making process, the quality of referrals, and performance in
collaborative diagnostic settings. CoRaX was tested on a simulated error
dataset of 271 samples with 28% (93 of 332) missed abnormalities. The system
corrected 21% (71 of 332) of these errors, leaving 7% (22 of 312) unresolved.
The Referral-Usefulness score, indicating the accuracy of predicted regions for
all true referrals, was 0.63 (95% CI 0.59, 0.68). The Total-Usefulness score,
reflecting the diagnostic accuracy of CoRaX's interactions with radiologists,
showed that 84% (237 of 280) of these interactions had a score above 0.40. In
conclusion, CoRaX efficiently collaborates with radiologists to address
perceptual errors across various abnormalities, with potential applications in
the education and training of novice radiologists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review in Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MimicMotion: High-Quality Human Motion Video Generation with
  Confidence-aware Pose Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, Fangyuan Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, generative artificial intelligence has achieved significant
advancements in the field of image generation, spawning a variety of
applications. However, video generation still faces considerable challenges in
various aspects, such as controllability, video length, and richness of
details, which hinder the application and popularization of this technology. In
this work, we propose a controllable video generation framework, dubbed
MimicMotion, which can generate high-quality videos of arbitrary length
mimicking specific motion guidance. Compared with previous methods, our
approach has several highlights. Firstly, we introduce confidence-aware pose
guidance that ensures high frame quality and temporal smoothness. Secondly, we
introduce regional loss amplification based on pose confidence, which
significantly reduces image distortion. Lastly, for generating long and smooth
videos, we propose a progressive latent fusion strategy. By this means, we can
produce videos of arbitrary length with acceptable resource consumption. With
extensive experiments and user studies, MimicMotion demonstrates significant
improvements over previous approaches in various aspects. Detailed results and
comparisons are available on our project page:
https://tencent.github.io/MimicMotion .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Function+Data Flow: A Framework to Specify Machine Learning Pipelines
  for Digital Twinning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo de Conto, Blaise Genest, Arvind Easwaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of digital twins (DTs) for physical systems increasingly
leverages artificial intelligence (AI), particularly for combining data from
different sources or for creating computationally efficient, reduced-dimension
models. Indeed, even in very different application domains, twinning employs
common techniques such as model order reduction and modelization with hybrid
data (that is, data sourced from both physics-based models and sensors).
Despite this apparent generality, current development practices are ad-hoc,
making the design of AI pipelines for digital twinning complex and
time-consuming. Here we propose Function+Data Flow (FDF), a domain-specific
language (DSL) to describe AI pipelines within DTs. FDF aims to facilitate the
design and validation of digital twins. Specifically, FDF treats functions as
first-class citizens, enabling effective manipulation of models learned with
AI. We illustrate the benefits of FDF on two concrete use cases from different
domains: predicting the plastic strain of a structure and modeling the
electromagnetic behavior of a bearing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, to be published in AIware'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ACES: Automatic Cohort Extraction System for Event-Stream Datasets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Xu, Jack Gallifant, Alistair E. W. Johnson, Matthew B. A. McDermott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reproducibility remains a significant challenge in machine learning (ML) for
healthcare. In this field, datasets, model pipelines, and even task/cohort
definitions are often private, leading to a significant barrier in sharing,
iterating, and understanding ML results on electronic health record (EHR)
datasets. In this paper, we address a significant part of this problem by
introducing the Automatic Cohort Extraction System for Event-Stream Datasets
(ACES). This tool is designed to simultaneously simplify the development of
task/cohorts for ML in healthcare and enable the reproduction of these cohorts,
both at an exact level for single datasets and at a conceptual level across
datasets. To accomplish this, ACES provides (1) a highly intuitive and
expressive configuration language for defining both dataset-specific concepts
and dataset-agnostic inclusion/exclusion criteria, and (2) a pipeline to
automatically extract patient records that meet these defined criteria from
real-world data. ACES can be automatically applied to any dataset in either the
Medical Event Data Standard (MEDS) or EventStreamGPT (ESGPT) formats, or to
*any* dataset for which the necessary task-specific predicates can be extracted
in an event-stream form. ACES has the potential to significantly lower the
barrier to entry for defining ML tasks, redefine the way researchers interact
with EHR datasets, and significantly improve the state of reproducibility for
ML studies in this modality. ACES is available at
https://github.com/justin13601/aces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For ACES Online Documentation, see
  https://eventstreamaces.readthedocs.io/en/latest/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CANDY: A Benchmark for Continuous Approximate Nearest Neighbor Search
  with Dynamic Data Ingestion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianzhi Zeng, Zhuoyan Wu, Xinjing Hu, Xuanhua Shi, Shixuan Sun, Shuhao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate K Nearest Neighbor (AKNN) algorithms play a pivotal role in
various AI applications, including information retrieval, computer vision, and
natural language processing. Although numerous AKNN algorithms and benchmarks
have been developed recently to evaluate their effectiveness, the dynamic
nature of real-world data presents significant challenges that existing
benchmarks fail to address. Traditional benchmarks primarily assess retrieval
effectiveness in static contexts and often overlook update efficiency, which is
crucial for handling continuous data ingestion. This limitation results in an
incomplete assessment of an AKNN algorithms ability to adapt to changing data
patterns, thereby restricting insights into their performance in dynamic
environments. To address these gaps, we introduce CANDY, a benchmark tailored
for Continuous Approximate Nearest Neighbor Search with Dynamic Data Ingestion.
CANDY comprehensively assesses a wide range of AKNN algorithms, integrating
advanced optimizations such as machine learning-driven inference to supplant
traditional heuristic scans, and improved distance computation methods to
reduce computational overhead. Our extensive evaluations across diverse
datasets demonstrate that simpler AKNN baselines often surpass more complex
alternatives in terms of recall and latency. These findings challenge
established beliefs about the necessity of algorithmic complexity for high
performance. Furthermore, our results underscore existing challenges and
illuminate future research opportunities. We have made the datasets and
implementation methods available at: https://github.com/intellistream/candy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing and Evaluating Multi-Chatbot Interface for Human-AI
  Communication: Preliminary Findings from a Persuasion Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sion Yoon, Tae Eun Kim, Yoo Jung Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dynamics of human-AI communication have been reshaped by language models
such as ChatGPT. However, extant research has primarily focused on dyadic
communication, leaving much to be explored regarding the dynamics of human-AI
communication in group settings. The availability of multiple language model
chatbots presents a unique opportunity for scholars to better understand the
interaction between humans and multiple chatbots. This study examines the
impact of multi-chatbot communication in a specific persuasion setting:
promoting charitable donations. We developed an online environment that enables
multi-chatbot communication and conducted a pilot experiment utilizing two
GPT-based chatbots, Save the Children and UNICEF chatbots, to promote
charitable donations. In this study, we present our development process of the
multi-chatbot interface and present preliminary findings from a pilot
experiment. Analysis of qualitative and quantitative feedback are presented,
and limitations are addressed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Human Preferences: Exploring Reinforcement Learning Trajectory
  Evaluation and Improvement through LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichao Shen, Tianchen Zhu, Qingyun Sun, Shiqi Gao, Jianxin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) faces challenges in evaluating policy
trajectories within intricate game tasks due to the difficulty in designing
comprehensive and precise reward functions. This inherent difficulty curtails
the broader application of RL within game environments characterized by diverse
constraints. Preference-based reinforcement learning (PbRL) presents a
pioneering framework that capitalizes on human preferences as pivotal reward
signals, thereby circumventing the need for meticulous reward engineering.
However, obtaining preference data from human experts is costly and
inefficient, especially under conditions marked by complex constraints. To
tackle this challenge, we propose a LLM-enabled automatic preference generation
framework named LLM4PG , which harnesses the capabilities of large language
models (LLMs) to abstract trajectories, rank preferences, and reconstruct
reward functions to optimize conditioned policies. Experiments on tasks with
complex language constraints demonstrated the effectiveness of our LLM-enabled
reward functions, accelerating RL convergence and overcoming stagnation caused
by slow or absent progress under original reward structures. This approach
mitigates the reliance on specialized human knowledge and demonstrates the
potential of LLMs to enhance RL's effectiveness in complex environments in the
wild.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking Varied Perspectives: A Persona-Based Multi-Agent Framework
  with Debate-Driven Text Planning for Argument Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Hu, Hou Pong Chan, Jing Li, Yu Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing persuasive arguments is a challenging task for both humans and
machines. It entails incorporating high-level beliefs from various perspectives
on the topic, along with deliberate reasoning and planning to construct a
coherent narrative. Current language models often generate surface tokens
autoregressively, lacking explicit integration of these underlying controls,
resulting in limited output diversity and coherence. In this work, we propose a
persona-based multi-agent framework for argument writing. Inspired by the human
debate, we first assign each agent a persona representing its high-level
beliefs from a unique perspective, and then design an agent interaction process
so that the agents can collaboratively debate and discuss the idea to form an
overall plan for argument writing. Such debate process enables fluid and
nonlinear development of ideas. We evaluate our framework on argumentative
essay writing. The results show that our framework can generate more diverse
and persuasive arguments through both automatic and human evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precision matters: Precision-aware ensemble for weakly supervised
  semantic segmentation <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsung Park, Hyunjung Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly Supervised Semantic Segmentation (WSSS) employs weak supervision, such
as image-level labels, to train the segmentation model. Despite the impressive
achievement in recent WSSS methods, we identify that introducing weak labels
with high mean Intersection of Union (mIoU) does not guarantee high
segmentation performance. Existing studies have emphasized the importance of
prioritizing precision and reducing noise to improve overall performance. In
the same vein, we propose ORANDNet, an advanced ensemble approach tailored for
WSSS. ORANDNet combines Class Activation Maps (CAMs) from two different
classifiers to increase the precision of pseudo-masks (PMs). To further
mitigate small noise in the PMs, we incorporate curriculum learning. This
involves training the segmentation model initially with pairs of smaller-sized
images and corresponding PMs, gradually transitioning to the original-sized
pairs. By combining the original CAMs of ResNet-50 and ViT, we significantly
improve the segmentation performance over the single-best model and the naive
ensemble model, respectively. We further extend our ensemble method to CAMs
from AMN (ResNet-like) and MCTformer (ViT-like) models, achieving performance
benefits in advanced WSSS models. It highlights the potential of our ORANDNet
as a final add-on module for WSSS models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, accepted in AAAI 2024 Edge Intelligence Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Video Compression using Pixel Shift Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hitesh Saai Mananchery Panneerselvam, Smit Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Video comprises approximately ~85\% of all internet traffic, but video
encoding/compression is being historically done with hard coded rules, which
has worked well but only to a certain limit. We have seen a surge in video
compression algorithms using ML-based models in the last few years and many of
them have outperformed several legacy codecs. The models range from encoding
video end to end using an ML approach or replacing some intermediate steps in
legacy codecs using ML models to increase the efficiency of those steps.
  Optimizing video storage is an essential aspect of video processing, so we
are proposing one of the possible approaches to achieve it is by avoiding
redundant data at each frame. In this paper, we want to introduce the approach
of redundancies removal in subsequent frames for a given video as a main
approach for video compression. We call this method Redundancy Removal using
Shift (R\textsuperscript2S). This method can be utilized across various Machine
Learning model algorithms, and make the compression more accessible and
adaptable. In this study, we have utilized a computer vision-based pixel point
tracking method to identify redundant pixels to encode video for optimal
storage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safety through feedback in Constrained RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Reddy Chirra, Pradeep Varakantham, Praveen Paruchuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In safety-critical RL settings, the inclusion of an additional cost function
is often favoured over the arduous task of modifying the reward function to
ensure the agent's safe behaviour. However, designing or evaluating such a cost
function can be prohibitively expensive. For instance, in the domain of
self-driving, designing a cost function that encompasses all unsafe behaviours
(e.g. aggressive lane changes) is inherently complex. In such scenarios, the
cost function can be learned from feedback collected offline in between
training rounds. This feedback can be system generated or elicited from a human
observing the training process. Previous approaches have not been able to scale
to complex environments and are constrained to receiving feedback at the state
level which can be expensive to collect. To this end, we introduce an approach
that scales to more complex domains and extends to beyond state-level feedback,
thus, reducing the burden on the evaluator. Inferring the cost function in such
settings poses challenges, particularly in assigning credit to individual
states based on trajectory-level feedback. To address this, we propose a
surrogate objective that transforms the problem into a state-level supervised
classification task with noisy labels, which can be solved efficiently.
Additionally, it is often infeasible to collect feedback on every trajectory
generated by the agent, hence, two fundamental questions arise: (1) Which
trajectories should be presented to the human? and (2) How many trajectories
are necessary for effective learning? To address these questions, we introduce
\textit{novelty-based sampling} that selectively involves the evaluator only
when the the agent encounters a \textit{novel} trajectory. We showcase the
efficiency of our method through experimentation on several benchmark Safety
Gymnasium environments and realistic self-driving scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Driven Lipschitz Continuity: A Cost-Effective Approach to Improve
  Adversarial Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erh-Chung Chen, Pin-Yu Chen, I-Hsin Chung, Che-Rung Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The security and robustness of deep neural networks (DNNs) have become
increasingly concerning. This paper aims to provide both a theoretical
foundation and a practical solution to ensure the reliability of DNNs. We
explore the concept of Lipschitz continuity to certify the robustness of DNNs
against adversarial attacks, which aim to mislead the network with adding
imperceptible perturbations into inputs. We propose a novel algorithm that
remaps the input domain into a constrained range, reducing the Lipschitz
constant and potentially enhancing robustness. Unlike existing adversarially
trained models, where robustness is enhanced by introducing additional examples
from other datasets or generative models, our method is almost cost-free as it
can be integrated with existing models without requiring re-training.
Experimental results demonstrate the generalizability of our method, as it can
be combined with various models and achieve enhancements in robustness.
Furthermore, our method achieves the best robust accuracy for CIFAR10,
CIFAR100, and ImageNet datasets on the RobustBench leaderboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Data Quality Dimensions and Tools for Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Zhou, Fengjiao Tu, Kewei Sha, Junhua Ding, Haihua Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) technologies have become substantial in practically all
aspects of our society, and data quality (DQ) is critical for the performance,
fairness, robustness, safety, and scalability of ML models. With the large and
complex data in data-centric AI, traditional methods like exploratory data
analysis (EDA) and cross-validation (CV) face challenges, highlighting the
importance of mastering DQ tools. In this survey, we review 17 DQ evaluation
and improvement tools in the last 5 years. By introducing the DQ dimensions,
metrics, and main functions embedded in these tools, we compare their strengths
and limitations and propose a roadmap for developing open-source DQ tools for
ML. Based on the discussions on the challenges and emerging trends, we further
highlight the potential applications of large language models (LLMs) and
generative AI in DQ evaluation and improvement for ML. We believe this
comprehensive survey can enhance understanding of DQ in ML and could drive
progress in data-centric AI. A complete list of the literature investigated in
this survey is available on GitHub at:
https://github.com/haihua0913/awesome-dq4ml.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by The 6th IEEE International Conference
  on Artificial Intelligence Testing (IEEE AITest 2024) as an invited paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Data Integration for Precision Oncology: Challenges and
  Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huajun Zhou, Fengtao Zhou, Chenyu Zhao, Yingxue Xu, Luyang Luo, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The essence of precision oncology lies in its commitment to tailor targeted
treatments and care measures to each patient based on the individual
characteristics of the tumor. The inherent heterogeneity of tumors necessitates
gathering information from diverse data sources to provide valuable insights
from various perspectives, fostering a holistic comprehension of the tumor.
Over the past decade, multimodal data integration technology for precision
oncology has made significant strides, showcasing remarkable progress in
understanding the intricate details within heterogeneous data modalities. These
strides have exhibited tremendous potential for improving clinical
decision-making and model interpretation, contributing to the advancement of
cancer care and treatment. Given the rapid progress that has been achieved, we
provide a comprehensive overview of about 300 papers detailing cutting-edge
multimodal data integration techniques in precision oncology. In addition, we
conclude the primary clinical applications that have reaped significant
benefits, including early assessment, diagnosis, prognosis, and biomarker
discovery. Finally, derived from the findings of this survey, we present an
in-depth analysis that explores the pivotal challenges and reveals essential
pathways for future research in the field of multimodal data integration for
precision oncology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Cyber Defense in Dynamic Active Directories through
  Reinforcement Learning <span class="chip">ESORICS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diksha Goel, Kristen Moore, Mingyu Guo, Derui Wang, Minjune Kim, Seyit Camtepe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses a significant gap in Autonomous Cyber Operations (ACO)
literature: the absence of effective edge-blocking ACO strategies in dynamic,
real-world networks. It specifically targets the cybersecurity vulnerabilities
of organizational Active Directory (AD) systems. Unlike the existing literature
on edge-blocking defenses which considers AD systems as static entities, our
study counters this by recognizing their dynamic nature and developing advanced
edge-blocking defenses through a Stackelberg game model between attacker and
defender. We devise a Reinforcement Learning (RL)-based attack strategy and an
RL-assisted Evolutionary Diversity Optimization-based defense strategy, where
the attacker and defender improve each other strategy via parallel gameplay. To
address the computational challenges of training attacker-defender strategies
on numerous dynamic AD graphs, we propose an RL Training Facilitator that
prunes environments and neural networks to eliminate irrelevant elements,
enabling efficient and scalable training for large graphs. We extensively train
the attacker strategy, as a sophisticated attacker model is essential for a
robust defense. Our empirical results successfully demonstrate that our
proposed approach enhances defender's proficiency in hardening dynamic AD
graphs while ensuring scalability for large-scale AD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The manuscript has been accepted as full paper at European Symposium
  on Research in Computer Security (ESORICS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoMix: Automatically Mixing Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12963v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12963v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranjal Aggarwal, Aman Madaan, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Manaal Faruqui,  Mausam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are now available from cloud API providers in
various sizes and configurations. While this diversity offers a broad spectrum
of choices, effectively leveraging the options to optimize computational cost
and performance remains challenging. In this work, we present Automix, an
approach that strategically routes queries to larger LMs, based on the
approximate correctness of outputs from a smaller LM. Central to Automix are
two key technical contributions. First, it has a few-shot self-verification
mechanism, which estimates the reliability of its own outputs without requiring
extensive training. Second, given that self-verification can be noisy, it
employs a POMDP based router that can effectively select an appropriately sized
model, based on answer confidence. Experiments across five language models and
five challenging datasets show that Automix consistently surpasses strong
baselines, reducing computational cost by over 50% for comparable performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Work started and partly
  done during Aman's internship at Google. This version adds results on
  additional models and datasets</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Impact of Feature Representation on the Accuracy of Photonic Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mauricio Gomes de Queiroz, Paul Jimenez, Raphael Cardoso, Mateus Vidaletti Costa, Mohab Abdalla, Ian O'Connor, Alberto Bosio, Fabio Pavanello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photonic Neural Networks (PNNs) are gaining significant interest in the
research community due to their potential for high parallelization, low
latency, and energy efficiency. PNNs compute using light, which leads to
several differences in implementation when compared to electronics, such as the
need to represent input features in the photonic domain before feeding them
into the network. In this encoding process, it is common to combine multiple
features into a single input to reduce the number of inputs and associated
devices, leading to smaller and more energy-efficient PNNs. Although this
alters the network's handling of input data, its impact on PNNs remains
understudied. This paper addresses this open question, investigating the effect
of commonly used encoding strategies that combine features on the performance
and learning capabilities of PNNs. Here, using the concept of feature
importance, we develop a mathematical methodology for analyzing feature
combination. Through this methodology, we demonstrate that encoding multiple
features together in a single input determines their relative importance, thus
limiting the network's ability to learn from the data. Given some prior
knowledge of the data, however, this can also be leveraged for higher accuracy.
By selecting an optimal encoding method, we achieve up to a 12.3% improvement
in accuracy of PNNs trained on the Iris dataset compared to other encoding
techniques, surpassing the performance of networks where features are not
combined. These findings highlight the importance of carefully choosing the
encoding to the accuracy and decision-making strategies of PNNs, particularly
in size or power constrained applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PruningBench: A Comprehensive Benchmark of Structural Pruning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12315v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12315v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoling Li, Changhao Li, Mengqi Xue, Gongfan Fang, Sheng Zhou, Zunlei Feng, Huiqiong Wang, Yong Wang, Lechao Cheng, Mingli Song, Jie Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural pruning has emerged as a promising approach for producing more
efficient models. Nevertheless, the community suffers from a lack of
standardized benchmarks and metrics, leaving the progress in this area not
fully comprehended. To fill this gap, we present the first comprehensive
benchmark, termed \textit{PruningBench}, for structural pruning. PruningBench
showcases the following three characteristics: 1) PruningBench employs a
unified and consistent framework for evaluating the effectiveness of diverse
structural pruning techniques; 2) PruningBench systematically evaluates 16
existing pruning methods, encompassing a wide array of models (e.g., CNNs and
ViTs) and tasks (e.g., classification and detection); 3) PruningBench provides
easily implementable interfaces to facilitate the implementation of future
pruning methods, and enables the subsequent researchers to incorporate their
work into our leaderboards. We provide an online pruning platform
http://pruning.vipazoo.cn for customizing pruning tasks and reproducing all
results in this paper. Codes will be made publicly on
https://github.com/HollyLee2000/PruningBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to NeurIPS 2024 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16035v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16035v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Shi, Shaochen Xu, Tianze Yang, Zhengliang Liu, Tianming Liu, Xiang Li, Ninghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), although powerful in general domains, often
perform poorly on domain-specific tasks like medical question answering (QA).
Moreover, they tend to function as "black-boxes," making it challenging to
modify their behavior. To address the problem, our study delves into retrieval
augmented generation (RAG), aiming to improve LLM responses without the need
for fine-tuning or retraining. Specifically, we propose a comprehensive
retrieval strategy to extract medical facts from an external knowledge base,
and then inject them into the query prompt for LLMs. Focusing on medical QA
using the MedQA-SMILE dataset, we evaluate the impact of different retrieval
models and the number of facts provided to the LLM. Notably, our
retrieval-augmented Vicuna-7B model exhibited an accuracy improvement from
44.46% to 48.54%. This work underscores the potential of RAG to enhance LLM
performance, offering a practical approach to mitigate the challenges of
black-box LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AMIA 2024 Annual Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs and Memorization: On Quality and Specificity of Copyright
  Compliance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix B Mueller, Rebekka Görge, Anna K Bernzen, Janna C Pirk, Maximilian Poretschkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memorization in large language models (LLMs) is a growing concern. LLMs have
been shown to easily reproduce parts of their training data, including
copyrighted work. This is an important problem to solve, as it may violate
existing copyright laws as well as the European AI Act. In this work, we
propose a systematic analysis to quantify the extent of potential copyright
infringements in LLMs using European law as an example. Unlike previous work,
we evaluate instruction-finetuned models in a realistic end-user scenario. Our
analysis builds on a proposed threshold of 160 characters, which we borrow from
the German Copyright Service Provider Act and a fuzzy text matching algorithm
to identify potentially copyright-infringing textual reproductions. The
specificity of countermeasures against copyright infringement is analyzed by
comparing model behavior on copyrighted and public domain data. We investigate
what behaviors models show instead of producing protected text (such as refusal
or hallucination) and provide a first legal assessment of these behaviors. We
find that there are huge differences in copyright compliance, specificity, and
appropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous
perform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing
a particularly low absolute number of potential copyright violations. Code will
be published soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling laws for learning with real and surrogate data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Jain, Andrea Montanari, Eren Sasoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting large quantities of high-quality data can be prohibitively
expensive or impractical, and a bottleneck in machine learning. One may instead
augment a small set of $n$ data points from the target distribution with data
from more accessible sources, e.g. data collected under different circumstances
or synthesized by generative models. We refer to such data as `surrogate data.'
We introduce a weighted empirical risk minimization (ERM) approach for
integrating surrogate data into training. We analyze mathematically this method
under several classical statistical models, and validate our findings
empirically on datasets from different domains. Our main findings are: $(i)$
Integrating surrogate data can significantly reduce the test error on the
original distribution. Surprisingly, this can happen even when the surrogate
data is unrelated to the original ones. We trace back this behavior to the
classical Stein's paradox. $(ii)$ In order to reap the benefit of surrogate
data, it is crucial to use optimally weighted ERM. $(iii)$ The test error of
models trained on mixtures of real and surrogate data is approximately
described by a scaling law. This scaling law can be used to predict the optimal
weighting scheme, and to choose the amount of surrogate data to add.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added new experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Speculative Inference of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Oren Pereg, Moshe Wasserblat, Tomer Galanti, Michal Gordon, David Harel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accelerating the inference of large language models (LLMs) is an important
challenge in artificial intelligence. This paper introduces distributed
speculative inference (DSI), a novel distributed inference algorithm that is
provably faster than speculative inference (SI) [leviathan2023fast,
chen2023accelerating, miao2023specinfer] and traditional autoregressive
inference (non-SI). Like other SI algorithms, DSI works on frozen LLMs,
requiring no training or architectural modifications, and it preserves the
target distribution.
  Prior studies on SI have demonstrated empirical speedups (compared to non-SI)
but require a fast and accurate drafter LLM. In practice, off-the-shelf LLMs
often do not have matching drafters that are sufficiently fast and accurate. We
show a gap: SI gets slower than non-SI when using slower or less accurate
drafters. We close this gap by proving that DSI is faster than both SI and
non-SI given any drafters. By orchestrating multiple instances of the target
and drafters, DSI is not only faster than SI but also supports LLMs that cannot
be accelerated with SI.
  Our simulations show speedups of off-the-shelf LLMs in realistic settings:
DSI is 1.29-1.92x faster than SI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic planning in hierarchical active inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11658v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11658v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Priorelli, Ivilin Peev Stoianov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By dynamic planning, we refer to the ability of the human brain to infer and
impose motor trajectories related to cognitive decisions. A recent paradigm,
active inference, brings fundamental insights into the adaptation of biological
organisms, constantly striving to minimize prediction errors to restrict
themselves to life-compatible states. Over the past years, many studies have
shown how human and animal behavior could be explained in terms of an active
inferential process - either as discrete decision-making or continuous motor
control - inspiring innovative solutions in robotics and artificial
intelligence. Still, the literature lacks a comprehensive outlook on how to
effectively plan actions in changing environments. Setting ourselves the goal
of modeling tool use, we delve into the topic of dynamic planning in active
inference, keeping in mind two crucial aspects of biological goal-directed
behavior: the capacity to understand and exploit affordances for object
manipulation, and to learn the hierarchical interactions between the self and
the environment, including other agents. We start from a simple unit and
gradually describe more advanced structures, comparing recently proposed design
choices and providing basic examples for each section. This study distances
itself from traditional views centered on neural networks and reinforcement
learning, and points toward a yet unexplored direction in active inference:
hybrid representations in hierarchical models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Monte Carlo tree search (MCTS) formulation with multiple root
  nodes for discrete sizing optimization of truss structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06045v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06045v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu-Yao Ko, Katsuyuki Suzuki, Kazuo Yonekura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new method for discrete optimum design of truss
structures utilizing Monte Carlo tree search (MCTS) with update process, the
best reward, accelerating technique, and terminal condition. An improved MCTS
formulation with multiple root nodes is developed in this study. Update process
means that once a final solution is found, it is used as the initial solution
for next search tree. The best reward is used in the backpropagation step.
Accelerating technique is introduced by decreasing the width of search tree and
reducing maximum number of iterations. The agent is trained to minimize the
total structural weight under various constraints until the terminal condition
is satisfied. Then, optimal solution is the minimum value of all solutions
found by search trees. These numerical examples show that the agent can find
optimal solution with low computational cost, stably produces an optimal
design, and is suitable for practical engineering problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 20 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Intelligible and Effective Graph Neural Additive Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maya Bechler-Speicher, Amir Globerson, Ran Gilad-Bachrach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have emerged as the predominant approach for
learning over graph-structured data. However, most GNNs operate as black-box
models and require post-hoc explanations, which may not suffice in high-stakes
scenarios where transparency is crucial. In this paper, we present a GNN that
is interpretable by design. Our model, Graph Neural Additive Network (GNAN), is
a novel extension of the interpretable class of Generalized Additive Models,
and can be visualized and fully understood by humans. GNAN is designed to be
fully interpretable, allowing both global and local explanations at the feature
and graph levels through direct visualization of the model. These
visualizations describe the exact way the model uses the relationships between
the target variable, the features, and the graph. We demonstrate the
intelligibility of GNANs in a series of examples on different tasks and
datasets. In addition, we show that the accuracy of GNAN is on par with
black-box GNNs, making it suitable for critical applications where transparency
is essential, alongside high accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative AI-Driven Human Digital Twin in IoT-Healthcare: A
  Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayuan Chen, You Shi, Changyan Yi, Hongyang Du, Jiawen Kang, Dusit Niyato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Internet of things (IoT) can significantly enhance the quality of human
life, specifically in healthcare, attracting extensive attentions to
IoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as
an innovative paradigm that can comprehensively characterize the replication of
the individual human body in the digital world and reflect its physical status
in real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the
application of healthcare monitoring by acting as a versatile and vivid human
digital testbed, simulating the outcomes and guiding the practical treatments.
However, successfully establishing HDT requires high-fidelity virtual modeling
and strong information interactions but possibly with scarce, biased and noisy
data. Fortunately, a recent popular technology called generative artificial
intelligence (GAI) may be a promising solution because it can leverage advanced
AI algorithms to automatically create, manipulate, and modify valuable while
diverse data. This survey particularly focuses on the implementation of
GAI-driven HDT in IoT-healthcare. We start by introducing the background of
IoT-healthcare and the potential of GAI-driven HDT. Then, we delve into the
fundamental techniques and present the overall framework of GAI-driven HDT.
After that, we explore the realization of GAI-driven HDT in detail, including
GAI-enabled data acquisition, communication, data management, digital modeling,
and data analysis. Besides, we discuss typical IoT-healthcare applications that
can be revolutionized by GAI-driven HDT, namely personalized health monitoring
and diagnosis, personalized prescription, and personalized rehabilitation.
Finally, we conclude this survey by highlighting some future research
directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving a <span class="highlight-title">Real-World</span> Package Delivery Routing Problem Using Quantum
  Annealers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15114v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15114v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eneko Osaba, Esther Villar-Rodriguez, Antón Asla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research focused on the conjunction between quantum computing and routing
problems has been very prolific in recent years. Most of the works revolve
around classical problems such as the Traveling Salesman Problem or the Vehicle
Routing Problem. The real-world applicability of these problems is dependent on
the objectives and constraints considered. Anyway, it is undeniable that it is
often difficult to translate complex requirements into these classical
formulations.The main objective of this research is to present a solving scheme
for dealing with realistic instances while maintaining all the characteristics
and restrictions of the original real-world problem. Thus, a quantum-classical
strategy has been developed, coined Q4RPD, that considers a set of real
constraints such as a heterogeneous fleet of vehicles, priority deliveries, and
capacities characterized by two values: weight and dimensions of the packages.
Q4RPD resorts to the Leap Constrained Quadratic Model Hybrid Solver of D-Wave.
To demonstrate the application of Q4RPD, an experimentation composed of six
different instances has been conducted, aiming to serve as illustrative
examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures and 4 tables. Paper submitted for review in
  Scientific Reports</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Networked Communication for Decentralised Agents in Mean-Field Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02766v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02766v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Benjamin, Alessandro Abate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce networked communication to the mean-field game framework, in
particular to oracle-free settings where $N$ decentralised agents learn along a
single, non-episodic run of the empirical system. We prove that our
architecture, with only a few reasonable assumptions about network structure,
has sample guarantees bounded between those of the centralised- and
independent-learning cases. We discuss how the sample guarantees of the three
theoretical algorithms do not actually result in practical convergence. We
therefore show that in practical settings where the theoretical parameters are
not observed (leading to poor estimation of the Q-function), our communication
scheme significantly accelerates convergence over the independent case (and
often even the centralised case), without relying on the assumption of a
centralised learner. We contribute further practical enhancements to all three
theoretical algorithms, allowing us to present their first empirical
demonstrations. Our experiments confirm that we can remove several of the
theoretical assumptions of the algorithms, and display the empirical
convergence benefits brought by our new networked communication. We
additionally show that the networked approach has significant advantages, over
both the centralised and independent alternatives, in terms of robustness to
unexpected learning failures and to changes in population size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spike Accumulation Forwarding for Effective Training of Spiking Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02772v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02772v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryuji Saiin, Tomoya Shirakawa, Sota Yoshihara, Yoshihide Sawada, Hiroyuki Kusumoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we propose a new paradigm for training spiking neural
networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are
energy-efficient but difficult to train. Consequently, many researchers have
proposed various methods to solve this problem, among which online training
through time (OTTT) is a method that allows inferring at each time step while
suppressing the memory cost. However, to compute efficiently on GPUs, OTTT
requires operations with spike trains and weighted summation of spike trains
during forwarding. In addition, OTTT has shown a relationship with the Spike
Representation, an alternative training method, though theoretical agreement
with Spike Representation has yet to be proven. Our proposed method can solve
these problems; namely, SAF can halve the number of operations during the
forward process, and it can be theoretically proven that SAF is consistent with
the Spike Representation and OTTT, respectively. Furthermore, we confirmed the
above contents through experiments and showed that it is possible to reduce
memory and training time while maintaining accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, Appendix:10 pages, 2 figures, v6:Published in
  Transactions on Machine Learning Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UINav: A Practical Approach to Train On-Device Automation Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10170v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10170v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Li, Fu-Lin Hsu, Will Bishop, Folawiyo Campbell-Ajala, Max Lin, Oriana Riva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automation systems that can autonomously drive application user interfaces to
complete user tasks are of great benefit, especially when users are
situationally or permanently impaired. Prior automation systems do not produce
generalizable models while AI-based automation agents work reliably only in
simple, hand-crafted applications or incur high computation costs. We propose
UINav, a demonstration-based approach to train automation agents that fit
mobile devices, yet achieving high success rates with modest numbers of
demonstrations. To reduce the demonstration overhead, UINav uses a referee
model that provides users with immediate feedback on tasks where the agent
fails, and automatically augments human demonstrations to increase diversity in
training data. Our evaluation shows that with only 10 demonstrations UINav can
achieve 70% accuracy, and that with enough demonstrations it can surpass 90%
accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Reasoning: Personalized Content Generation Without the Cold
  Start Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davor Hafnar, Jure Demšar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural content generation uses algorithmic techniques to create large
amounts of new content for games at much lower production costs. In newer
approaches, procedural content generation utilizes machine learning. However,
these methods usually require expensive collection of large amounts of data, as
well as the development and training of fairly complex learning models, which
can be both extremely time-consuming and expensive. The core of our research is
to explore whether we can lower the barrier to the use of personalized
procedural content generation through a more practical and generalizable
approach with large language models. Matching game content with player
preferences benefits both players, who enjoy the game more, and developers, who
increasingly depend on players enjoying the game before being able to monetize
it. Therefore, this paper presents a novel approach to achieving
personalization by using large language models to propose levels based on the
gameplay data continuously collected from individual players. We compared the
levels generated using our approach with levels generated with more traditional
procedural generation techniques. Our easily reproducible method has proven
viable in a production setting and outperformed levels generated by traditional
methods in the probability that a player will not quit the game mid-level.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures. Paper accepted to IEEE Transactions on Games</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in
  Large Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Haotian Wang, Ming Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grasping the concept of time is a fundamental facet of human cognition,
indispensable for truly comprehending the intricacies of the world. Previous
studies typically focus on specific aspects of time, lacking a comprehensive
temporal reasoning benchmark. To address this, we propose TimeBench, a
comprehensive hierarchical temporal reasoning benchmark that covers a broad
spectrum of temporal reasoning phenomena. TimeBench provides a thorough
evaluation for investigating the temporal reasoning capabilities of large
language models. We conduct extensive experiments on GPT-4, LLaMA2, and other
popular LLMs under various settings. Our experimental results indicate a
significant performance gap between the state-of-the-art LLMs and humans,
highlighting that there is still a considerable distance to cover in temporal
reasoning. Besides, LLMs exhibit capability discrepancies across different
reasoning categories. Furthermore, we thoroughly analyze the impact of multiple
aspects on temporal reasoning and emphasize the associated challenges. We
aspire for TimeBench to serve as a comprehensive benchmark, fostering research
in temporal reasoning. Resources are available at:
https://github.com/zchuz/TimeBench
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multimodal Foundation Agent for Financial Trading: Tool-Augmented,
  Diversified, and Generalist 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18485v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18485v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, Longtao Zheng, Xinrun Wang, Bo An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial trading is a crucial component of the markets, informed by a
multimodal information landscape encompassing news, prices, and Kline charts,
and encompasses diverse tasks such as quantitative trading and high-frequency
trading with various assets. While advanced AI techniques like deep learning
and reinforcement learning are extensively utilized in finance, their
application in financial trading tasks often faces challenges due to inadequate
handling of multimodal data and limited generalizability across various tasks.
To address these challenges, we present FinAgent, a multimodal foundational
agent with tool augmentation for financial trading. FinAgent's market
intelligence module processes a diverse range of data-numerical, textual, and
visual-to accurately analyze the financial market. Its unique dual-level
reflection module not only enables rapid adaptation to market dynamics but also
incorporates a diversified memory retrieval system, enhancing the agent's
ability to learn from historical data and improve decision-making processes.
The agent's emphasis on reasoning for actions fosters trust in its financial
decisions. Moreover, FinAgent integrates established trading strategies and
expert insights, ensuring that its trading approaches are both data-driven and
rooted in sound financial principles. With comprehensive experiments on 6
financial datasets, including stocks and Crypto, FinAgent significantly
outperforms 9 state-of-the-art baselines in terms of 6 financial metrics with
over 36% average improvement on profit. Specifically, a 92.27% return (a 84.39%
relative improvement) is achieved on one dataset. Notably, FinAgent is the
first advanced multimodal foundation agent designed for financial trading
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Maheshwary, Vikas Yadav, Hoang Nguyen, Khyati Mahajan, Sathwik Tejaswi Madhusudhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction finetuning (IFT) is critical for aligning Large Language Models
(LLMs) to follow instructions. While many effective IFT datasets have been
introduced recently, they predominantly focus on high-resource languages like
English. To better align LLMs across a broad spectrum of languages and tasks,
we propose a fully synthetic, novel taxonomy (Evol) guided Multilingual,
Multi-turn instruction finetuning dataset, called M2Lingual. It is constructed
by first selecting a diverse set of seed examples and then utilizing the
proposed Evol taxonomy to convert these seeds into complex and challenging
multi-turn instructions. We demonstrate the effectiveness of M2Lingual by
training LLMs of varying sizes and showcasing the enhanced performance across a
diverse set of languages. We contribute the 2 step Evol taxonomy with the
guided generation code: https://github.com/ServiceNow/M2Lingual, as well as the
first fully synthetic, general and task-oriented, multi-turn, multilingual
dataset built with Evol - M2Lingual:
https://huggingface.co/datasets/ServiceNow-AI/ M2Lingual - containing 182K
total IFT pairs, covering 70 languages and 17+ NLP tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity
  Text Embeddings Through Self-Knowledge <span class="highlight-title">Distillation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03216v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03216v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new embedding model, called M3-Embedding, which
is distinguished for its versatility in Multi-Linguality, Multi-Functionality,
and Multi-Granularity. It can support more than 100 working languages, leading
to new state-of-the-art performances on multi-lingual and cross-lingual
retrieval tasks. It can simultaneously perform the three common retrieval
functionalities of embedding model: dense retrieval, multi-vector retrieval,
and sparse retrieval, which provides a unified model foundation for real-world
IR applications. It is able to process inputs of different granularities,
spanning from short sentences to long documents of up to 8192 tokens. The
effective training of M3-Embedding involves the following technical
contributions. We propose a novel self-knowledge distillation approach, where
the relevance scores from different retrieval functionalities can be integrated
as the teacher signal to enhance the training quality. We also optimize the
batching strategy, enabling a large batch size and high training throughput to
ensure the discriminativeness of embeddings. To the best of our knowledge,
M3-Embedding is the first embedding model which realizes such a strong
versatility. The model and code will be publicly available at
https://github.com/FlagOpen/FlagEmbedding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Data Curation for Self-Supervised Learning: A Clustering-Based
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy V. Vo, Vasil Khalidov, Timothée Darcet, Théo Moutakanni, Nikita Smetanin, Marc Szafraniec, Hugo Touvron, Camille Couprie, Maxime Oquab, Armand Joulin, Hervé Jégou, Patrick Labatut, Piotr Bojanowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised features are the cornerstone of modern machine learning
systems. They are typically pre-trained on data collections whose construction
and curation typically require extensive human effort. This manual process has
some limitations similar to those encountered in supervised learning, e.g., the
crowd-sourced selection of data is costly and time-consuming, preventing
scaling the dataset size. In this work, we consider the problem of automatic
curation of high-quality datasets for self-supervised pre-training. We posit
that such datasets should be large, diverse and balanced, and propose a
clustering-based approach for building ones satisfying all these criteria. Our
method involves successive and hierarchical applications of $k$-means on a
large and diverse data repository to obtain clusters that distribute uniformly
among data concepts, followed by a hierarchical, balanced sampling step from
these clusters. Extensive experiments on three different data domains including
web-based images, satellite images and text show that features trained on our
automatically curated datasets outperform those trained on uncurated data while
being on par or better than ones trained on manually curated data. Code is
available at https://github.com/facebookresearch/ssl-data-curation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DLRover-RM: Resource Optimization for Deep Recommendation Models
  Training in the Cloud <span class="chip">VLDB'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.01468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.01468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinlong Wang, Tingfeng Lan, Yinghao Tang, Ziling Huang, Yiheng Du, Haitao Zhang, Jian Sha, Hui Lu, Yuanchun Zhou, Ke Zhang, Mingjie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning recommendation models (DLRM) rely on large embedding tables to
manage categorical sparse features. Expanding such embedding tables can
significantly enhance model performance, but at the cost of increased
GPU/CPU/memory usage. Meanwhile, tech companies have built extensive
cloud-based services to accelerate training DLRM models at scale. In this
paper, we conduct a deep investigation of the DLRM training platforms at
AntGroup and reveal two critical challenges: low resource utilization due to
suboptimal configurations by users and the tendency to encounter abnormalities
due to an unstable cloud environment. To overcome them, we introduce
DLRover-RM, an elastic training framework for DLRMs designed to increase
resource utilization and handle the instability of a cloud environment.
DLRover-RM develops a resource-performance model by considering the unique
characteristics of DLRMs and a three-stage heuristic strategy to automatically
allocate and dynamically adjust resources for DLRM training jobs for higher
resource utilization. Further, DLRover-RM develops multiple mechanisms to
ensure efficient and reliable execution of DLRM training jobs. Our extensive
evaluation shows that DLRover-RM reduces job completion times by 31%, increases
the job completion rate by 6%, enhances CPU usage by 15%, and improves memory
utilization by 20%, compared to state-of-the-art resource scheduling
frameworks. DLRover-RM has been widely deployed at AntGroup and processes
thousands of DLRM training jobs on a daily basis. DLRover-RM is open-sourced
and has been adopted by 10+ companies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in VLDB'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Enhanced Clustering for News Event Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10552v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10552v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adane Nega Tarekegn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The news landscape is continuously evolving, with an ever-increasing volume
of information from around the world. Automated event detection within this
vast data repository is essential for monitoring, identifying, and categorizing
significant news occurrences across diverse platforms. This paper presents an
event detection framework that leverages Large Language Models (LLMs) combined
with clustering analysis to detect news events from the Global Database of
Events, Language, and Tone (GDELT). The framework enhances event clustering
through both pre-event detection tasks (keyword extraction and text embedding)
and post-event detection tasks (event summarization and topic labelling). We
also evaluate the impact of various textual embeddings on the quality of
clustering outcomes, ensuring robust news categorization. Additionally, we
introduce a novel Cluster Stability Assessment Index (CSAI) to assess the
validity and robustness of clustering results. CSAI utilizes multiple feature
vectors to provide a new way of measuring clustering quality. Our experiments
indicate that the use of LLM embedding in the event detection framework has
significantly improved the results, demonstrating greater robustness in terms
of CSAI scores. Moreover, post-event detection tasks generate meaningful
insights, facilitating effective interpretation of event clustering results.
Overall, our experimental results indicate that the proposed framework offers
valuable insights and could enhance the accuracy in news analysis and
reporting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample<span class="highlight-title">Attention</span>: Near-Lossless Acceleration of Long Context LLM
  Inference with Adaptive Structured Sparse <span class="highlight-title">Attention</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianchao Zhu, Jiangfei Duan, Chang Chen, Siran Liu, Xiuhong Li, Guanyu Feng, Xin Lv, Huanqi Cao, Xiao Chuanfu, Xingcheng Zhang, Dahua Lin, Chao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) now support extremely long context windows, but
the quadratic complexity of vanilla attention results in significantly long
Time-to-First-Token (TTFT) latency. Existing approaches to address this
complexity require additional pretraining or finetuning, and often sacrifice
model accuracy. In this paper, we first provide both theoretical and empirical
foundations for near-lossless sparse attention. We find dynamically capturing
head-specific sparse patterns at runtime with low overhead is crucial. To
address this, we propose SampleAttention, an adaptive structured and
near-lossless sparse attention. Leveraging observed significant sparse
patterns, SampleAttention attends to a fixed percentage of adjacent tokens to
capture local window patterns, and employs a two-stage query-guided key-value
filtering approach, which adaptively select a minimum set of key-values with
low overhead, to capture column stripe patterns. Comprehensive evaluations show
that SampleAttention can seamlessly replace vanilla attention in off-the-shelf
LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\times$
compared with FlashAttention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ULLER: A Unified Language for Learning and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emile van Krieken, Samy Badreddine, Robin Manhaeve, Eleonora Giunchiglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of neuro-symbolic artificial intelligence (NeSy), which combines
learning and reasoning, has recently experienced significant growth. There now
are a wide variety of NeSy frameworks, each with its own specific language for
expressing background knowledge and how to relate it to neural networks. This
heterogeneity hinders accessibility for newcomers and makes comparing different
NeSy frameworks challenging. We propose a language for NeSy, which we call
ULLER, a Unfied Language for LEarning and Reasoning. ULLER encompasses a wide
variety of settings, while ensuring that knowledge described in it can be used
in existing NeSy systems. ULLER has a first-order logic syntax specialised for
NeSy for which we provide example semantics including classical FOL, fuzzy
logic, and probabilistic logic. We believe ULLER is a first step towards making
NeSy research more accessible and comparable, paving the way for libraries that
streamline training and evaluation across a multitude of semantics, knowledge
bases, and NeSy systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeSy 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position: Explain to Question not to Justify 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Przemyslaw Biecek, Wojciech Samek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (XAI) is a young but very promising field
of research. Unfortunately, the progress in this field is currently slowed down
by divergent and incompatible goals. We separate various threads tangled within
the area of XAI into two complementary cultures of human/value-oriented
explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI).
This position paper argues that the area of RED XAI is currently
under-explored, i.e., more methods for explainability are desperately needed to
question models (e.g., extract knowledge from well-performing models as well as
spotting and fixing bugs in faulty models), and the area of RED XAI hides great
opportunities and potential for important research necessary to ensure the
safety of AI systems. We conclude this paper by presenting promising challenges
in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciBench: Evaluating College-Level Scientific Problem-Solving Abilities
  of Large Language Models <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10635v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10635v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the existing Large Language Model (LLM) benchmarks on scientific
problem reasoning focus on problems grounded in high-school subjects and are
confined to elementary algebraic operations. To systematically examine the
reasoning capabilities required for solving complex scientific problems, we
introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a
carefully curated dataset featuring a range of collegiate-level scientific
problems from mathematics, chemistry, and physics domains. Based on the
dataset, we conduct an in-depth benchmarking study of representative
open-source and proprietary LLMs with various prompting strategies. The results
reveal that the current LLMs fall short of delivering satisfactory performance,
with the best overall score of merely 43.22%. Furthermore, through a detailed
user study, we categorize the errors made by LLMs into ten problem-solving
abilities. Our analysis indicates that no single prompting strategy
significantly outperforms the others and some strategies that demonstrate
improvements in certain problem-solving skills could result in declines in
other skills. We envision that SciBench will catalyze further developments in
the reasoning abilities of LLMs, thereby ultimately contributing to scientific
research and discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Preference Learning for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Muldrew, Peter Hayes, Mingtian Zhang, David Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become more capable, fine-tuning techniques
for aligning with human intent are increasingly important. A key consideration
for aligning these models is how to most effectively use human resources, or
model resources in the case where LLMs themselves are used as oracles.
Reinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most
prominent example of such a technique, but is complex and often unstable.
Direct Preference Optimization (DPO) has recently been proposed as a simpler
and more stable alternative. In this work, we develop an active learning
strategy for DPO to make better use of preference labels. We propose a
practical acquisition function for prompt/completion pairs based on the
predictive entropy of the language model and a measure of certainty of the
implicit preference model optimized by DPO. We demonstrate how our approach
improves both the rate of learning and final performance of fine-tuning on
pairwise preference data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concept-aware Data Construction Improves In-context Learning of Language
  Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Štefánik, Marek Kadlčík, Petr Sojka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent language models (LMs) are capable of in-context learning (ICL),
manifested in the LMs' ability to perform a new task solely from
natural-language instruction. Previous work curating in-context learners
assumes that ICL emerges from a vast over-parametrization or the scale of
multi-task training. However, recent theoretical work attributes the ICL
ability to concept-dependent training data and creates functional in-context
learners even in small-scale, synthetic settings.
  In this work, we practically explore this newly identified axis of ICL
quality. We propose Concept-aware Training (CoAT), a framework for constructing
training scenarios that make it beneficial for the LM to learn to utilize the
analogical reasoning concepts from demonstrations. We find that by using CoAT,
pre-trained transformers can learn to better utilise new latent concepts from
demonstrations and that such ability makes ICL more robust to the functional
deficiencies of the previous models. Finally, we show that concept-aware
in-context learning is more effective for a majority of new tasks when compared
to traditional instruction tuning, resulting in a performance comparable to the
previous in-context learners using magnitudes of more training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long paper to appear in Findings of ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessment of Sentinel-2 spatial and temporal coverage based on the
  scene classification layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristhian Sanchez, Francisco Mena, Marcela Charfuelan, Marlon Nuske, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the launch of the Sentinel-2 (S2) satellites, many ML models have used
the data for diverse applications. The scene classification layer (SCL) inside
the S2 product provides rich information for training, such as filtering images
with high cloud coverage. However, there is more potential in this. We propose
a technique to assess the clean optical coverage of a region, expressed by a
SITS and calculated with the S2-based SCL data. With a manual threshold and
specific labels in the SCL, the proposed technique assigns a percentage of
spatial and temporal coverage across the time series and a high/low assessment.
By evaluating the AI4EO challenge for Enhanced Agriculture, we show that the
assessment is correlated to the predictive results of ML models. The
classification results in a region with low spatial and temporal coverage is
worse than in a region with high coverage. Finally, we applied the technique
across all continents of the global dataset LandCoverNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE International Geoscience and Remote Sensing
  Symposium 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Logical Closed Loop: Uncovering Object Hallucinations in Large
  Vision-Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object hallucination has been an Achilles' heel which hinders the broader
applications of large vision-language models (LVLMs). Object hallucination
refers to the phenomenon that the LVLMs claim non-existent objects in the
image. To mitigate the object hallucinations, instruction tuning and external
model-based detection methods have been proposed, which either require
large-scare computational resources or depend on the detection result of
external models. However, there remains an under-explored field to utilize the
LVLM itself to alleviate object hallucinations. In this work, we adopt the
intuition that the LVLM tends to respond logically consistently for existent
objects but inconsistently for hallucinated objects. Therefore, we propose a
Logical Closed Loop-based framework for Object Hallucination Detection and
Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency
probing to raise questions with logical correlations, inquiring about
attributes from objects and vice versa. Whether their responses can form a
logical closed loop serves as an indicator of object hallucination. As a
plug-and-play method, it can be seamlessly applied to all existing LVLMs.
Comprehensive experiments conducted on three benchmarks across four LVLMs have
demonstrated significant improvements brought by our method, indicating its
effectiveness and generality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ACL 2024; 19 Pages, 15 Figures, 6 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ANLS* -- A Universal Document Processing Metric for Generative Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03848v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03848v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Peer, Philemon Schöpf, Volckmar Nebendahl, Alexander Rietzler, Sebastian Stabinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, discriminative models have been the predominant choice for
tasks like document classification and information extraction. These models
make predictions that fall into a limited number of predefined classes,
facilitating a binary true or false evaluation and enabling the direct
calculation of metrics such as the F1 score. However, recent advancements in
generative large language models (GLLMs) have prompted a shift in the field due
to their enhanced zero-shot capabilities, which eliminate the need for a
downstream dataset and computationally expensive fine-tuning. However,
evaluating GLLMs presents a challenge as the binary true or false evaluation
used for discriminative models is not applicable to the predictions made by
GLLMs.
  This paper introduces a new metric for generative models called ANLS* for
evaluating a wide variety of tasks, including information extraction and
classification tasks. The ANLS* metric extends existing ANLS metrics as a
drop-in-replacement and is still compatible with previously reported ANLS
scores. An evaluation of 7 different datasets, and more than 10 different GLLMs
together with 3 different prompting methods using the ANLS* metric is also
provided, demonstrating the importance of the proposed metric.
  We also benchmark a novel approach to generate prompts for documents, called
SFT, against other prompting techniques such as LATIN. In 6 out of 7 cases, SFT
outperforms other techniques and improves the state-of-the-art, sometimes by as
much as $10$ percentage points.
  Sources are available at https://github.com/deepopinion/anls_star_metric
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Apollo: A <span class="highlight-title">Lightweight</span> Multilingual Medical LLM towards Democratizing
  Medical AI to 6B People 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03640v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03640v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu, Anningzhe Gao, Xiang Wan, Haizhou Li, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the vast repository of global medical knowledge predominantly being
in English, local languages are crucial for delivering tailored healthcare
services, particularly in areas with limited medical resources. To extend the
reach of medical AI advancements to a broader population, we aim to develop
medical LLMs across the six most widely spoken languages, encompassing a global
population of 6.1 billion. This effort culminates in the creation of the
ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the
multilingual medical benchmark, the released Apollo models, at various
relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best
performance among models of equivalent size. Especially, Apollo-7B is the
state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite
models could be used to improve the multi-lingual medical capabilities of
larger models without fine-tuning in a proxy-tuning fashion. We will
open-source training corpora, code, model weights and evaluation benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transcendence: Generative Models Can Outperform The Experts That Train
  Them 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11741v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11741v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edwin Zhang, Vincent Zhu, Naomi Saphra, Anat Kleiman, Benjamin L. Edelman, Milind Tambe, Sham M. Kakade, Eran Malach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models are trained with the simple objective of imitating the
conditional probability distribution induced by the data they are trained on.
Therefore, when trained on data generated by humans, we may not expect the
artificial model to outperform the humans on their original objectives. In this
work, we study the phenomenon of transcendence: when a generative model
achieves capabilities that surpass the abilities of the experts generating its
data. We demonstrate transcendence by training an autoregressive transformer to
play chess from game transcripts, and show that the trained model can sometimes
achieve better performance than all players in the dataset. We theoretically
prove that transcendence can be enabled by low-temperature sampling, and
rigorously assess this claim experimentally. Finally, we discuss other sources
of transcendence, laying the groundwork for future investigation of this
phenomenon in a broader setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code, models, and data at https://transcendence.eddie.win</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Behavior Generation with Latent Actions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungjae Lee, Yibin Wang, Haritheja Etukuru, H. Jin Kim, Nur Muhammad Mahi Shafiullah, Lerrel Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative modeling of complex behaviors from labeled datasets has been a
longstanding problem in decision making. Unlike language or image generation,
decision making requires modeling actions - continuous-valued vectors that are
multimodal in their distribution, potentially drawn from uncurated sources,
where generation errors can compound in sequential prediction. A recent class
of models called Behavior Transformers (BeT) addresses this by discretizing
actions using k-means clustering to capture different modes. However, k-means
struggles to scale for high-dimensional action spaces or long sequences, and
lacks gradient information, and thus BeT suffers in modeling long-range
actions. In this work, we present Vector-Quantized Behavior Transformer
(VQ-BeT), a versatile model for behavior generation that handles multimodal
action prediction, conditional generation, and partial observations. VQ-BeT
augments BeT by tokenizing continuous actions with a hierarchical vector
quantization module. Across seven environments including simulated
manipulation, autonomous driving, and robotics, VQ-BeT improves on
state-of-the-art models such as BeT and Diffusion Policies. Importantly, we
demonstrate VQ-BeT's improved ability to capture behavior modes while
accelerating inference speed 5x over Diffusion Policies. Videos and code can be
found https://sjlee.cc/vq-bet
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github repo: https://github.com/jayLEE0301/vq_bet_official</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WellDunn: On the Robustness and Explainability of Language Models and
  Large Language Models in Identifying Wellness Dimensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12058v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12058v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyedali Mohammadi, Edward Raff, Jinendra Malekar, Vedant Palit, Francis Ferraro, Manas Gaur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) are being proposed for mental health applications where
the heightened risk of adverse outcomes means predictive performance may not be
a sufficient litmus test of a model's utility in clinical practice. A model
that can be trusted for practice should have a correspondence between
explanation and clinical determination, yet no prior research has examined the
attention fidelity of these models and their effect on ground truth
explanations. We introduce an evaluation design that focuses on the robustness
and explainability of LMs in identifying Wellness Dimensions (WD). We focus on
two mental health and well-being datasets: (a) Multi-label Classification-based
MultiWD, and (b) WellXplain for evaluating attention mechanism veracity against
expert-labeled explanations. The labels are based on Halbert Dunn's theory of
wellness, which gives grounding to our evaluation. We reveal four surprising
results about LMs/LLMs: (1) Despite their human-like capabilities, GPT-3.5/4
lag behind RoBERTa, and MedAlpaca, a fine-tuned LLM fails to deliver any
remarkable improvements in performance or explanations. (2) Re-examining LMs'
predictions based on a confidence-oriented loss function reveals a significant
performance drop. (3) Across all LMs/LLMs, the alignment between attention and
explanations remains low, with LLMs scoring a dismal 0.0. (4) Most mental
health-specific LMs/LLMs overlook domain-specific knowledge and undervalue
explanations, causing these discrepancies. This study highlights the need for
further research into their consistency and explanations in mental health and
well-being.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, including reference and appendix sections, 8 figures, and
  16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kernel vs. Kernel: Exploring How the Data Structure Affects Neural
  Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vignesh Kothapalli, Tom Tirer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a vast amount of literature has focused on the "Neural Collapse"
(NC) phenomenon, which emerges when training neural network (NN) classifiers
beyond the zero training error point. The core component of NC is the decrease
in the within class variability of the network's deepest features, dubbed as
NC1. The theoretical works that study NC are typically based on simplified
unconstrained features models (UFMs) that mask any effect of the data on the
extent of collapse. In this paper, we provide a kernel-based analysis that does
not suffer from this limitation. First, given a kernel function, we establish
expressions for the traces of the within- and between-class covariance matrices
of the samples' features (and consequently an NC1 metric). Then, we turn to
focus on kernels associated with shallow NNs. First, we consider the NN
Gaussian Process kernel (NNGP), associated with the network at initialization,
and the complement Neural Tangent Kernel (NTK), associated with its training in
the "lazy regime". Interestingly, we show that the NTK does not represent more
collapsed features than the NNGP for prototypical data models. As NC emerges
from training, we then consider an alternative to NTK: the recently proposed
adaptive kernel, which generalizes NNGP to model the feature mapping learned
from the training data. Contrasting our NC1 analysis for these two kernels
enables gaining insights into the effect of data distribution on the extent of
collapse, which are empirically aligned with the behavior observed with
practical training of NNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIGB: Generative Auto-bidding via <span class="highlight-title">Diffusion</span> Modeling <span class="chip">KDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16141v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16141v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayan Guo, Yusen Huo, Zhilin Zhang, Tianyu Wang, Chuan Yu, Jian Xu, Yan Zhang, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auto-bidding plays a crucial role in facilitating online advertising by
automatically providing bids for advertisers. Reinforcement learning (RL) has
gained popularity for auto-bidding. However, most current RL auto-bidding
methods are modeled through the Markovian Decision Process (MDP), which assumes
the Markovian state transition. This assumption restricts the ability to
perform in long horizon scenarios and makes the model unstable when dealing
with highly random online advertising environments. To tackle this issue, this
paper introduces AI-Generated Bidding (AIGB), a novel paradigm for auto-bidding
through generative modeling. In this paradigm, we propose DiffBid, a
conditional diffusion modeling approach for bid generation. DiffBid directly
models the correlation between the return and the entire trajectory,
effectively avoiding error propagation across time steps in long horizons.
Additionally, DiffBid offers a versatile approach for generating trajectories
that maximize given targets while adhering to specific constraints. Extensive
experiments conducted on the real-world dataset and online A/B test on Alibaba
advertising platform demonstrate the effectiveness of DiffBid, achieving 2.81%
increase in GMV and 3.36% increase in ROI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAdam: Adam is a natural gradient optimizer using diagonal empirical
  Fisher information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12807v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12807v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongseong Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper establishes a mathematical foundation for the Adam optimizer,
elucidating its connection to natural gradient descent through Riemannian and
information geometry. We rigorously analyze the diagonal empirical Fisher
information matrix (FIM) in Adam, clarifying all detailed approximations and
advocating for the use of log probability functions as loss, which should be
based on discrete distributions, due to the limitations of empirical FIM. Our
analysis uncovers flaws in the original Adam algorithm, leading to proposed
corrections such as enhanced momentum calculations, adjusted bias corrections,
adaptive epsilon, and gradient clipping. We refine the weight decay term based
on our theoretical framework. Our modified algorithm, Fisher Adam (FAdam),
demonstrates superior performance across diverse domains including LLM, ASR,
and VQ-VAE, achieving state-of-the-art results in ASR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 4 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MeGA: Merging Multiple Independently Trained Neural Networks Based on
  Genetic Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04607v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04607v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel method for merging the weights of
multiple pre-trained neural networks using a genetic algorithm called MeGA.
Traditional techniques, such as weight averaging and ensemble methods, often
fail to fully harness the capabilities of pre-trained networks. Our approach
leverages a genetic algorithm with tournament selection, crossover, and
mutation to optimize weight combinations, creating a more effective fusion.
This technique allows the merged model to inherit advantageous features from
both parent models, resulting in enhanced accuracy and robustness. Through
experiments on the CIFAR-10 dataset, we demonstrate that our genetic
algorithm-based weight merging method improves test accuracy compared to
individual models and conventional methods. This approach provides a scalable
solution for integrating multiple pre-trained networks across various deep
learning applications. Github is available at:
https://github.com/YUNBLAK/MeGA-Merging-Multiple-Independently-Trained-Neural-Networks-Based-on-Genetic-Algorithm
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Model-Based Optimization for Challenging Fitness Landscapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saba Ghaffari, Ehsan Saleh, Alexander G. Schwing, Yu-Xiong Wang, Martin D. Burke, Saurabh Sinha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein design, a grand challenge of the day, involves optimization on a
fitness landscape, and leading methods adopt a model-based approach where a
model is trained on a training set (protein sequences and fitness) and proposes
candidates to explore next. These methods are challenged by sparsity of
high-fitness samples in the training set, a problem that has been in the
literature. A less recognized but equally important problem stems from the
distribution of training samples in the design space: leading methods are not
designed for scenarios where the desired optimum is in a region that is not
only poorly represented in training data, but also relatively far from the
highly represented low-fitness regions. We show that this problem of
"separation" in the design space is a significant bottleneck in existing
model-based optimization tools and propose a new approach that uses a novel VAE
as its search model to overcome the problem. We demonstrate its advantage over
prior methods in robustly finding improved samples, regardless of the imbalance
and separation between low- and high-fitness samples. Our comprehensive
benchmark on real and semi-synthetic protein datasets as well as solution
design for physics-informed neural networks, showcases the generality of our
approach in discrete and continuous design spaces. Our implementation is
available at https://github.com/sabagh1994/PGVAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Character-Adapter: Prompt-Guided Region Control for High-Fidelity
  Character Customization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Ma, Wenting Xu, Jiji Tang, Qinfeng Jin, Rongsheng Zhang, Zeng Zhao, Changjie Fan, Zhipeng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customized image generation, which seeks to synthesize images with consistent
characters, holds significant relevance for applications such as storytelling,
portrait generation, and character design. However, previous approaches have
encountered challenges in preserving characters with high-fidelity consistency
due to inadequate feature extraction and concept confusion of reference
characters. Therefore, we propose Character-Adapter, a plug-and-play framework
designed to generate images that preserve the details of reference characters,
ensuring high-fidelity consistency. Character-Adapter employs prompt-guided
segmentation to ensure fine-grained regional features of reference characters
and dynamic region-level adapters to mitigate concept confusion. Extensive
experiments are conducted to validate the effectiveness of Character-Adapter.
Both quantitative and qualitative results demonstrate that Character-Adapter
achieves the state-of-the-art performance of consistent character generation,
with an improvement of 24.8% compared with other methods. Our code will be
released at https://github.com/Character-Adapter/Character-Adapte
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11160v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11160v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengjin Xu, Muzhi Li, Cehao Yang, Xuhui Jiang, Lumingyuan Tang, Yiyan Qi, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs (KGs) are foundational structures in many AI applications,
representing entities and their interrelations through triples. However,
triple-based KGs lack the contextual information of relational knowledge, like
temporal dynamics and provenance details, which are crucial for comprehensive
knowledge representation and effective reasoning. Instead, \textbf{Context
Graphs} (CGs) expand upon the conventional structure by incorporating
additional information such as time validity, geographic location, and source
provenance. This integration provides a more nuanced and accurate understanding
of knowledge, enabling KGs to offer richer insights and support more
sophisticated reasoning processes. In this work, we first discuss the inherent
limitations of triple-based KGs and introduce the concept of CGs, highlighting
their advantages in knowledge representation and reasoning. We then present a
context graph reasoning \textbf{CGR$^3$} paradigm that leverages large language
models (LLMs) to retrieve candidate entities and related contexts, rank them
based on the retrieved information, and reason whether sufficient information
has been obtained to answer a query. Our experimental results demonstrate that
CGR$^3$ significantly improves performance on KG completion (KGC) and KG
question answering (KGQA) tasks, validating the effectiveness of incorporating
contextual information on KG representation and reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MolX: Enhancing Large Language Models for Molecular Learning with A
  Multi-Modal Extension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06777v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06777v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khiem Le, Zhichun Guo, Kaiwen Dong, Xiaobao Huang, Bozhao Nan, Roshni Iyer, Xiangliang Zhang, Olaf Wiest, Wei Wang, Nitesh V. Chawla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) with their strong task-handling
capabilities have shown remarkable advancements across a spectrum of fields,
moving beyond natural language understanding. However, their proficiency within
the chemistry domain remains restricted, especially in solving professional
molecule-related tasks. This challenge is attributed to their inherent
limitations in comprehending molecules using only common textual
representations, i.e., SMILES strings. In this study, we seek to enhance the
ability of LLMs to comprehend molecules by designing and equipping them with a
multi-modal external module, namely MolX. In particular, instead of directly
using a SMILES string to represent a molecule, we utilize specific encoders to
extract fine-grained features from both SMILES string and 2D molecular graph
representations for feeding into an LLM. Moreover, a human-defined molecular
fingerprint is incorporated to leverage its embedded domain knowledge. Then, to
establish an alignment between MolX and the LLM's textual input space, the
whole model in which the LLM is frozen, is pre-trained with a versatile
strategy including a diverse set of tasks. Extensive experimental evaluations
demonstrate that our proposed method only introduces a small number of
trainable parameters while outperforming baselines on various downstream
molecule-related tasks ranging from molecule-to-text translation to
retrosynthesis, with and without fine-tuning the LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating LLM Ethics: Advancements, Challenges, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Jiao, Saleh Afroogh, Yiming Xu, Connor Phillips
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses ethical issues surrounding Large Language Models (LLMs)
within the field of artificial intelligence. It explores the common ethical
challenges posed by both LLMs and other AI systems, such as privacy and
fairness, as well as ethical challenges uniquely arising from LLMs. It
highlights challenges such as hallucination, verifiable accountability, and
decoding censorship complexity, which are unique to LLMs and distinct from
those encountered in traditional AI systems. The study underscores the need to
tackle these complexities to ensure accountability, reduce biases, and enhance
transparency in the influential role that LLMs play in shaping information
dissemination. It proposes mitigation strategies and future directions for LLM
ethics, advocating for interdisciplinary collaboration. It recommends ethical
frameworks tailored to specific domains and dynamic auditing systems adapted to
diverse contexts. This roadmap aims to guide responsible development and
integration of LLMs, envisioning a future where ethical considerations govern
AI advancements in society.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The global landscape of academic guidelines for generative AI and Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Jiao, Saleh Afroogh, Kevin Chen, David Atkinson, Amit Dhurandhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Generative Artificial Intelligence (GAI) and Large
Language Models (LLMs) in academia has spurred a global discourse on their
potential pedagogical benefits and ethical considerations. Positive reactions
highlight some potential, such as collaborative creativity, increased access to
education, and empowerment of trainers and trainees. However, negative
reactions raise concerns about ethical complexities, balancing innovation and
academic integrity, unequal access, and misinformation risks. Through a
systematic survey and text-mining-based analysis of global and national
directives, insights from independent research, and eighty university-level
guidelines, this study provides a nuanced understanding of the opportunities
and challenges posed by GAI and LLMs in education. It emphasizes the importance
of balanced approaches that harness the benefits of these technologies while
addressing ethical considerations and ensuring equitable access and educational
outcomes. The paper concludes with recommendations for fostering responsible
innovation and ethical practices to guide the integration of GAI and LLMs in
academia.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02990v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02990v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen Chen, Wenhan Xia, Junjie Hu, Anh Tuan Luu, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving field of large language models (LLMs), data
augmentation (DA) has emerged as a pivotal technique for enhancing model
performance by diversifying training examples without the need for additional
data collection. This survey explores the transformative impact of LLMs on DA,
particularly addressing the unique challenges and opportunities they present in
the context of natural language processing (NLP) and beyond. From both data and
learning perspectives, we examine various strategies that utilize LLMs for data
augmentation, including a novel exploration of learning paradigms where
LLM-generated data is used for diverse forms of further training. Additionally,
this paper highlights the primary open challenges faced in this domain, ranging
from controllable data augmentation to multi-modal data augmentation. This
survey highlights a paradigm shift introduced by LLMs in DA, and aims to serve
as a comprehensive guide for researchers and practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Dynamics between Cobot's Production Rhythm, Locus of
  Control and Emotional State in a Collaborative Assembly Scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Mondellini, Matteo Lavit Nicora, Pooja Prajod, Elisabeth André, Rocco Vertechy, Alessandro Antonietti, Matteo Malosio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In industrial scenarios, there is widespread use of collaborative robots
(cobots), and growing interest is directed at evaluating and measuring the
impact of some characteristics of the cobot on the human factor. In the present
pilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 -
Adapted to the participant's pace) of a cobot has on the Experiential Locus of
Control (ELoC) and the emotional state of 31 participants has been examined.
The operators' performance, the degree of basic internal Locus of Control, and
the attitude towards the robots were also considered. No difference was found
regarding the emotional state and the ELoC in the three conditions, but
considering the other psychological variables, a more complex situation
emerges. Overall, results seem to indicate a need to consider the person's
psychological characteristics to offer a differentiated and optimal interaction
experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 4th IEEE International Conference on Human-Machine
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FloorSet -- a VLSI Floorplanning Dataset with Design Constraints of
  <span class="highlight-title">Real-World</span> SoCs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uday Mallappa, Hesham Mostafa, Mikhail Galkin, Mariano Phielipp, Somdeb Majumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Floorplanning for systems-on-a-chip (SoCs) and its sub-systems is a crucial
and non-trivial step of the physical design flow. It represents a difficult
combinatorial optimization problem. A typical large scale SoC with 120
partitions generates a search-space of nearly 10E250. As novel machine learning
(ML) approaches emerge to tackle such problems, there is a growing need for a
modern benchmark that comprises a large training dataset and performance
metrics that better reflect real-world constraints and objectives compared to
existing benchmarks. To address this need, we present FloorSet -- two
comprehensive datasets of synthetic fixed-outline floorplan layouts that
reflect the distribution of real SoCs. Each dataset has 1M training samples and
100 test samples where each sample is a synthetic floor-plan. FloorSet-Prime
comprises fully-abutted rectilinear partitions and near-optimal wire-length. A
simplified dataset that reflects early design phases, FloorSet-Lite comprises
rectangular partitions, with under 5 percent white-space and near-optimal
wire-length. Both datasets define hard constraints seen in modern design flows
such as shape constraints, edge-affinity, grouping constraints, and
pre-placement constraints. FloorSet is intended to spur fundamental research on
large-scale constrained optimization problems. Crucially, FloorSet alleviates
the core issue of reproducibility in modern ML driven solutions to such
problems. FloorSet is available as an open-source repository for the research
community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-27T00:00:00Z">2024-06-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Image and Video Processing <span class="chip" style="font-size: 60%">25</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Temporal Sequence Classification and Mathematical Modeling for Cell
  Tracking in Dense 3D Microscopy Videos of Bacterial Biofilms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanjin Taher Toma, Yibo Wang, Andreas Gahlmann, Scott T. Acton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic cell tracking in dense environments is plagued by inaccurate
correspondences and misidentification of parent-offspring relationships. In
this paper, we introduce a novel cell tracking algorithm named DenseTrack,
which integrates deep learning with mathematical model-based strategies to
effectively establish correspondences between consecutive frames and detect
cell division events in crowded scenarios. We formulate the cell tracking
problem as a deep learning-based temporal sequence classification task followed
by solving a constrained one-to-one matching optimization problem exploiting
the classifier's confidence scores. Additionally, we present an
eigendecomposition-based cell division detection strategy that leverages
knowledge of cellular geometry. The performance of the proposed approach has
been evaluated by tracking densely packed cells in 3D time-lapse image
sequences of bacterial biofilm development. The experimental results on
simulated as well as experimental fluorescence image sequences suggest that the
proposed tracking method achieves superior performance in terms of both
qualitative and quantitative evaluation measures compared to recent
state-of-the-art cell tracking approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-<span class="highlight-title">efficient</span> Active Illumination Camera For Hyper-spectral
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang, T. M. Sazzad, Yangyang Song, Spencer J. Chang, Ritesh Chowdhry, Tomas Mejia, Anna Hampton, Shelby Kucharski, Stefan Gerber, Barry Tillman, Marcio F. R. Resende, William M. Hammond, Chris H. Wilson, Alina Zare, Sanjeev J. Koppal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyper-spectral imaging has recently gained increasing attention for use in
different applications, including agricultural investigation, ground tracking,
remote sensing and many other. However, the high cost, large physical size and
complicated operation process stop hyperspectral cameras from being employed
for various applications and research fields. In this paper, we introduce a
cost-efficient, compact and easy to use active illumination camera that may
benefit many applications. We developed a fully functional prototype of such
camera. With the hope of helping with agricultural research, we tested our
camera for plant root imaging. In addition, a U-Net model for spectral
reconstruction was trained by using a reference hyperspectral camera's data as
ground truth and our camera's data as input. We demonstrated our camera's
ability to obtain additional information over a typical RGB camera. In
addition, the ability to reconstruct hyperspectral data from multi-spectral
input makes our device compatible to models and algorithms developed for
hyperspectral applications with no modifications required.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustness Testing of Black-Box Models Against CT Degradation Through
  Test-Time Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Highton, Quok Zong Chong, Samuel Finestone, Arian Beqiri, Julia A. Schnabel, Kanwal K. Bhatia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models for medical image segmentation and object detection are
becoming increasingly available as clinical products. However, as details are
rarely provided about the training data, models may unexpectedly fail when
cases differ from those in the training distribution. An approach allowing
potential users to independently test the robustness of a model, treating it as
a black box and using only a few cases from their own site, is key for
adoption. To address this, a method to test the robustness of these models
against CT image quality variation is presented. In this work we present this
framework by demonstrating that given the same training data, the model
architecture and data pre processing greatly affect the robustness of several
frequently used segmentation and object detection methods to simulated CT
imaging artifacts and degradation. Our framework also addresses the concern
about the sustainability of deep learning models in clinical use, by
considering future shifts in image quality due to scanner deterioration or
imaging protocol changes which are not reflected in a limited local test
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BOrg: A Brain Organoid-Based Mitosis Dataset for Automatic Analysis of
  Brain Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Awais, Mehaboobathunnisa Sahul Hameed, Bidisha Bhattacharya, Orly Reiner, Rao Muhammad Anwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have enabled the study of human brain development using brain
organoids derived from stem cells. Quantifying cellular processes like mitosis
in these organoids offers insights into neurodevelopmental disorders, but the
manual analysis is time-consuming, and existing datasets lack specific details
for brain organoid studies. We introduce BOrg, a dataset designed to study
mitotic events in the embryonic development of the brain using confocal
microscopy images of brain organoids. BOrg utilizes an efficient annotation
pipeline with sparse point annotations and techniques that minimize expert
effort, overcoming limitations of standard deep learning approaches on sparse
data. We adapt and benchmark state-of-the-art object detection and cell
counting models on BOrg for detecting and analyzing mitotic cells across
prophase, metaphase, anaphase, and telophase stages. Our results demonstrate
these adapted models significantly improve mitosis analysis efficiency and
accuracy for brain organoid research compared to existing methods. BOrg
facilitates the development of automated tools to quantify statistics like
mitosis rates, aiding mechanistic studies of neurodevelopmental processes and
disorders. Data and code are available at https://github.com/awaisrauf/borg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-resolution segmentations of the hypothalamus and its subregions for
  training of segmentation models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Livia Rodrigues, Martina Bocchetta, Oula Puonti, Douglas Greve, Ana Carolina Londe, Marcondes França, Simone Appenzeller, Leticia Rittner, Juan Eugenio Iglesias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of brain structures on magnetic resonance imaging (MRI) is a
highly relevant neuroimaging topic, as it is a prerequisite for different
analyses such as volumetry or shape analysis. Automated segmentation
facilitates the study of brain structures in larger cohorts when compared with
manual segmentation, which is time-consuming. However, the development of most
automated methods relies on large and manually annotated datasets, which limits
the generalizability of these methods. Recently, new techniques using synthetic
images have emerged, reducing the need for manual annotation. Here we provide
HELM, Hypothalamic ex vivo Label Maps, a dataset composed of label maps built
from publicly available ultra-high resolution ex vivo MRI from 10 whole
hemispheres, which can be used to develop segmentation methods using synthetic
data. The label maps are obtained with a combination of manual labels for the
hypothalamic regions and automated segmentations for the rest of the brain, and
mirrored to simulate entire brains. We also provide the pre-processed ex vivo
scans, as this dataset can support future projects to include other structures
after these are manually segmented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAPNet: Granularity <span class="highlight-title">Attention</span> Network with Anatomy-Prior-Constraint for
  Carotid Artery Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Zhang, Chenggang Lu, Xin-yang Shi, Caifeng Shan, Jiong Zhang, Da Chen, Laurent D. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Atherosclerosis is a chronic, progressive disease that primarily affects the
arterial walls. It is one of the major causes of cardiovascular disease.
Magnetic Resonance (MR) black-blood vessel wall imaging (BB-VWI) offers crucial
insights into vascular disease diagnosis by clearly visualizing vascular
structures. However, the complex anatomy of the neck poses challenges in
distinguishing the carotid artery (CA) from surrounding structures, especially
with changes like atherosclerosis. In order to address these issues, we propose
GAPNet, which is a consisting of a novel geometric prior deduced from.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALMA: a mathematics-driven approach for determining tuning parameters in
  generalized LASSO problems, with applications to MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Giacchi, Isidoros Iakovidis, Bastien Milani, Matthias Stuber, Micah Murray, Benedetta Franceschiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic Resonance Imaging (MRI) is a powerful technique employed for
non-invasive in vivo visualization of internal structures. Sparsity is often
deployed to accelerate the signal acquisition or overcome the presence of
motion artifacts, improving the quality of image reconstruction. Image
reconstruction algorithms use TV-regularized LASSO (Total Variation-regularized
LASSO) to retrieve the missing information of undersampled signals, by cleaning
the data of noise and while optimizing sparsity. A tuning parameter moderates
the balance between these two aspects; its choice affecting the quality of the
reconstructions. Currently, there is a lack of general deterministic techniques
to choose these parameters, which are oftentimes manually selected and thus
hinder the reliability of the reconstructions. Here, we present ALMA (Algorithm
for Lagrange Multipliers Approximation), an iterative mathematics-inspired
technique that computes tuning parameters for generalized LASSO problems during
MRI reconstruction. We analyze quantitatively the performance of these
parameters for imaging reconstructions via TV-LASSO in an MRI context on
phantoms. Although our study concentrates on TV-LASSO, the techniques developed
here hold significant promise for a wide array of applications. ALMA is not
only adaptable to more generalized LASSO problems but is also robust to
accommodate other forms of regularization beyond total variation. Moreover, it
extends effectively to handle non-Cartesian sampling trajectories, broadening
its utility in complex data reconstruction scenarios. More generally, ALMA
provides a powerful tool for numerically solving constrained optimization
problems across various disciplines, offering a versatile and impactful
solution for advanced computational challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Latent Stain Adaption for Digital Pathology <span class="chip">MICCAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Reisenbüchler, Lucas Luttner, Nadine S. Schaadt, Friedrich Feuerhake, Dorit Merhof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In digital pathology, deep learning (DL) models for tasks such as
segmentation or tissue classification are known to suffer from domain shifts
due to different staining techniques. Stain adaptation aims to reduce the
generalization error between different stains by training a model on source
stains that generalizes to target stains. Despite the abundance of target stain
data, a key challenge is the lack of annotations. To address this, we propose a
joint training between artificially labeled and unlabeled data including all
available stained images called Unsupervised Latent Stain Adaption (ULSA). Our
method uses stain translation to enrich labeled source images with synthetic
target images in order to increase supervised signals. Moreover, we leverage
unlabeled target stain images using stain-invariant feature consistency
learning. With ULSA we present a semi-supervised strategy for efficient stain
adaption without access to annotated target stain data. Remarkably, ULSA is
task agnostic in patch-level analysis for whole slide images (WSIs). Through
extensive evaluation on external datasets, we demonstrate that ULSA achieves
state-of-the-art (SOTA) performance in kidney tissue segmentation and breast
cancer classification across a spectrum of staining variations. Our findings
suggest that ULSA is an important framework towards stain adaption in digital
pathology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in MICCAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting
  Universal Machine Learning for Accelerated Cardiac MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Wang, Fanwen Wang, Chen Qin, Jun Lyu, Ouyang Cheng, Shuo Wang, Yan Li, Mengyao Yu, Haoyu Zhang, Kunyuan Guo, Zhang Shi, Qirong Li, Ziqiang Xu, Yajing Zhang, Hao Li, Sha Hua, Binghua Chen, Longyu Sun, Mengting Sun, Qin Li, Ying-Hua Chu, Wenjia Bai, Jing Qin, Xiahai Zhuang, Claudia Prieto, Alistair Young, Michael Markl, He Wang, Lianming Wu, Guang Yang, Xiaobo Qu, Chengyan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac magnetic resonance imaging (MRI) has emerged as a clinically
gold-standard technique for diagnosing cardiac diseases, thanks to its ability
to provide diverse information with multiple modalities and anatomical views.
Accelerated cardiac MRI is highly expected to achieve time-efficient and
patient-friendly imaging, and then advanced image reconstruction approaches are
required to recover high-quality, clinically interpretable images from
undersampled measurements. However, the lack of publicly available cardiac MRI
k-space dataset in terms of both quantity and diversity has severely hindered
substantial technological progress, particularly for data-driven artificial
intelligence. Here, we provide a standardized, diverse, and high-quality
CMRxRecon2024 dataset to facilitate the technical development, fair evaluation,
and clinical transfer of cardiac MRI reconstruction approaches, towards
promoting the universal frameworks that enable fast and robust reconstructions
across different cardiac MRI protocols in clinical practice. To the best of our
knowledge, the CMRxRecon2024 dataset is the largest and most diverse publicly
available cardiac k-space dataset. It is acquired from 330 healthy volunteers,
covering commonly used modalities, anatomical views, and acquisition
trajectories in clinical cardiac MRI workflows. Besides, an open platform with
tutorials, benchmarks, and data processing tools is provided to facilitate data
usage, advanced method development, and fair performance evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMR-<span class="highlight-title">Mamba</span>: Multi-Contrast MRI Reconstruction with <span class="highlight-title">Mamba</span> and
  Spatial-Frequency Information Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Zou, Lanqing Liu, Qi Chen, Shujun Wang, Xiaohan Xing, Jing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-contrast MRI acceleration has become prevalent in MR imaging, enabling
the reconstruction of high-quality MR images from under-sampled k-space data of
the target modality, using guidance from a fully-sampled auxiliary modality.
The main crux lies in efficiently and comprehensively integrating complementary
information from the auxiliary modality. Existing methods either suffer from
quadratic computational complexity or fail to capture long-range correlated
features comprehensively. In this work, we propose MMR-Mamba, a novel framework
that achieves comprehensive integration of multi-contrast features through
Mamba and spatial-frequency information fusion. Firstly, we design the
\textit{Target modality-guided Cross Mamba} (TCM) module in the spatial domain,
which maximally restores the target modality information by selectively
absorbing useful information from the auxiliary modality. Secondly, leveraging
global properties of the Fourier domain, we introduce the \textit{Selective
Frequency Fusion} (SFF) module to efficiently integrate global information in
the frequency domain and recover high-frequency signals for the reconstruction
of structure details. Additionally, we present the \textit{Adaptive
Spatial-Frequency Fusion} (ASFF) module, which enhances fused features by
supplementing less informative features from one domain with corresponding
features from the other domain. These innovative strategies ensure efficient
feature fusion across spatial and frequency domains, avoiding the introduction
of redundant information and facilitating the reconstruction of high-quality
target images. Extensive experiments on the BraTS and fastMRI knee datasets
demonstrate the superiority of the proposed MMR-Mamba over state-of-the-art MRI
reconstruction methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classification of Carotid Plaque with Jellyfish Sign Through
  Convolutional and Recurrent Neural Networks Utilizing Plaque Surface Edges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takeshi Yoshidomi, Shinji Kume, Hiroaki Aizawa, Akira Furui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In carotid arteries, plaque can develop as localized elevated lesions. The
Jellyfish sign, marked by fluctuating plaque surfaces with blood flow
pulsation, is a dynamic characteristic of these plaques that has recently
attracted attention. Detecting this sign is vital, as it is often associated
with cerebral infarction. This paper proposes an ultrasound video-based
classification method for the Jellyfish sign, using deep neural networks. The
proposed method first preprocesses carotid ultrasound videos to separate the
movement of the vascular wall from plaque movements. These preprocessed videos
are then combined with plaque surface information and fed into a deep learning
model comprising convolutional and recurrent neural networks, enabling the
efficient classification of the Jellyfish sign. The proposed method was
verified using ultrasound video images from 200 patients. Ablation studies
demonstrated the effectiveness of each component of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, accepted at IEEE EMBC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shorter SPECT Scans Using Self-supervised Coordinate Learning to
  Synthesize Skipped Projection Views 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyu Li, Yixuan Jia, Xiaojian Xu, Jason Hu, Jeffrey A. Fessler, Yuni K. Dewaraja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: This study addresses the challenge of extended SPECT imaging
duration under low-count conditions, as encountered in Lu-177 SPECT imaging, by
developing a self-supervised learning approach to synthesize skipped SPECT
projection views, thus shortening scan times in clinical settings. Methods: We
employed a self-supervised coordinate-based learning technique, adapting the
neural radiance field (NeRF) concept in computer vision to synthesize
under-sampled SPECT projection views. For each single scan, we used
self-supervised coordinate learning to estimate skipped SPECT projection views.
The method was tested with various down-sampling factors (DFs=2, 4, 8) on both
Lu-177 phantom SPECT/CT measurements and clinical SPECT/CT datasets, from 11
patients undergoing Lu-177 DOTATATE and 6 patients undergoing Lu-177 PSMA-617
radiopharmaceutical therapy. Results: For SPECT reconstructions, our method
outperformed the use of linearly interpolated projections and partial
projection views in relative contrast-to-noise-ratios (RCNR) averaged across
different downsampling factors: 1) DOTATATE: 83% vs. 65% vs. 67% for lesions
and 86% vs. 70% vs. 67% for kidney, 2) PSMA: 76% vs. 69% vs. 68% for lesions
and 75% vs. 55% vs. 66% for organs, including kidneys, lacrimal glands, parotid
glands, and submandibular glands. Conclusion: The proposed method enables
reduction in acquisition time (by factors of 2, 4, or 8) while maintaining
quantitative accuracy in clinical SPECT protocols by allowing for the
collection of fewer projections. Importantly, the self-supervised nature of
this NeRF-based approach eliminates the need for extensive training data,
instead learning from each patient's projection data alone. The reduction in
acquisition time is particularly relevant for imaging under low-count
conditions and for protocols that require multiple-bed positions such as
whole-body imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 5568 words</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-supervised variational autoencoder for cell feature extraction in
  multiplexed immunofluorescence images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piumi Sandarenu, Julia Chen, Iveta Slapetova, Lois Browne, Peter H. Graham, Alexander Swarbrick, Ewan K. A. Millar, Yang Song, Erik Meijering
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in digital imaging technologies have sparked increased interest
in using multiplexed immunofluorescence (mIF) images to visualise and identify
the interactions between specific immunophenotypes with the tumour
microenvironment at the cellular level. Current state-of-the-art multiplexed
immunofluorescence image analysis pipelines depend on cell feature
representations characterised by morphological and stain intensity-based
metrics generated using simple statistical and machine learning-based tools.
However, these methods are not capable of generating complex representations of
cells. We propose a deep learning-based cell feature extraction model using a
variational autoencoder with supervision using a latent subspace to extract
cell features in mIF images. We perform cell phenotype classification using a
cohort of more than 44,000 multiplexed immunofluorescence cell image patches
extracted across 1,093 tissue microarray cores of breast cancer patients, to
demonstrate the success of our model against current and alternative methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Jung Ling, Salomé Bru, Julia Puig, Florian Vixège, Simon Mendez, Franck Nicoud, Pierre-Yves Courand, Olivier Bernard, Damien Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify
color Doppler in cardiac imaging. In this study, we propose novel alternatives
to the traditional iVFM optimization scheme by utilizing physics-informed
neural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.
When evaluated on simulated color Doppler images derived from a
patient-specific computational fluid dynamics model and in vivo Doppler
acquisitions, both approaches demonstrate comparable reconstruction performance
to the original iVFM algorithm. The efficiency of PINNs is boosted through
dual-stage optimization and pre-optimized weights. On the other hand, the
nnU-Net method excels in generalizability and real-time capabilities. Notably,
nnU-Net shows superior robustness on sparse and truncated Doppler data while
maintaining independence from explicit boundary conditions. Overall, our
results highlight the effectiveness of these methods in reconstructing
intraventricular vector blood flow. The study also suggests potential
applications of PINNs in ultrafast color Doppler imaging and the incorporation
of fluid dynamics equations to derive biomarkers for cardiovascular diseases
based on blood flow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, accepted for publication in IEEE TUFFC; camera ready
  corrections, corrected acknowledgments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $μ$GUIDE: a framework for quantitative imaging via generalized
  uncertainty-driven inference using deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17293v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17293v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maëliss Jallais, Marco Palombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes $\mu$GUIDE: a general Bayesian framework to estimate
posterior distributions of tissue microstructure parameters from any given
biophysical model or MRI signal representation, with exemplar demonstration in
diffusion-weighted MRI. Harnessing a new deep learning architecture for
automatic signal feature selection combined with simulation-based inference and
efficient sampling of the posterior distributions, $\mu$GUIDE bypasses the high
computational and time cost of conventional Bayesian approaches and does not
rely on acquisition constraints to define model-specific summary statistics.
The obtained posterior distributions allow to highlight degeneracies present in
the model definition and quantify the uncertainty and ambiguity of the
estimated parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shortcut Learning in Medical Image Segmentation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manxi Lin, Nina Weng, Kamil Mikolaj, Zahra Bashir, Morten Bo Søndergaard Svendsen, Martin Tolsgaard, Anders Nymark Christensen, Aasa Feragen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shortcut learning is a phenomenon where machine learning models prioritize
learning simple, potentially misleading cues from data that do not generalize
well beyond the training set. While existing research primarily investigates
this in the realm of image classification, this study extends the exploration
of shortcut learning into medical image segmentation. We demonstrate that
clinical annotations such as calipers, and the combination of zero-padded
convolutions and center-cropped training sets in the dataset can inadvertently
serve as shortcuts, impacting segmentation accuracy. We identify and evaluate
the shortcut learning on two different but common medical image segmentation
tasks. In addition, we suggest strategies to mitigate the influence of shortcut
learning and improve the generalizability of the segmentation models. By
uncovering the presence and implications of shortcuts in medical image
segmentation, we provide insights and methodologies for evaluating and
overcoming this pervasive challenge and call for attention in the community for
shortcuts in segmentation. Our code is public at
https://github.com/nina-weng/shortcut_skinseg .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, accepted at MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Automotive Radar Detector using the RaDelft Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ignacio Roldan, Andras Palffy, Julian F. P. Kooij, Dariu M. Gavrila, Francesco Fioranelli, Alexander Yarovoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of multiple extended targets in complex environments using
high-resolution automotive radar is considered. A data-driven approach is
proposed where unlabeled synchronized lidar data is used as ground truth to
train a neural network with only radar data as input. To this end, the novel,
large-scale, real-life, and multi-sensor RaDelft dataset has been recorded
using a demonstrator vehicle in different locations in the city of Delft. The
dataset, as well as the documentation and example code, is publicly available
for those researchers in the field of automotive radar or machine perception.
The proposed data-driven detector is able to generate lidar-like point clouds
using only radar data from a high-resolution system, which preserves the shape
and size of extended targets. The results are compared against conventional
CFAR detectors as well as variations of the method to emulate the available
approaches in the literature, using the probability of detection, the
probability of false alarm, and the Chamfer distance as performance metrics.
Moreover, an ablation study was carried out to assess the impact of Doppler and
temporal information on detection performance. The proposed method outperforms
the different baselines in terms of Chamfer distance, achieving a reduction of
75% against conventional CFAR detectors and 10% against the modified
state-of-the-art deep learning-based approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at IEEE Transaction on Radar Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable <span class="highlight-title">Diffusion</span> Segmentation for Biomedical Images with Single-step
  Reverse Process <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Lin, Zhiguang Chen, Zhonghao Yan, Weijiang Yu, Fudan Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated their effectiveness across various
generative tasks. However, when applied to medical image segmentation, these
models encounter several challenges, including significant resource and time
requirements. They also necessitate a multi-step reverse process and multiple
samples to produce reliable predictions. To address these challenges, we
introduce the first latent diffusion segmentation model, named SDSeg, built
upon stable diffusion (SD). SDSeg incorporates a straightforward latent
estimation strategy to facilitate a single-step reverse process and utilizes
latent fusion concatenation to remove the necessity for multiple samples.
Extensive experiments indicate that SDSeg surpasses existing state-of-the-art
methods on five benchmark datasets featuring diverse imaging modalities.
Remarkably, SDSeg is capable of generating stable predictions with a solitary
reverse step and sample, epitomizing the model's stability as implied by its
name. The code is available at
https://github.com/lin-tianyu/Stable-Diffusion-Seg
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2024. Code and citation info see
  https://github.com/lin-tianyu/Stable-Diffusion-Seg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Uncertainty Estimation by Hamiltonian Monte Carlo: Applications
  to Cardiac MRI Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02311v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02311v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidong Zhao, Joao Tourais, Iain Pierce, Christian Nitsche, Thomas A. Treibel, Sebastian Weingärtner, Artur M. Schweidtmann, Qian Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL)-based methods have achieved state-of-the-art performance
for many medical image segmentation tasks. Nevertheless, recent studies show
that deep neural networks (DNNs) can be miscalibrated and overconfident,
leading to "silent failures" that are risky for clinical applications. Bayesian
DL provides an intuitive approach to DL failure detection, based on posterior
probability estimation. However, the posterior is intractable for large medical
image segmentation DNNs. To tackle this challenge, we propose a Bayesian
learning framework using Hamiltonian Monte Carlo (HMC), tempered by cold
posterior (CP) to accommodate medical data augmentation, named HMC-CP. For HMC
computation, we further propose a cyclical annealing strategy, capturing both
local and global geometries of the posterior distribution, enabling highly
efficient Bayesian DNN training with the same computational budget as training
a single DNN. The resulting Bayesian DNN outputs an ensemble segmentation along
with the segmentation uncertainty. We evaluate the proposed HMC-CP extensively
on cardiac magnetic resonance image (MRI) segmentation, using in-domain
steady-state free precession (SSFP) cine images as well as out-of-domain
datasets of quantitative T1 and T2 mapping. Our results show that the proposed
method improves both segmentation accuracy and uncertainty estimation for in-
and out-of-domain data, compared with well-established baseline methods such as
Monte Carlo Dropout and Deep Ensembles. Additionally, we establish a conceptual
link between HMC and the commonly known stochastic gradient descent (SGD) and
provide general insight into the uncertainty of DL. This uncertainty is
implicitly encoded in the training dynamics but often overlooked. With reliable
uncertainty estimation, our method provides a promising direction toward
trustworthy DL in clinical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024:011</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards AI Lesion Tracking in PET/CT Imaging: A Siamese-based CNN
  Pipeline applied on PSMA PET/CT Scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan P. Hein, Manuel Schultheiss, Andrei Gafita, Raphael Zaum, Farid Yagubbayli, Robert Tauber, Isabel Rauscher, Matthias Eiber, Franz Pfeiffer, Wolfgang A. Weber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing tumor response to systemic therapies is one of the main
applications of PET/CT. Routinely, only a small subset of index lesions out of
multiple lesions is analyzed. However, this operator dependent selection may
bias the results due to possible significant inter-metastatic heterogeneity of
response to therapy. Automated, AI based approaches for lesion tracking hold
promise in enabling the analysis of many more lesions and thus providing a
better assessment of tumor response. This work introduces a Siamese CNN
approach for lesion tracking between PET/CT scans. Our approach is applied on
the laborious task of tracking a high number of bone lesions in full-body
baseline and follow-up [68Ga]Ga- or [18F]F-PSMA PET/CT scans after two cycles
of [177Lu]Lu-PSMA therapy of metastatic castration resistant prostate cancer
patients. Data preparation includes lesion segmentation and affine
registration. Our algorithm extracts suitable lesion patches and forwards them
into a Siamese CNN trained to classify the lesion patch pairs as corresponding
or non-corresponding lesions. Experiments have been performed with different
input patch types and a Siamese network in 2D and 3D. The CNN model
successfully learned to classify lesion assignments, reaching a lesion tracking
accuracy of 83 % in its best configuration with an AUC = 0.91. For remaining
lesions the pipeline accomplished a re-identification rate of 89 %. We proved
that a CNN may facilitate the tracking of multiple lesions in PSMA PET/CT
scans. Future clinical studies are necessary if this improves the prediction of
the outcome of therapies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 9 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiDAR Depth Map Guided Image Compression Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06517v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06517v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Gnutti, Stefano Della Fiore, Mattia Savardi, Yi-Hsin Chen, Riccardo Leonardi, Wen-Hsiao Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The incorporation of LiDAR technology into some high-end smartphones has
unlocked numerous possibilities across various applications, including
photography, image restoration, augmented reality, and more. In this paper, we
introduce a novel direction that harnesses LiDAR depth maps to enhance the
compression of the corresponding RGB camera images. To the best of our
knowledge, this represents the initial exploration in this particular research
direction. Specifically, we propose a Transformer-based learned image
compression system capable of achieving variable-rate compression using a
single model while utilizing the LiDAR depth map as supplementary information
for both the encoding and decoding processes. Experimental results demonstrate
that integrating LiDAR yields an average PSNR gain of 0.83 dB and an average
bitrate reduction of 16% as compared to its absence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuous 3D Myocardial Motion Tracking via Echocardiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkang Shen, Hao Zhu, You Zhou, Yu Liu, Si Yi, Lili Dong, Weipeng Zhao, David J. Brady, Xun Cao, Zhan Ma, Yi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Myocardial motion tracking stands as an essential clinical tool in the
prevention and detection of cardiovascular diseases (CVDs), the foremost cause
of death globally. However, current techniques suffer from incomplete and
inaccurate motion estimation of the myocardium in both spatial and temporal
dimensions, hindering the early identification of myocardial dysfunction. To
address these challenges, this paper introduces the Neural Cardiac Motion Field
(NeuralCMF). NeuralCMF leverages implicit neural representation (INR) to model
the 3D structure and the comprehensive 6D forward/backward motion of the heart.
This method surpasses pixel-wise limitations by offering the capability to
continuously query the precise shape and motion of the myocardium at any
specific point throughout the cardiac cycle, enhancing the detailed analysis of
cardiac dynamics beyond traditional speckle tracking. Notably, NeuralCMF
operates without the need for paired datasets, and its optimization is
self-supervised through the physics knowledge priors in both space and time
dimensions, ensuring compatibility with both 2D and 3D echocardiogram video
inputs. Experimental validations across three representative datasets support
the robustness and innovative nature of the NeuralCMF, marking significant
advantages over existing state-of-the-art methods in cardiac imaging and motion
tracking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PlaNet-S: Automatic Semantic Segmentation of Placenta 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11580v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11580v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shinnosuke Yamamoto, Isso Saito, Eichi Takaya, Ayaka Harigai, Tomomi Sato, Tomoya Kobayashi, Kei Takase, Takuya Ueda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  [Purpose] To develop a fully automated semantic placenta segmentation model
that integrates the U-Net and SegNeXt architectures through ensemble learning.
[Methods] A total of 218 pregnant women with suspected placental anomalies who
underwent magnetic resonance imaging (MRI) were enrolled, yielding 1090
annotated images for developing a deep learning model for placental
segmentation. The images were standardized and divided into training and test
sets. The performance of PlaNet-S, which integrates U-Net and SegNeXt within an
ensemble framework, was assessed using Intersection over Union (IoU) and
counting connected components (CCC) against the U-Net model. [Results] PlaNet-S
had significantly higher IoU (0.73 +/- 0.13) than that of U-Net (0.78 +/-
0.010) (p<0.01). The CCC for PlaNet-S was significantly higher than that for
U-Net (p<0.01), matching the ground truth in 86.0\% and 56.7\% of the cases,
respectively. [Conclusion]PlaNet-S performed better than the traditional U-Net
in placental segmentation tasks. This model addresses the challenges of
time-consuming physician-assisted manual segmentation and offers the potential
for diverse applications in placental imaging analyses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, Shinnosuke Yamamoto and Isso Saito equally
  contributed to this work. In the original submission, there was a
  typographical error in the reported standard deviation for the Intersection
  over Union (IoU) values of the PlaNet-S model. The standard deviation was
  incorrectly listed as 0.01 instead of the correct value of 0.1. This has been
  corrected in the revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Twofold Structured Features-Based Siamese Network for Infrared Target
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Jie Yan, Yun-Kai Xu, Qian Chen, Xiao-Fang Kong, Guo-Hua Gu, A-Jun Shao, Min-Jie Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, infrared target tracking has been a critical technology in the
field of computer vision and has many applications, such as motion analysis,
pedestrian surveillance, intelligent detection, and so forth. Unfortunately,
due to the lack of color, texture and other detailed information, tracking
drift often occurs when the tracker encounters infrared targets that vary in
size or shape. To address this issue, we present a twofold structured
features-based Siamese network for infrared target tracking. First of all, in
order to improve the discriminative capacity for infrared targets, a novel
feature fusion network is proposed to fuse both shallow spatial information and
deep semantic information into the extracted features in a comprehensive
manner. Then, a multi-template update module based on template update mechanism
is designed to effectively deal with interferences from target appearance
changes which are prone to cause early tracking failures. Finally, both
qualitative and quantitative experiments are carried out on VOT-TIR 2016
dataset, which demonstrates that our method achieves the balance of promising
tracking performance and real-time tracking speed against other out-of-the-art
trackers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,9 figures,references added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FDDM: Unsupervised Medical Image Translation with a Frequency-Decoupled
  <span class="highlight-title">Diffusion</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiang Li, Hua-Chieh Shao, Xiaoxue Qian, You Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated significant potential in producing
high-quality images in medical image translation to aid disease diagnosis,
localization, and treatment. Nevertheless, current diffusion models have
limited success in achieving faithful image translations that can accurately
preserve the anatomical structures of medical images, especially for unpaired
datasets. The preservation of structural and anatomical details is essential to
reliable medical diagnosis and treatment planning, as structural mismatches can
lead to disease misidentification and treatment errors. In this study, we
introduce the Frequency Decoupled Diffusion Model (FDDM) for MR-to-CT
conversion. FDDM first obtains the anatomical information of the CT image from
the MR image through an initial conversion module. This anatomical information
then guides a subsequent diffusion model to generate high-quality CT images.
Our diffusion model uses a dual-path reverse diffusion process for
low-frequency and high-frequency information, achieving a better balance
between image quality and anatomical accuracy. We extensively evaluated FDDM
using public datasets for brain MR-to-CT and pelvis MR-to-CT translations,
demonstrating its superior performance to other GAN-based, VAE-based, and
diffusion-based models. The evaluation metrics included Frechet Inception
Distance (FID), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity
Index Measure (SSIM). FDDM achieved the best scores on all metrics for both
datasets, particularly excelling in FID, with scores of 25.9 for brain data and
29.2 for pelvis data, significantly outperforming other methods. These results
demonstrate that FDDM can generate high-quality target domain images while
maintaining the accuracy of translated anatomical structures.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">60</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PathAlign: A vision-language model for whole slide images in
  histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faruk Ahmed, Andrew Sellergren, Lin Yang, Shawn Xu, Boris Babenko, Abbi Ward, Niels Olson, Arash Mohtashamian, Yossi Matias, Greg S. Corrado, Quang Duong, Dale R. Webster, Shravya Shetty, Daniel Golden, Yun Liu, David F. Steiner, Ellery Wulczyn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microscopic interpretation of histopathology images underlies many important
diagnostic and treatment decisions. While advances in vision-language modeling
raise new opportunities for analysis of such images, the gigapixel-scale size
of whole slide images (WSIs) introduces unique challenges. Additionally,
pathology reports simultaneously highlight key findings from small regions
while also aggregating interpretation across multiple slides, often making it
difficult to create robust image-text pairs. As such, pathology reports remain
a largely untapped source of supervision in computational pathology, with most
efforts relying on region-of-interest annotations or self-supervision at the
patch-level. In this work, we develop a vision-language model based on the
BLIP-2 framework using WSIs paired with curated text from pathology reports.
This enables applications utilizing a shared image-text embedding space, such
as text or image retrieval for finding cases of interest, as well as
integration of the WSI encoder with a frozen large language model (LLM) for
WSI-based generative text capabilities such as report generation or
AI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000
WSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure
types, and tissue types. We present pathologist evaluation of text generation
and text retrieval using WSI embeddings, as well as results for WSI
classification and workflow prioritization (slide-level triaging).
Model-generated text for WSIs was rated by pathologists as accurate, without
clinically significant error or omission, for 78% of WSIs on average. This work
demonstrates exciting potential capabilities for language-aligned WSI
embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 main pages and 19 pages of supplemental material; 3 main tables, 3
  main figures and 11 supplemental tables, 7 supplemental figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Matters in Detecting AI-Generated Videos like Sora? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chirui Chang, Zhengzhe Liu, Xiaoyang Lyu, Xiaojuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion-based video generation have showcased
remarkable results, yet the gap between synthetic and real-world videos remains
under-explored. In this study, we examine this gap from three fundamental
perspectives: appearance, motion, and geometry, comparing real-world videos
with those generated by a state-of-the-art AI model, Stable Video Diffusion. To
achieve this, we train three classifiers using 3D convolutional networks, each
targeting distinct aspects: vision foundation model features for appearance,
optical flow for motion, and monocular depth for geometry. Each classifier
exhibits strong performance in fake video detection, both qualitatively and
quantitatively. This indicates that AI-generated videos are still easily
detectable, and a significant gap between real and fake videos persists.
Furthermore, utilizing the Grad-CAM, we pinpoint systematic failures of
AI-generated videos in appearance, motion, and geometry. Finally, we propose an
Ensemble-of-Experts model that integrates appearance, optical flow, and depth
information for fake video detection, resulting in enhanced robustness and
generalization ability. Our model is capable of detecting videos generated by
Sora with high accuracy, even without exposure to any Sora videos during
training. This suggests that the gap between real and fake videos can be
generalized across various video generative models. Project page:
https://justin-crchang.github.io/3DCNNDetection.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-<span class="highlight-title">efficient</span> Active Illumination Camera For Hyper-spectral
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang, T. M. Sazzad, Yangyang Song, Spencer J. Chang, Ritesh Chowdhry, Tomas Mejia, Anna Hampton, Shelby Kucharski, Stefan Gerber, Barry Tillman, Marcio F. R. Resende, William M. Hammond, Chris H. Wilson, Alina Zare, Sanjeev J. Koppal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyper-spectral imaging has recently gained increasing attention for use in
different applications, including agricultural investigation, ground tracking,
remote sensing and many other. However, the high cost, large physical size and
complicated operation process stop hyperspectral cameras from being employed
for various applications and research fields. In this paper, we introduce a
cost-efficient, compact and easy to use active illumination camera that may
benefit many applications. We developed a fully functional prototype of such
camera. With the hope of helping with agricultural research, we tested our
camera for plant root imaging. In addition, a U-Net model for spectral
reconstruction was trained by using a reference hyperspectral camera's data as
ground truth and our camera's data as input. We demonstrated our camera's
ability to obtain additional information over a typical RGB camera. In
addition, the ability to reconstruct hyperspectral data from multi-spectral
input makes our device compatible to models and algorithms developed for
hyperspectral applications with no modifications required.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustness Testing of Black-Box Models Against CT Degradation Through
  Test-Time Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack Highton, Quok Zong Chong, Samuel Finestone, Arian Beqiri, Julia A. Schnabel, Kanwal K. Bhatia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models for medical image segmentation and object detection are
becoming increasingly available as clinical products. However, as details are
rarely provided about the training data, models may unexpectedly fail when
cases differ from those in the training distribution. An approach allowing
potential users to independently test the robustness of a model, treating it as
a black box and using only a few cases from their own site, is key for
adoption. To address this, a method to test the robustness of these models
against CT image quality variation is presented. In this work we present this
framework by demonstrating that given the same training data, the model
architecture and data pre processing greatly affect the robustness of several
frequently used segmentation and object detection methods to simulated CT
imaging artifacts and degradation. Our framework also addresses the concern
about the sustainability of deep learning models in clinical use, by
considering future shifts in image quality due to scanner deterioration or
imaging protocol changes which are not reflected in a limited local test
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BOrg: A Brain Organoid-Based Mitosis Dataset for Automatic Analysis of
  Brain Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Awais, Mehaboobathunnisa Sahul Hameed, Bidisha Bhattacharya, Orly Reiner, Rao Muhammad Anwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances have enabled the study of human brain development using brain
organoids derived from stem cells. Quantifying cellular processes like mitosis
in these organoids offers insights into neurodevelopmental disorders, but the
manual analysis is time-consuming, and existing datasets lack specific details
for brain organoid studies. We introduce BOrg, a dataset designed to study
mitotic events in the embryonic development of the brain using confocal
microscopy images of brain organoids. BOrg utilizes an efficient annotation
pipeline with sparse point annotations and techniques that minimize expert
effort, overcoming limitations of standard deep learning approaches on sparse
data. We adapt and benchmark state-of-the-art object detection and cell
counting models on BOrg for detecting and analyzing mitotic cells across
prophase, metaphase, anaphase, and telophase stages. Our results demonstrate
these adapted models significantly improve mitosis analysis efficiency and
accuracy for brain organoid research compared to existing methods. BOrg
facilitates the development of automated tools to quantify statistics like
mitosis rates, aiding mechanistic studies of neurodevelopmental processes and
disorders. Data and code are available at https://github.com/awaisrauf/borg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weighted Circle Fusion: Ensembling Circle Representation from Different
  Object Detection Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialin Yue, Tianyuan Yao, Ruining Deng, Quan Liu, Juming Xiong, Haichun Yang, Yuankai Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the use of circle representation has emerged as a method to improve
the identification of spherical objects (such as glomeruli, cells, and nuclei)
in medical imaging studies. In traditional bounding box-based object detection,
combining results from multiple models improves accuracy, especially when
real-time processing isn't crucial. Unfortunately, this widely adopted strategy
is not readily available for combining circle representations. In this paper,
we propose Weighted Circle Fusion (WCF), a simple approach for merging
predictions from various circle detection models. Our method leverages
confidence scores associated with each proposed bounding circle to generate
averaged circles. Our method undergoes thorough evaluation on a proprietary
dataset for glomerular detection in object detection within whole slide imaging
(WSI). The findings reveal a performance gain of 5 %, respectively, compared to
existing ensemble methods. Furthermore, the Weighted Circle Fusion technique
not only improves the precision of object detection in medical images but also
notably decreases false detections, presenting a promising direction for future
research and application in pathological image analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Analysis Of Color Models For Human Perception And Visual
  Color Difference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aruzhan Burambekova, Pakizar Shamoi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color is integral to human experience, influencing emotions, decisions, and
perceptions. This paper presents a comparative analysis of various color
models' alignment with human visual perception. The study evaluates color
models such as RGB, HSV, HSL, XYZ, CIELAB, and CIELUV to assess their
effectiveness in accurately representing how humans perceive color. We evaluate
each model based on its ability to accurately reflect visual color differences
and dominant palette extraction compatible with the human eye. In image
processing, accurate assessment of color difference is essential for
applications ranging from digital design to quality control. Current color
difference metrics do not always match how people see colors, causing issues in
accurately judging subtle differences. Understanding how different color models
align with human visual perception is crucial for various applications in image
processing, digital media, and design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been submitted to EJMCA journal for consideration.
  Current version is a preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stereo Vision Based Robot for Remote Monitoring with VR Support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Fazil M. S., Arockia Selvakumar A., Daniel Schilberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The machine vision systems have been playing a significant role in visual
monitoring systems. With the help of stereovision and machine learning, it will
be able to mimic human-like visual system and behaviour towards the
environment. In this paper, we present a stereo vision based 3-DOF robot which
will be used to monitor places from remote using cloud server and internet
devices. The 3-DOF robot will transmit human-like head movements, i.e., yaw,
pitch, roll and produce 3D stereoscopic video and stream it in Real-time. This
video stream is sent to the user through any generic internet devices with VR
box support, i.e., smartphones giving the user a First-person real-time 3D
experience and transfers the head motion of the user to the robot also in
Real-time. The robot will also be able to track moving objects and faces as a
target using deep neural networks which enables it to be a standalone
monitoring robot. The user will be able to choose specific subjects to monitor
in a space. The stereovision enables us to track the depth information of
different objects detected and will be used to track human interest objects
with its distances and sent to the cloud. A full working prototype is developed
which showcases the capabilities of a monitoring system based on stereo vision,
robotics, and machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 Pages, 10 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-resolution segmentations of the hypothalamus and its subregions for
  training of segmentation models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Livia Rodrigues, Martina Bocchetta, Oula Puonti, Douglas Greve, Ana Carolina Londe, Marcondes França, Simone Appenzeller, Leticia Rittner, Juan Eugenio Iglesias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segmentation of brain structures on magnetic resonance imaging (MRI) is a
highly relevant neuroimaging topic, as it is a prerequisite for different
analyses such as volumetry or shape analysis. Automated segmentation
facilitates the study of brain structures in larger cohorts when compared with
manual segmentation, which is time-consuming. However, the development of most
automated methods relies on large and manually annotated datasets, which limits
the generalizability of these methods. Recently, new techniques using synthetic
images have emerged, reducing the need for manual annotation. Here we provide
HELM, Hypothalamic ex vivo Label Maps, a dataset composed of label maps built
from publicly available ultra-high resolution ex vivo MRI from 10 whole
hemispheres, which can be used to develop segmentation methods using synthetic
data. The label maps are obtained with a combination of manual labels for the
hypothalamic regions and automated segmentations for the rest of the brain, and
mirrored to simulate entire brains. We also provide the pre-processed ex vivo
scans, as this dataset can support future projects to include other structures
after these are manually segmented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAPNet: Granularity <span class="highlight-title">Attention</span> Network with Anatomy-Prior-Constraint for
  Carotid Artery Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Zhang, Chenggang Lu, Xin-yang Shi, Caifeng Shan, Jiong Zhang, Da Chen, Laurent D. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Atherosclerosis is a chronic, progressive disease that primarily affects the
arterial walls. It is one of the major causes of cardiovascular disease.
Magnetic Resonance (MR) black-blood vessel wall imaging (BB-VWI) offers crucial
insights into vascular disease diagnosis by clearly visualizing vascular
structures. However, the complex anatomy of the neck poses challenges in
distinguishing the carotid artery (CA) from surrounding structures, especially
with changes like atherosclerosis. In order to address these issues, we propose
GAPNet, which is a consisting of a novel geometric prior deduced from.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyi Liu, Cheng Chi, Eric Cousineau, Naveen Kuppuswamy, Benjamin Burchfiel, Shuran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio signals provide rich information for the robot interaction and object
properties through contact. These information can surprisingly ease the
learning of contact-rich robot manipulation skills, especially when the visual
information alone is ambiguous or incomplete. However, the usage of audio data
in robot manipulation has been constrained to teleoperated demonstrations
collected by either attaching a microphone to the robot or object, which
significantly limits its usage in robot learning pipelines. In this work, we
introduce ManiWAV: an 'ear-in-hand' data collection device to collect
in-the-wild human demonstrations with synchronous audio and visual feedback,
and a corresponding policy interface to learn robot manipulation policy
directly from the demonstrations. We demonstrate the capabilities of our system
through four contact-rich manipulation tasks that require either passively
sensing the contact events and modes, or actively sensing the object surface
materials and states. In addition, we show that our system can generalize to
unseen in-the-wild environments, by learning from diverse in-the-wild human
demonstrations. Project website: https://mani-wav.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Efficient</span> and Distributed Large-Scale 3D Map Registration using
  Tomographic Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Halil Utku Unlu, Anthony Tzes, Prashanth Krishnamurthy, Farshad Khorrami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A robust, resource-efficient, distributed, and minimally parameterized 3D map
matching and merging algorithm is proposed. The suggested algorithm utilizes
tomographic features from 2D projections of horizontal cross-sections of
gravity-aligned local maps, and matches these projection slices at all possible
height differences, enabling the estimation of four degrees of freedom in an
efficient and parallelizable manner. The advocated algorithm improves
state-of-the-art feature extraction and registration pipelines by an order of
magnitude in memory use and execution time. Experimental studies are offered to
investigate the efficiency of this 3D map merging scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Elsevier Journal: Robotics and Autonomous Systems (RAS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dataset Size Recovery from LoRA Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Salama, Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model inversion and membership inference attacks aim to reconstruct and
verify the data which a model was trained on. However, they are not guaranteed
to find all training samples as they do not know the size of the training set.
In this paper, we introduce a new task: dataset size recovery, that aims to
determine the number of samples used to train a model, directly from its
weights. We then propose DSiRe, a method for recovering the number of images
used to fine-tune a model, in the common case where fine-tuning uses LoRA. We
discover that both the norm and the spectrum of the LoRA matrices are closely
linked to the fine-tuning dataset size; we leverage this finding to propose a
simple yet effective prediction algorithm. To evaluate dataset size recovery of
LoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of
over 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.
Our best classifier can predict the number of fine-tuning images with a mean
absolute error of 0.36 images, establishing the feasibility of this attack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HUWSOD: Holistic Self-training for Unified Weakly Supervised Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liujuan Cao, Jianghang Lin, Zebo Hong, Yunhang Shen, Shaohui Lin, Chao Chen, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most WSOD methods rely on traditional object proposals to generate candidate
regions and are confronted with unstable training, which easily gets stuck in a
poor local optimum. In this paper, we introduce a unified, high-capacity weakly
supervised object detection (WSOD) network called HUWSOD, which utilizes a
comprehensive self-training framework without needing external modules or
additional supervision. HUWSOD innovatively incorporates a self-supervised
proposal generator and an autoencoder proposal generator with a multi-rate
resampling pyramid to replace traditional object proposals, enabling end-to-end
WSOD training and inference. Additionally, we implement a holistic
self-training scheme that refines detection scores and coordinates through
step-wise entropy minimization and consistency-constraint regularization,
ensuring consistent predictions across stochastic augmentations of the same
image. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSOD
competes with state-of-the-art WSOD methods, eliminating the need for offline
proposals and additional data. The peak performance of HUWSOD approaches that
of fully-supervised Faster R-CNN. Our findings also indicate that randomly
initialized boxes, although significantly different from well-designed offline
object proposals, are effective for WSOD training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Sanity Check for AI-generated Image Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilin Yan, Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of generative models, discerning AI-generated
content has evoked increasing attention from both industry and academia. In
this paper, we conduct a sanity check on "whether the task of AI-generated
image detection has been solved". To start with, we present Chameleon dataset,
consisting AIgenerated images that are genuinely challenging for human
perception. To quantify the generalization of existing methods, we evaluate 9
off-the-shelf AI-generated image detectors on Chameleon dataset. Upon analysis,
almost all models classify AI-generated images as real ones. Later, we propose
AIDE (AI-generated Image DEtector with Hybrid Features), which leverages
multiple experts to simultaneously extract visual artifacts and noise patterns.
Specifically, to capture the high-level semantics, we utilize CLIP to compute
the visual embedding. This effectively enables the model to discern
AI-generated images based on semantics or contextual information; Secondly, we
select the highest frequency patches and the lowest frequency patches in the
image, and compute the low-level patchwise features, aiming to detect
AI-generated images by low-level artifacts, for example, noise pattern,
anti-aliasing, etc. While evaluating on existing benchmarks, for example,
AIGCDetectBenchmark and GenImage, AIDE achieves +3.5% and +4.6% improvements to
state-of-the-art methods, and on our proposed challenging Chameleon benchmarks,
it also achieves the promising results, despite this problem for detecting
AI-generated images is far from being solved. The dataset, codes, and pre-train
models will be published at https://github.com/shilinyan99/AIDE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://shilinyan99.github.io/AIDE Code:
  https://github.com/shilinyan99/AIDE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looking 3D: Anomaly Detection with 2D-3D Alignment <span class="chip">CVPR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankan Bhunia, Changjian Li, Hakan Bilen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic anomaly detection based on visual cues holds practical significance
in various domains, such as manufacturing and product quality assessment. This
paper introduces a new conditional anomaly detection problem, which involves
identifying anomalies in a query image by comparing it to a reference shape. To
address this challenge, we have created a large dataset, BrokenChairs-180K,
consisting of around 180K images, with diverse anomalies, geometries, and
textures paired with 8,143 reference 3D shapes. To tackle this task, we have
proposed a novel transformer-based approach that explicitly learns the
correspondence between the query image and reference 3D shape via feature
alignment and leverages a customized attention mechanism for anomaly detection.
Our approach has been rigorously evaluated through comprehensive experiments,
serving as a benchmark for future research in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR'24. Codes & dataset available at
  https://github.com/VICO-UoE/Looking3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jr-Jen Chen, Yu-Chien Liao, Hsi-Che Lin, Yu-Chu Yu, Yen-Chun Chen, Yu-Chiang Frank Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ReXTime, a benchmark designed to rigorously test AI models'
ability to perform temporal reasoning within video events. Specifically,
ReXTime focuses on reasoning across time, i.e. human-like understanding when
the question and its corresponding answer occur in different video segments.
This form of reasoning, requiring advanced understanding of cause-and-effect
relationships across video segments, poses significant challenges to even the
frontier multimodal large language models. To facilitate this evaluation, we
develop an automated pipeline for generating temporal reasoning question-answer
pairs, significantly reducing the need for labor-intensive manual annotations.
Our benchmark includes 921 carefully vetted validation samples and 2,143 test
samples, each manually curated for accuracy and relevance. Evaluation results
show that while frontier large language models outperform academic models, they
still lag behind human performance by a significant 14.3% accuracy gap.
Additionally, our pipeline creates a training dataset of 9,695 machine
generated samples without manual effort, which empirical studies suggest can
enhance the across-time reasoning via fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fibottention: Inceptive Visual Representation Learning with Diverse
  <span class="highlight-title">Attention</span> Across Heads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Khaleghi Rahimian, Manish Kumar Govind, Subhajit Maity, Dominick Reilly, Christian Kümmerle, Srijan Das, Aritra Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual perception tasks are predominantly solved by Vision Transformer (ViT)
architectures, which, despite their effectiveness, encounter a computational
bottleneck due to the quadratic complexity of computing self-attention. This
inefficiency is largely due to the self-attention heads capturing redundant
token interactions, reflecting inherent redundancy within visual data. Many
works have aimed to reduce the computational complexity of self-attention in
ViTs, leading to the development of efficient and sparse transformer
architectures. In this paper, viewing through the efficiency lens, we realized
that introducing any sparse self-attention strategy in ViTs can keep the
computational overhead low. However, these strategies are sub-optimal as they
often fail to capture fine-grained visual details. This observation leads us to
propose a general, efficient, sparse architecture, named Fibottention, for
approximating self-attention with superlinear complexity that is built upon
Fibonacci sequences. The key strategies in Fibottention include: it excludes
proximate tokens to reduce redundancy, employs structured sparsity by design to
decrease computational demands, and incorporates inception-like diversity
across attention heads. This diversity ensures the capture of complementary
information through non-overlapping token interactions, optimizing both
performance and resource utilization in ViTs for visual representation
learning. We embed our Fibottention mechanism into multiple state-of-the-art
transformer architectures dedicated to visual tasks. Leveraging only 2-6% of
the elements in the self-attention heads, Fibottention in conjunction with ViT
and its variants, consistently achieves significant performance boosts compared
to standard ViTs in nine datasets across three domains $\unicode{x2013}$ image
classification, video understanding, and robot learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is publicly available at
  https://github.com/Charlotte-CharMLab/Fibottention</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SALVe: Semantic Alignment Verification for Floorplan Reconstruction from
  Sparse Panoramas <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Lambert, Yuguang Li, Ivaylo Boyadzhiev, Lambert Wixson, Manjunath Narayana, Will Hutchcroft, James Hays, Frank Dellaert, Sing Bing Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new system for automatic 2D floorplan reconstruction that is
enabled by SALVe, our novel pairwise learned alignment verifier. The inputs to
our system are sparsely located 360$^\circ$ panoramas, whose semantic features
(windows, doors, and openings) are inferred and used to hypothesize pairwise
room adjacency or overlap. SALVe initializes a pose graph, which is
subsequently optimized using GTSAM. Once the room poses are computed, room
layouts are inferred using HorizonNet, and the floorplan is constructed by
stitching the most confident layout boundaries. We validate our system
qualitatively and quantitatively as well as through ablation studies, showing
that it outperforms state-of-the-art SfM systems in completeness by over 200%,
without sacrificing accuracy. Our results point to the significance of our
work: poses of 81% of panoramas are localized in the first 2 connected
components (CCs), and 89% in the first 3 CCs. Code and models are publicly
available at https://github.com/zillow/salve.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current universal segmentation methods demonstrate strong capabilities in
pixel-level image and video understanding. However, they lack reasoning
abilities and cannot be controlled via text instructions. In contrast, large
vision-language multimodal models exhibit powerful vision-based conversation
and reasoning capabilities but lack pixel-level understanding and have
difficulty accepting visual prompts for flexible user interaction. This paper
proposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level
vision understanding with reasoning abilities. It can accept various visual and
text prompts for flexible user interaction. Specifically, we use a universal
segmentation method as the visual encoder, integrating image information,
perception priors, and visual prompts into visual tokens provided to the LLM.
The LLM is responsible for understanding the user's text instructions and
providing text responses and pixel-level segmentation results based on the
visual information. We propose perception prior embedding to better integrate
perception priors with image features. OMG-LLaVA achieves image-level,
object-level, and pixel-level reasoning and understanding in a single model,
matching or surpassing the performance of specialized methods on multiple
benchmarks. Rather than using LLM to connect each specialist, our work aims at
end-to-end training on one encoder, one decoder, and one LLM. The code and
model have been released for further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Data and <span class="highlight-title">Transformer</span>s for Audio Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Guha Balakrishnan, Sergey Tulyakov, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating ambient sounds and effects is a challenging problem due to data
scarcity and often insufficient caption quality, making it difficult to employ
large-scale generative models for the task. In this work, we tackle the problem
by introducing two new models. First, we propose AutoCap, a high-quality and
efficient automatic audio captioning model. We show that by leveraging metadata
available with the audio modality, we can substantially improve the quality of
captions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from
the best available captioning model at four times faster inference speed. We
then use AutoCap to caption clips from existing datasets, obtaining 761,000
audio clips with high-quality captions, forming the largest available
audio-text dataset. Second, we propose GenAu, a scalable transformer-based
audio generation architecture that we scale up to 1.25B parameters and train
with our new dataset. When compared to state-of-the-art audio generators, GenAu
obtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%
in CLAP score, indicating significantly improved quality of generated audio
compared to previous works. This shows that the quality of data is often as
important as its quantity. Besides, since AutoCap is fully automatic, new audio
samples can be added to the training dataset, unlocking the training of even
larger generative models for audio synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage: https://snap-research.github.io/GenAU/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Mamba</span> or RWKV: Exploring High-Quality and High-Efficiency Segment
  Anything Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobo Yuan, Xiangtai Li, Lu Qi, Tao Zhang, Ming-Hsuan Yang, Shuicheng Yan, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based segmentation methods face the challenge of efficient
inference when dealing with high-resolution images. Recently, several linear
attention architectures, such as Mamba and RWKV, have attracted much attention
as they can process long sequences efficiently. In this work, we focus on
designing an efficient segment-anything model by exploring these different
architectures. Specifically, we design a mixed backbone that contains
convolution and RWKV operation, which achieves the best for both accuracy and
efficiency. In addition, we design an efficient decoder to utilize the
multiscale tokens to obtain high-quality masks. We denote our method as
RWKV-SAM, a simple, effective, fast baseline for SAM-like models. Moreover, we
build a benchmark containing various high-quality segmentation datasets and
jointly train one efficient yet high-quality segmentation model using this
benchmark. Based on the benchmark results, our RWKV-SAM achieves outstanding
performance in efficiency and segmentation quality compared to transformers and
other linear attention models. For example, compared with the same-scale
transformer model, RWKV-SAM achieves more than 2x speedup and can achieve
better segmentation performance on various datasets. In addition, RWKV-SAM
outperforms recent vision Mamba models with better classification and semantic
segmentation results. Code and models will be publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages; 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via
  Collaborating Self-Training and Adversarial Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Zhang, Chao Zhou, Di Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing 3D object detection suffers from expensive annotation costs and poor
transferability to unknown data due to the domain gap, Unsupervised Domain
Adaptation (UDA) aims to generalize detection models trained in labeled source
domains to perform robustly on unexplored target domains, providing a promising
solution for cross-domain 3D object detection. Although Self-Training (ST)
based cross-domain 3D detection methods with the assistance of pseudo-labeling
techniques have achieved remarkable progress, they still face the issue of
low-quality pseudo-labels when there are significant domain disparities due to
the absence of a process for feature distribution alignment. While Adversarial
Learning (AL) based methods can effectively align the feature distributions of
the source and target domains, the inability to obtain labels in the target
domain forces the adoption of asymmetric optimization losses, resulting in a
challenging issue of source domain bias. To overcome these limitations, we
propose a novel unsupervised domain adaptation framework for 3D object
detection via collaborating ST and AL, dubbed as STAL3D, unleashing the
complementary advantages of pseudo labels and feature distribution alignment.
Additionally, a Background Suppression Adversarial Learning (BS-AL) module and
a Scale Filtering Module (SFM) are designed tailored for 3D cross-domain
scenes, effectively alleviating the issues of the large proportion of
background interference and source domain size bias. Our STAL3D achieves
state-of-the-art performance on multiple cross-domain tasks and even surpasses
the Oracle results on Waymo $\rightarrow$ KITTI and Waymo $\rightarrow$
KITTI-rain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE-TIV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative
  Object REarrangement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengwen Zhang, Yun Liu, Ruofan Xing, Bingda Tang, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how humans cooperatively rearrange household objects is
critical for VR/AR and human-robot interaction. However, in-depth studies on
modeling these behaviors are under-researched due to the lack of relevant
datasets. We fill this gap by presenting CORE4D, a novel large-scale 4D
human-object-human interaction dataset focusing on collaborative object
rearrangement, which encompasses diverse compositions of various object
geometries, collaboration modes, and 3D scenes. With 1K human-object-human
motion sequences captured in the real world, we enrich CORE4D by contributing
an iterative collaboration retargeting strategy to augment motions to a variety
of novel objects. Leveraging this approach, CORE4D comprises a total of 11K
collaboration sequences spanning 3K real and virtual object shapes. Benefiting
from extensive motion patterns provided by CORE4D, we benchmark two tasks
aiming at generating human-object interaction: human-object motion forecasting
and interaction synthesis. Extensive experiments demonstrate the effectiveness
of our collaboration retargeting strategy and indicate that CORE4D has posed
new challenges to existing human-object interaction generation methodologies.
Our dataset and code are available at
https://github.com/leolyliu/CORE4D-Instructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Visual Conditioning Tokens to Correct Domain Shift for Fully
  Test-time Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushun Tang, Shuoshuo Chen, Zhehan Kan, Yi Zhang, Qinghai Guo, Zhihai He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fully test-time adaptation aims to adapt the network model based on
sequential analysis of input samples during the inference stage to address the
cross-domain performance degradation problem of deep neural networks. This work
is based on the following interesting finding: in transformer-based image
classification, the class token at the first transformer encoder layer can be
learned to capture the domain-specific characteristics of target samples during
test-time adaptation. This learned token, when combined with input image patch
embeddings, is able to gradually remove the domain-specific information from
the feature representations of input samples during the transformer encoding
process, thereby significantly improving the test-time adaptation performance
of the source model across different domains. We refer to this class token as
visual conditioning token (VCT). To successfully learn the VCT, we propose a
bi-level learning approach to capture the long-term variations of
domain-specific characteristics while accommodating local variations of
instance-specific characteristics. Experimental results on the benchmark
datasets demonstrate that our proposed bi-level visual conditioning token
learning method is able to achieve significantly improved test-time adaptation
performance by up to 1.9%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by TMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Efficient</span> World Models with Context-Aware Tokenization <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Micheli, Eloi Alonso, François Fleuret
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up deep Reinforcement Learning (RL) methods presents a significant
challenge. Following developments in generative modelling, model-based RL
positions itself as a strong contender. Recent advances in sequence modelling
have led to effective transformer-based world models, albeit at the price of
heavy computations due to the long sequences of tokens required to accurately
simulate environments. In this work, we propose $\Delta$-IRIS, a new agent with
a world model architecture composed of a discrete autoencoder that encodes
stochastic deltas between time steps and an autoregressive transformer that
predicts future deltas by summarizing the current state of the world with
continuous tokens. In the Crafter benchmark, $\Delta$-IRIS sets a new state of
the art at multiple frame budgets, while being an order of magnitude faster to
train than previous attention-based approaches. We release our code and models
at https://github.com/vmicheli/delta-iris.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Data Transfer Cooperating with Artificial Triplets for Scene
  Graph Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        KuanChao Chu, Satoshi Yamazaki, Hideki Nakayama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work focuses on training dataset enhancement of informative relational
triplets for Scene Graph Generation (SGG). Due to the lack of effective
supervision, the current SGG model predictions perform poorly for informative
relational triplets with inadequate training samples. Therefore, we propose two
novel training dataset enhancement modules: Feature Space Triplet Augmentation
(FSTA) and Soft Transfer. FSTA leverages a feature generator trained to
generate representations of an object in relational triplets. The biased
prediction based sampling in FSTA efficiently augments artificial triplets
focusing on the challenging ones. In addition, we introduce Soft Transfer,
which assigns soft predicate labels to general relational triplets to make more
supervisions for informative predicate classes effectively. Experimental
results show that integrating FSTA and Soft Transfer achieve high levels of
both Recall and mean Recall in Visual Genome dataset. The mean of Recall and
mean Recall is the highest among all the existing model-agnostic methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEICE Transactions on Information and Systems in April
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping Land Naturalness from Sentinel-2 using Deep Contextual and
  Geographical Priors <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burak Ekim, Michael Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent decades, the causes and consequences of climate change have
accelerated, affecting our planet on an unprecedented scale. This change is
closely tied to the ways in which humans alter their surroundings. As our
actions continue to impact natural areas, using satellite images to observe and
measure these effects has become crucial for understanding and combating
climate change. Aiming to map land naturalness on the continuum of modern human
pressure, we have developed a multi-modal supervised deep learning framework
that addresses the unique challenges of satellite data and the task at hand. We
incorporate contextual and geographical priors, represented by corresponding
coordinate information and broader contextual information, including and
surrounding the immediate patch to be predicted. Our framework improves the
model's predictive performance in mapping land naturalness from Sentinel-2
data, a type of multi-spectral optical satellite imagery. Recognizing that our
protective measures are only as effective as our understanding of the
ecosystem, quantifying naturalness serves as a crucial step toward enhancing
our environmental stewardship.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, ICLR 2024 Tackling Climate Change with Machine
  Learning Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PNeRV: A Polynomial Neural Representation for Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonam Gupta, Snehal Singh Tomar, Grigorios G Chrysos, Sukhendu Das, A. N. Rajagopalan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting Implicit Neural Representations (INRs) on video data poses unique
challenges due to the additional temporal dimension. In the context of videos,
INRs have predominantly relied on a frame-only parameterization, which
sacrifices the spatiotemporal continuity observed in pixel-level (spatial)
representations. To mitigate this, we introduce Polynomial Neural
Representation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR
for videos that preserves spatiotemporal continuity. PNeRV leverages the
modeling capabilities of Polynomial Neural Networks to perform the modulation
of a continuous spatial (patch) signal with a continuous time (frame) signal.
We further propose a custom Hierarchical Patch-wise Spatial Sampling Scheme
that ensures spatial continuity while retaining parameter efficiency. We also
employ a carefully designed Positional Embedding methodology to further enhance
PNeRV's performance. Our extensive experimentation demonstrates that PNeRV
outperforms the baselines in conventional Implicit Neural Representation tasks
like compression along with downstream applications that require spatiotemporal
continuity in the underlying representation. PNeRV not only addresses the
challenges posed by video data in the realm of INRs but also opens new avenues
for advanced video processing and analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 17 figures, published at TMLR, Feb 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compositional Image Decomposition with <span class="highlight-title">Diffusion</span> Models <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jocelin Su, Nan Liu, Yanbo Wang, Joshua B. Tenenbaum, Yilun Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given an image of a natural scene, we are able to quickly decompose it into a
set of components such as objects, lighting, shadows, and foreground. We can
then envision a scene where we combine certain components with those from other
images, for instance a set of objects from our bedroom and animals from a zoo
under the lighting conditions of a forest, even if we have never encountered
such a scene before. In this paper, we present a method to decompose an image
into such compositional components. Our approach, Decomp Diffusion, is an
unsupervised method which, when given a single image, infers a set of different
components in the image, each represented by a diffusion model. We demonstrate
how components can capture different factors of the scene, ranging from global
scene descriptors like shadows or facial expression to local scene descriptors
like constituent objects. We further illustrate how inferred factors can be
flexibly composed, even with factors inferred from other models, to generate a
variety of scenes sharply different than those seen in training time. Website
and code at https://energy-based-model.github.io/decomp-diffusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024, Webpage:
  https://energy-based-model.github.io/decomp-diffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Continual Learning in Visual Question Answering with
  Modality-Aware Feature <span class="highlight-title">Distillation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malvina Nikandrou, Georgios Pantazopoulos, Ioannis Konstas, Alessandro Suglia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual learning focuses on incrementally training a model on a sequence of
tasks with the aim of learning new tasks while minimizing performance drop on
previous tasks. Existing approaches at the intersection of Continual Learning
and Visual Question Answering (VQA) do not study how the multimodal nature of
the input affects the learning dynamics of a model. In this paper, we
demonstrate that each modality evolves at different rates across a continuum of
tasks and that this behavior occurs in established encoder-only models as well
as modern recipes for developing Vision & Language (VL) models. Motivated by
this observation, we propose a modality-aware feature distillation (MAFED)
approach which outperforms existing baselines across models of varying scale in
three multimodal continual learning settings. Furthermore, we provide ablations
showcasing that modality-aware distillation complements experience replay.
Overall, our results emphasize the importance of addressing modality-specific
dynamics to prevent forgetting in multimodal continual learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Modelling and Pose Estimation Overview 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pawel Knap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human modelling and pose estimation stands at the crossroads of Computer
Vision, Computer Graphics, and Machine Learning. This paper presents a thorough
investigation of this interdisciplinary field, examining various algorithms,
methodologies, and practical applications. It explores the diverse range of
sensor technologies relevant to this domain and delves into a wide array of
application areas. Additionally, we discuss the challenges and advancements in
2D and 3D human modelling methodologies, along with popular datasets, metrics,
and future research directions. The main contribution of this paper lies in its
up-to-date comparison of state-of-the-art (SOTA) human pose estimation
algorithms in both 2D and 3D domains. By providing this comprehensive overview,
the paper aims to enhance understanding of 3D human modelling and pose
estimation, offering insights into current SOTA achievements, challenges, and
future prospects within the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into
  Multimodal LLMs at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of multimodal large language models (MLLMs), such as
GPT-4V, has led to significant advancements. However, these models still face
challenges in medical multimodal capabilities due to limitations in the
quantity and quality of medical vision-text data, stemming from data privacy
concerns and high annotation costs. While pioneering approaches utilize
PubMed's large-scale, de-identified medical image-text pairs to address these
limitations, they still fall short due to inherent data noise. To tackle this,
we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in
an 'unblinded' capacity to denoise and reformat the data, resulting in the
creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our
validation demonstrates that: (1) PubMedVision can significantly enhance the
medical multimodal capabilities of current MLLMs, showing significant
improvement in benchmarks including the MMMU Health & Medicine track; (2)
manual checks by medical experts and empirical results validate the superior
data quality of our dataset compared to other data construction methods. Using
PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows
superior performance in medical multimodal scenarios among open-source MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens
  Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Fan, Lei Ding, Ching-Chen Kuo, Shan Jiang, Yang Zhao, Xinze Guan, Jie Yang, Yi Zhang, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical User Interfaces (GUIs) are central to our interaction with digital
devices. Recently, growing efforts have been made to build models for various
GUI understanding tasks. However, these efforts largely overlook an important
GUI-referring task: screen reading based on user-indicated points, which we
name the Screen Point-and-Read (SPR) task. This task is predominantly handled
by rigid accessible screen reading tools, in great need of new models driven by
advancements in Multimodal Large Language Models (MLLMs). In this paper, we
propose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,
to address the SPR task. Based on the input point coordinate and the
corresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout
Tree. Based on the tree, our ToL agent not only comprehends the content of the
indicated area but also articulates the layout and spatial relationships
between elements. Such layout information is crucial for accurately
interpreting information on the screen, distinguishing our ToL agent from other
screen reading tools. We also thoroughly evaluate the ToL agent against other
baselines on a newly proposed SPR benchmark, which includes GUIs from mobile,
web, and operating systems. Last but not least, we test the ToL agent on mobile
GUI navigation tasks, demonstrating its utility in identifying incorrect
actions along the path of agent execution trajectories. Code and data:
screen-point-and-read.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Video-Language Representations with Structural Spatio-Temporal
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While pre-training large-scale video-language models (VLMs) has shown
remarkable potential for various downstream video-language tasks, existing VLMs
can still suffer from certain commonly seen limitations, e.g., coarse-grained
cross-modal aligning , under-modeling of temporal dynamics, detached
video-language view. In this work, we target enhancing VLMs with a fine-grained
structural spatio-temporal alignment learning method (namely Finsta). First of
all, we represent the input texts and videos with fine-grained scene graph (SG)
structures, both of which are further unified into a holistic SG (HSG) for
bridging two modalities. Then, an SG-based framework is built, where the
textual SG (TSG) is encoded with a graph Transformer, while the video dynamic
SG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for
spatial and temporal feature propagation. A spatial-temporal Gaussian
differential graph Transformer is further devised to strengthen the sense of
the changes in objects across spatial and temporal dimensions. Next, based on
the fine-grained structural features of TSG and DSG, we perform object-centered
spatial alignment and predicate-centered temporal alignment respectively,
enhancing the video-language grounding in both the spatiality and temporality.
We design our method as a plug&play system, which can be integrated into
existing well-trained VLMs for further representation augmentation, without
training from scratch or relying on SG annotations in downstream applications.
On 6 representative VL modeling tasks over 12 datasets in both standard and
long-form video scenarios, Finsta consistently improves the existing 13
strong-performing VLMs persistently, and refreshes the current state-of-the-art
end task performance significantly in both the fine-tuning and zero-shot
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Manifold Learning for No-Reference Image Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timin Gao, Wensheng Pan, Yan Zhang, Sicheng Zhao, Shengchuan Zhang, Xiawu Zheng, Ke Li, Liujuan Cao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has considerably advanced the field of Image Quality
Assessment (IQA), emerging as a widely adopted technique. The core mechanism of
contrastive learning involves minimizing the distance between quality-similar
(positive) examples while maximizing the distance between quality-dissimilar
(negative) examples. Despite its successes, current contrastive learning
methods often neglect the importance of preserving the local manifold
structure. This oversight can result in a high degree of similarity among hard
examples within the feature space, thereby impeding effective differentiation
and assessment. To address this issue, we propose an innovative framework that
integrates local manifold learning with contrastive learning for No-Reference
Image Quality Assessment (NR-IQA). Our method begins by sampling multiple crops
from a given image, identifying the most visually salient crop. This crop is
then used to cluster other crops from the same image as the positive class,
while crops from different images are treated as negative classes to increase
inter-class distance. Uniquely, our approach also considers non-saliency crops
from the same image as intra-class negative classes to preserve their
distinctiveness. Additionally, we employ a mutual learning framework, which
further enhances the model's ability to adaptively learn and identify visual
saliency regions. Our approach demonstrates a better performance compared to
state-of-the-art methods in 7 standard datasets, achieving PLCC values of 0.942
(compared to 0.908 in TID2013) and 0.914 (compared to 0.894 in LIVEC).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALMA: a mathematics-driven approach for determining tuning parameters in
  generalized LASSO problems, with applications to MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Giacchi, Isidoros Iakovidis, Bastien Milani, Matthias Stuber, Micah Murray, Benedetta Franceschiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic Resonance Imaging (MRI) is a powerful technique employed for
non-invasive in vivo visualization of internal structures. Sparsity is often
deployed to accelerate the signal acquisition or overcome the presence of
motion artifacts, improving the quality of image reconstruction. Image
reconstruction algorithms use TV-regularized LASSO (Total Variation-regularized
LASSO) to retrieve the missing information of undersampled signals, by cleaning
the data of noise and while optimizing sparsity. A tuning parameter moderates
the balance between these two aspects; its choice affecting the quality of the
reconstructions. Currently, there is a lack of general deterministic techniques
to choose these parameters, which are oftentimes manually selected and thus
hinder the reliability of the reconstructions. Here, we present ALMA (Algorithm
for Lagrange Multipliers Approximation), an iterative mathematics-inspired
technique that computes tuning parameters for generalized LASSO problems during
MRI reconstruction. We analyze quantitatively the performance of these
parameters for imaging reconstructions via TV-LASSO in an MRI context on
phantoms. Although our study concentrates on TV-LASSO, the techniques developed
here hold significant promise for a wide array of applications. ALMA is not
only adaptable to more generalized LASSO problems but is also robust to
accommodate other forms of regularization beyond total variation. Moreover, it
extends effectively to handle non-Cartesian sampling trajectories, broadening
its utility in complex data reconstruction scenarios. More generally, ALMA
provides a powerful tool for numerically solving constrained optimization
problems across various disciplines, offering a versatile and impactful
solution for advanced computational challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Aware Vision-and-Language Navigation: Bridging Simulation to
  Reality with Dynamic Human Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghan Li, Heng Li, Zhi-Qi Cheng, Yifei Dong, Yuxuan Zhou, Jun-Yan He, Qi Dai, Teruko Mitamura, Alexander G. Hauptmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) aims to develop embodied agents that
navigate based on human instructions. However, current VLN frameworks often
rely on static environments and optimal expert supervision, limiting their
real-world applicability. To address this, we introduce Human-Aware
Vision-and-Language Navigation (HA-VLN), extending traditional VLN by
incorporating dynamic human activities and relaxing key assumptions. We propose
the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities
with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R)
dataset, extending R2R with human activity descriptions. To tackle HA-VLN
challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and
Non-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing
cross-modal fusion and diverse training strategies for effective navigation in
dynamic human environments. A comprehensive evaluation, including metrics
considering human activities, and systematic analysis of HA-VLN's unique
challenges, underscores the need for further research to enhance HA-VLN agents'
real-world robustness and adaptability. Ultimately, this work provides
benchmarks and insights for future research on embodied AI and Sim2Real
transfer, paving the way for more realistic and applicable VLN systems in
human-populated environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 18 figures, Project Page:
  https://lpercc.github.io/HA3D_simulator/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model
  for Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nazanin Moradinasab, Laura S. Shankman, Rebecca A. Deaton, Gary K. Owens, Donald E. Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptive semantic segmentation aims to generate accurate and dense
predictions for an unlabeled target domain by leveraging a supervised model
trained on a labeled source domain. The prevalent self-training approach
involves retraining the dense discriminative classifier of $p(class|pixel
feature)$ using the pseudo-labels from the target domain. While many methods
focus on mitigating the issue of noisy pseudo-labels, they often overlook the
underlying data distribution p(pixel feature|class) in both the source and
target domains. To address this limitation, we propose the multi-prototype
Gaussian-Mixture-based (ProtoGMM) model, which incorporates the GMM into
contrastive losses to perform guided contrastive learning. Contrastive losses
are commonly executed in the literature using memory banks, which can lead to
class biases due to underrepresented classes. Furthermore, memory banks often
have fixed capacities, potentially restricting the model's ability to capture
diverse representations of the target/source domains. An alternative approach
is to use global class prototypes (i.e. averaged features per category).
However, the global prototypes are based on the unimodal distribution
assumption per class, disregarding within-class variation. To address these
challenges, we propose the ProtoGMM model. This novel approach involves
estimating the underlying multi-prototype source distribution by utilizing the
GMM on the feature space of the source samples. The components of the GMM model
act as representative prototypes. To achieve increased intra-class semantic
similarity, decreased inter-class similarity, and domain alignment between the
source and target domains, we employ multi-prototype contrastive learning
between source distribution and target samples. The experiments show the
effectiveness of our method on UDA benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think Step by Step: Chain-of-Gesture Prompting for Error Detection in
  Robotic Surgical Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimin Shao, Jialang Xu, Danail Stoyanov, Evangelos B. Mazomenos, Yueming Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in robotic systems and surgical data
science, ensuring safe and optimal execution in robot-assisted minimally
invasive surgery (RMIS) remains a complex challenge. Current surgical error
detection methods involve two parts: identifying surgical gestures and then
detecting errors within each gesture clip. These methods seldom consider the
rich contextual and semantic information inherent in surgical videos, limiting
their performance due to reliance on accurate gesture identification. Motivated
by the chain-of-thought prompting in natural language processing, this letter
presents a novel and real-time end-to-end error detection framework,
Chain-of-Thought (COG) prompting, leveraging contextual information from
surgical videos. This encompasses two reasoning modules designed to mimic the
decision-making processes of expert surgeons. Concretely, we first design a
Gestural-Visual Reasoning module, which utilizes transformer and attention
architectures for gesture prompting, while the second, a Multi-Scale Temporal
Reasoning module, employs a multi-stage temporal convolutional network with
both slow and fast paths for temporal information extraction. We extensively
validate our method on the public benchmark RMIS dataset JIGSAWS. Our method
encapsulates the reasoning processes inherent to surgical activities enabling
it to outperform the state-of-the-art by 4.6% in F1 score, 4.6% in Accuracy,
and 5.9% in Jaccard index while processing each frame in 6.69 milliseconds on
average, demonstrating the great potential of our approach in enhancing the
safety and efficacy of RMIS procedures and surgical education. The code will be
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Reducing Data Acquisition and Labeling for Defect Detection
  using Simulated Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Malte Kemeter, Rasmus Hvingelby, Paulina Sierak, Tobias Schön, Bishwajit Gosswam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many manufacturing settings, annotating data for machine learning and
computer vision is costly, but synthetic data can be generated at significantly
lower cost. Substituting the real-world data with synthetic data is therefore
appealing for many machine learning applications that require large amounts of
training data. However, relying solely on synthetic data is frequently
inadequate for effectively training models that perform well on real-world
data, primarily due to domain shifts between the synthetic and real-world data.
We discuss approaches for dealing with such a domain shift when detecting
defects in X-ray scans of aluminium wheels. Using both simulated and real-world
X-ray images, we train an object detection model with different strategies to
identify the training approach that generates the best detection results while
minimising the demand for annotated real-world training samples. Our
preliminary findings suggest that the sim-2-real domain adaptation approach is
more cost-efficient than a fully supervised oracle - if the total number of
available annotated samples is fixed. Given a certain number of labeled
real-world samples, training on a mix of synthetic and unlabeled real-world
data achieved comparable or even better detection results at significantly
lower cost. We argue that future research into the cost-efficiency of different
training strategies is important for a better understanding of how to allocate
budget in applied machine learning projects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single Image Estimation of Cell Migration Direction by Deep Circular
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Bruns, Lucas Lamparter, Milos Galic, Xiaoyi Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we study the problem of estimating the migration direction of
cells based on a single image. To the best of our knowledge, there is only one
related work that uses a classification CNN for four classes (quadrants). This
approach does not allow detailed directional resolution. We solve the single
image estimation problem using deep circular regression with special attention
to cycle-sensitive methods. On two databases we achieve an average accuracy of
$\sim$17 degrees, which is a significant improvement over the previous work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAVEN: Multitask Retrieval Augmented Vision-Language Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Nagaraj Rao, Siddharth Choudhary, Aditya Deshpande, Ravi Kumar Satzoda, Srikar Appalaraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling of large language models to encode all the world's knowledge in
model parameters is unsustainable and has exacerbated resource barriers.
Retrieval-Augmented Generation (RAG) presents a potential solution, yet its
application to vision-language models (VLMs) is under explored. Existing
methods focus on models designed for single tasks. Furthermore, they're limited
by the need for resource intensive pre training, additional parameter
requirements, unaddressed modality prioritization and lack of clear benefit
over non-retrieval baselines. This paper introduces RAVEN, a multitask
retrieval augmented VLM framework that enhances base VLMs through efficient,
task specific fine-tuning. By integrating retrieval augmented samples without
the need for additional retrieval-specific parameters, we show that the model
acquires retrieval properties that are effective across multiple tasks. Our
results and extensive ablations across retrieved modalities for the image
captioning and VQA tasks indicate significant performance improvements compared
to non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a
+3\% accuracy on specific VQA question types. This underscores the efficacy of
applying RAG approaches to VLMs, marking a stride toward more efficient and
accessible multimodal learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Utilizing Adversarial Examples for Bias Mitigation and Accuracy
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11819v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11819v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pushkar Shukla, Dhruv Srikanth, Lee Cohen, Matthew Turk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach to mitigate biases in computer vision models by
utilizing counterfactual generation and fine-tuning. While counterfactuals have
been used to analyze and address biases in DNN models, the counterfactuals
themselves are often generated from biased generative models, which can
introduce additional biases or spurious correlations. To address this issue, we
propose using adversarial images, that is images that deceive a deep neural
network but not humans, as counterfactuals for fair model training. Our
approach leverages a curriculum learning framework combined with a fine-grained
adversarial loss to fine-tune the model using adversarial examples. By
incorporating adversarial images into the training data, we aim to prevent
biases from propagating through the pipeline. We validate our approach through
both qualitative and quantitative assessments, demonstrating improved bias
mitigation and accuracy compared to existing methods. Qualitatively, our
results indicate that post-training, the decisions made by the model are less
dependent on the sensitive attribute and our model better disentangles the
relationship between sensitive attributes and classification variables.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-supervised variational autoencoder for cell feature extraction in
  multiplexed immunofluorescence images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piumi Sandarenu, Julia Chen, Iveta Slapetova, Lois Browne, Peter H. Graham, Alexander Swarbrick, Ewan K. A. Millar, Yang Song, Erik Meijering
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in digital imaging technologies have sparked increased interest
in using multiplexed immunofluorescence (mIF) images to visualise and identify
the interactions between specific immunophenotypes with the tumour
microenvironment at the cellular level. Current state-of-the-art multiplexed
immunofluorescence image analysis pipelines depend on cell feature
representations characterised by morphological and stain intensity-based
metrics generated using simple statistical and machine learning-based tools.
However, these methods are not capable of generating complex representations of
cells. We propose a deep learning-based cell feature extraction model using a
variational autoencoder with supervision using a latent subspace to extract
cell features in mIF images. We perform cell phenotype classification using a
cohort of more than 44,000 multiplexed immunofluorescence cell image patches
extracted across 1,093 tissue microarray cores of breast cancer patients, to
demonstrate the success of our model against current and alternative methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FishNet: Deep Neural Networks for Low-Cost Fish Stock Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moseli Mots'oehli, Anton Nikolaev, Wawan B. IGede, John Lynham, Peter J. Mous, Peter Sadowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fish stock assessment often involves manual fish counting by taxonomy
specialists, which is both time-consuming and costly. We propose FishNet, an
automated computer vision system for both taxonomic classification and fish
size estimation from images captured with a low-cost digital camera. The system
first performs object detection and segmentation using a Mask R-CNN to identify
individual fish from images containing multiple fish, possibly consisting of
different species. Then each fish species is classified and the length is
predicted using separate machine learning models. To develop the model, we use
a dataset of 300,000 hand-labeled images containing 1.2M fish of 163 different
species and ranging in length from 10cm to 250cm, with additional annotations
and quality control methods used to curate high-quality training data. On
held-out test data sets, our system achieves a 92% intersection over union on
the fish segmentation task, a 89% top-1 classification accuracy on single fish
species classification, and a 2.3cm mean absolute error on the fish length
estimation task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE COINS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Step Differences in Instructional Video <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tushar Nagarajan, Lorenzo Torresani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comparing a user video to a reference how-to video is a key requirement for
AR/VR technology delivering personalized assistance tailored to the user's
progress. However, current approaches for language-based assistance can only
answer questions about a single video. We propose an approach that first
automatically generates large amounts of visual instruction tuning data
involving pairs of videos from HowTo100M by leveraging existing step
annotations and accompanying narrations, and then trains a video-conditioned
language model to jointly reason across multiple raw videos. Our model achieves
state-of-the-art performance at identifying differences between video pairs and
ranking videos based on the severity of these differences, and shows promising
ability to perform general reasoning over multiple videos. Project page:
https://github.com/facebookresearch/stepdiff
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Stackable and Skippable LEGO Bricks for <span class="highlight-title">Efficient</span>,
  Reconfigurable, and Variable-Resolution <span class="highlight-title">Diffusion</span> Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06389v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06389v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huangjie Zheng, Zhendong Wang, Jianbo Yuan, Guanghan Ning, Pengcheng He, Quanzeng You, Hongxia Yang, Mingyuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models excel at generating photo-realistic images but come with
significant computational costs in both training and sampling. While various
techniques address these computational challenges, a less-explored issue is
designing an efficient and adaptable network backbone for iterative refinement.
Current options like U-Net and Vision Transformer often rely on
resource-intensive deep networks and lack the flexibility needed for generating
images at variable resolutions or with a smaller network than used in training.
This study introduces LEGO bricks, which seamlessly integrate Local-feature
Enrichment and Global-content Orchestration. These bricks can be stacked to
create a test-time reconfigurable diffusion backbone, allowing selective
skipping of bricks to reduce sampling costs and generate higher-resolution
images than the training data. LEGO bricks enrich local regions with an MLP and
transform them using a Transformer block while maintaining a consistent
full-resolution image across all bricks. Experimental results demonstrate that
LEGO bricks enhance training efficiency, expedite convergence, and facilitate
variable-resolution image generation while maintaining strong generative
performance. Moreover, LEGO significantly reduces sampling time compared to
other methods, establishing it as a valuable enhancement for diffusion models.
Our code and project page are available at
https://jegzheng.github.io/LEGODiffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Semantic Equivalence of Tokenization in Multimodal LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05127v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05127v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have demonstrated exceptional
capabilities in processing vision-language tasks. One of the crux of MLLMs lies
in vision tokenization, which involves efficiently transforming input visual
signals into feature representations that are most beneficial for LLMs.
However, existing vision tokenizers, essential for semantic alignment between
vision and language, remain problematic. Existing methods aggressively fragment
visual input, corrupting the visual semantic integrity. To address this, this
paper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),
which groups visual features into semantic units via a dynamic clustering
algorithm, flexibly determining the number of tokens based on image complexity.
The resulting vision tokens effectively preserve semantic integrity and capture
both low-frequency and high-frequency visual features. The proposed MLLM
(Setokim) equipped with SeTok significantly demonstrates superior performance
across various tasks, as evidenced by our experimental results. The project
page is at https://chocowu.github.io/SeTok-web/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report. The project page:
  https://chocowu.github.io/SeTok-web/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Jung Ling, Salomé Bru, Julia Puig, Florian Vixège, Simon Mendez, Franck Nicoud, Pierre-Yves Courand, Olivier Bernard, Damien Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify
color Doppler in cardiac imaging. In this study, we propose novel alternatives
to the traditional iVFM optimization scheme by utilizing physics-informed
neural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.
When evaluated on simulated color Doppler images derived from a
patient-specific computational fluid dynamics model and in vivo Doppler
acquisitions, both approaches demonstrate comparable reconstruction performance
to the original iVFM algorithm. The efficiency of PINNs is boosted through
dual-stage optimization and pre-optimized weights. On the other hand, the
nnU-Net method excels in generalizability and real-time capabilities. Notably,
nnU-Net shows superior robustness on sparse and truncated Doppler data while
maintaining independence from explicit boundary conditions. Overall, our
results highlight the effectiveness of these methods in reconstructing
intraventricular vector blood flow. The study also suggests potential
applications of PINNs in ultrafast color Doppler imaging and the incorporation
of fluid dynamics equations to derive biomarkers for cardiovascular diseases
based on blood flow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, accepted for publication in IEEE TUFFC; camera ready
  corrections, corrected acknowledgments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VDebugger: Harnessing Execution Feedback for Debugging Visual Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueqing Wu, Zongyu Lin, Songyan Zhao, Te-Lin Wu, Pan Lu, Nanyun Peng, Kai-Wei Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual programs are executable code generated by large language models to
address visual reasoning problems. They decompose complex questions into
multiple reasoning steps and invoke specialized models for each step to solve
the problems. However, these programs are prone to logic errors, with our
preliminary evaluation showing that 58% of the total errors are caused by
program logic errors. Debugging complex visual programs remains a major
bottleneck for visual reasoning. To address this, we introduce VDebugger, a
novel critic-refiner framework trained to localize and debug visual programs by
tracking execution step by step. VDebugger identifies and corrects program
errors leveraging detailed execution feedback, improving interpretability and
accuracy. The training data is generated through an automated pipeline that
injects errors into correct visual programs using a novel mask-best decoding
technique. Evaluations on six datasets demonstrate VDebugger's effectiveness,
showing performance improvements of up to 3.2% in downstream task accuracy.
Further studies show VDebugger's ability to generalize to unseen tasks,
bringing a notable improvement of 2.3% on the unseen COVR task. Code, data and
models are made publicly available at https://github.com/shirley-wu/vdebugger/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>update reference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpatialBot: Precise Spatial Understanding with Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13642v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13642v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiao Cai, Yaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, Bo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have achieved impressive performance in 2D
image understanding, however they are still struggling with spatial
understanding which is the foundation of Embodied AI. In this paper, we propose
SpatialBot for better spatial understanding by feeding both RGB and depth
images. Additionally, we have constructed the SpatialQA dataset, which involves
multi-level depth-related questions to train VLMs for depth understanding.
Finally, we present SpatialBench to comprehensively evaluate VLMs' capabilities
in spatial understanding at different levels. Extensive experiments on our
spatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks,
demonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The
model, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Muffin or Chihuahua? Challenging Multimodal Large Language Models with
  Multipanel VQA <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15847v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15847v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Xinze Guan, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multipanel images, commonly seen as web screenshots, posters, etc., pervade
our daily lives. These images, characterized by their composition of multiple
subfigures in distinct layouts, effectively convey information to people.
Toward building advanced multimodal AI applications, such as agents that
understand complex scenes and navigate through webpages, the skill of
multipanel visual reasoning is essential, and a comprehensive evaluation of
models in this regard is important. Therefore, we introduce Multipanel Visual
Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets
of questions, answers, and multipanel images that specifically challenge models
in comprehending multipanel images. Our evaluation shows that questions in the
MultipanelVQA benchmark pose significant challenges to the state-of-the-art
Multimodal Large Language Models (MLLMs) tested, even though humans can attain
approximately 99% accuracy on these questions. Distinctively, the MultipanelVQA
benchmark features synthetically generated multipanel images specifically
crafted to isolate and assess the impact of various factors, such as the
layout, on MLLMs' multipanel image comprehension abilities. As a result, in
addition to benchmarking the capabilities of MLLMs in understanding multipanel
images, we analyze various factors of the multipanel image that affect MLLMs'
performance with synthetic data and offer insights for enhancement. Code and
data are released at https://sites.google.com/view/multipanelvqa/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shortcut Learning in Medical Image Segmentation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manxi Lin, Nina Weng, Kamil Mikolaj, Zahra Bashir, Morten Bo Søndergaard Svendsen, Martin Tolsgaard, Anders Nymark Christensen, Aasa Feragen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shortcut learning is a phenomenon where machine learning models prioritize
learning simple, potentially misleading cues from data that do not generalize
well beyond the training set. While existing research primarily investigates
this in the realm of image classification, this study extends the exploration
of shortcut learning into medical image segmentation. We demonstrate that
clinical annotations such as calipers, and the combination of zero-padded
convolutions and center-cropped training sets in the dataset can inadvertently
serve as shortcuts, impacting segmentation accuracy. We identify and evaluate
the shortcut learning on two different but common medical image segmentation
tasks. In addition, we suggest strategies to mitigate the influence of shortcut
learning and improve the generalizability of the segmentation models. By
uncovering the presence and implications of shortcuts in medical image
segmentation, we provide insights and methodologies for evaluating and
overcoming this pervasive challenge and call for attention in the community for
shortcuts in segmentation. Our code is public at
https://github.com/nina-weng/shortcut_skinseg .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, accepted at MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S4: Self-Supervised Sensing Across the Spectrum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jayanth Shenoy, Xingjian Davis Zhang, Shlok Mehrotra, Bill Tao, Rem Yang, Han Zhao, Deepak Vasisht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Satellite image time series (SITS) segmentation is crucial for many
applications like environmental monitoring, land cover mapping and agricultural
crop type classification. However, training models for SITS segmentation
remains a challenging task due to the lack of abundant training data, which
requires fine grained annotation. We propose S4 a new self-supervised
pre-training approach that significantly reduces the requirement for labeled
training data by utilizing two new insights: (a) Satellites capture images in
different parts of the spectrum such as radio frequencies, and visible
frequencies. (b) Satellite imagery is geo-registered allowing for fine-grained
spatial alignment. We use these insights to formulate pre-training tasks in S4.
We also curate m2s2-SITS, a large-scale dataset of unlabeled,
spatially-aligned, multi-modal and geographic specific SITS that serves as
representative pre-training data for S4. Finally, we evaluate S4 on multiple
SITS segmentation datasets and demonstrate its efficacy against competing
baselines while using limited labeled data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic infant 2D pose estimation from videos: comparing seven deep
  neural network methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filipe Gama, Matej Misar, Lukas Navara, Sergiu T. Popescu, Matej Hoffmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic markerless estimation of infant posture and motion from ordinary
videos carries great potential for movement studies "in the wild", facilitating
understanding of motor development and massively increasing the chances of
early diagnosis of disorders. There is rapid development of human pose
estimation methods in computer vision thanks to advances in deep learning and
machine learning. However, these methods are trained on datasets featuring
adults in different contexts. This work tests and compares seven popular
methods (AlphaPose, DeepLabCut/DeeperCut, Detectron2, HRNet,
MediaPipe/BlazePose, OpenPose, and ViTPose) on videos of infants in supine
position. Surprisingly, all methods except DeepLabCut and MediaPipe have
competitive performance without additional finetuning, with ViTPose performing
best. Next to standard performance metrics (object keypoint similarity, average
precision and recall), we introduce errors expressed in the neck-mid-hip ratio
and additionally study missed and redundant detections and the reliability of
the internal confidence ratings of the different methods, which are relevant
for downstream tasks. Among the networks with competitive performance, only
AlphaPose could run close to real time (27 fps) on our machine. We provide
documented Docker containers or instructions for all the methods we used, our
analysis scripts, and processed data at https://hub.docker.com/u/humanoidsctu
and https://osf.io/x465b/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SRC-Net: Bi-Temporal Spatial Relationship Concerned Network for Change
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjia Chen, Xin Xu, Fangling Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Change detection (CD) in remote sensing imagery is a crucial task with
applications in environmental monitoring, urban development, and disaster
management. CD involves utilizing bi-temporal images to identify changes over
time. The bi-temporal spatial relationships between features at the same
location at different times play a key role in this process. However, existing
change detection networks often do not fully leverage these spatial
relationships during bi-temporal feature extraction and fusion. In this work,
we propose SRC-Net: a bi-temporal spatial relationship concerned network for
CD. The proposed SRC-Net includes a Perception and Interaction Module that
incorporates spatial relationships and establishes a cross-branch perception
mechanism to enhance the precision and robustness of feature extraction.
Additionally, a Patch-Mode joint Feature Fusion Module is introduced to address
information loss in current methods. It considers different change modes and
concerns about spatial relationships, resulting in more expressive fusion
features. Furthermore, we construct a novel network using these two
relationship concerned modules and conducted experiments on the LEVIR-CD and
WHU Building datasets. The experimental results demonstrate that our network
outperforms state-of-the-art (SOTA) methods while maintaining a modest
parameter count. We believe our approach sets a new paradigm for change
detection and will inspire further advancements in the field. The code and
models are publicly available at https://github.com/Chnja/SRCNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures, IEEE Journal of Selected Topics in Applied
  Earth Observations and Remote Sensing (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLSM-Adapter: Finetuning Vision-Language Segmentation <span class="highlight-title">Efficient</span>ly with
  <span class="highlight-title">Lightweight</span> Blocks <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06196v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06196v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manish Dhakal, Rabin Adhikari, Safal Thapaliya, Bishesh Khanal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation Vision-Language Models (VLMs) trained using large-scale
open-domain images and text pairs have recently been adapted to develop
Vision-Language Segmentation Models (VLSMs) that allow providing text prompts
during inference to guide image segmentation. If robust and powerful VLSMs can
be built for medical images, it could aid medical professionals in many
clinical tasks where they must spend substantial time delineating the target
structure of interest. VLSMs for medical images resort to fine-tuning base VLM
or VLSM pretrained on open-domain natural image datasets due to fewer annotated
medical image datasets; this fine-tuning is resource-consuming and expensive as
it usually requires updating all or a significant fraction of the pretrained
parameters. Recently, lightweight blocks called adapters have been proposed in
VLMs that keep the pretrained model frozen and only train adapters during
fine-tuning, substantially reducing the computing resources required. We
introduce a novel adapter, VLSM-Adapter, that can fine-tune pretrained
vision-language segmentation models using transformer encoders. Our experiments
in widely used CLIP-based segmentation models show that with only 3 million
trainable parameters, the VLSM-Adapter outperforms state-of-the-art and is
comparable to the upper bound end-to-end fine-tuning. The source code is
available at: https://github.com/naamiinepal/vlsm-adapter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2024, the 27th International Conference on Medical
  Image Computing and Computer Assisted Intervention</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMGPL: Multimodal Medical Data Analysis with Graph Prompt Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Peng, Songyue Cai, Zongqian Wu, Huifang Shang, Xiaofeng Zhu, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt learning has demonstrated impressive efficacy in the fine-tuning of
multimodal large models to a wide range of downstream tasks. Nonetheless,
applying existing prompt learning methods for the diagnosis of neurological
disorder still suffers from two issues: (i) existing methods typically treat
all patches equally, despite the fact that only a small number of patches in
neuroimaging are relevant to the disease, and (ii) they ignore the structural
information inherent in the brain connection network which is crucial for
understanding and diagnosing neurological disorders. To tackle these issues, we
introduce a novel prompt learning model by learning graph prompts during the
fine-tuning process of multimodal large models for diagnosing neurological
disorders. Specifically, we first leverage GPT-4 to obtain relevant disease
concepts and compute semantic similarity between these concepts and all
patches. Secondly, we reduce the weight of irrelevant patches according to the
semantic similarity between each patch and disease-related concepts. Moreover,
we construct a graph among tokens based on these concepts and employ a graph
convolutional network layer to extract the structural information of the graph,
which is used to prompt the pre-trained multimodal large models for diagnosing
neurological disorders. Extensive experiments demonstrate that our method
achieves superior performance for neurological disorder diagnosis compared with
state-of-the-art methods and validated by clinicians.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Supervised Detection of Perfect and Partial Input-Dependent
  Symmetries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12223v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12223v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alonso Urbano, David W. Romero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group equivariance can overly constrain models if the symmetries in the group
differ from those observed in data. While common methods address this by
determining the appropriate level of symmetry at the dataset level, they are
limited to supervised settings and ignore scenarios in which multiple levels of
symmetry co-exist in the same dataset. In this paper, we propose a method able
to detect the level of symmetry of each input without the need for labels. Our
framework is general enough to accommodate different families of both
continuous and discrete symmetry distributions, such as arbitrary unimodal,
symmetric distributions and discrete groups. We validate the effectiveness of
our approach on synthetic datasets with different per-class levels of
symmetries, and demonstrate practical applications such as the detection of
out-of-distribution symmetries. Our code is publicly available at
https://github.com/aurban0/ssl-sym.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 8 figures, corrected typos, revised argument in Appendix
  B.1, results unchanged</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">47</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PathAlign: A vision-language model for whole slide images in
  histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faruk Ahmed, Andrew Sellergren, Lin Yang, Shawn Xu, Boris Babenko, Abbi Ward, Niels Olson, Arash Mohtashamian, Yossi Matias, Greg S. Corrado, Quang Duong, Dale R. Webster, Shravya Shetty, Daniel Golden, Yun Liu, David F. Steiner, Ellery Wulczyn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microscopic interpretation of histopathology images underlies many important
diagnostic and treatment decisions. While advances in vision-language modeling
raise new opportunities for analysis of such images, the gigapixel-scale size
of whole slide images (WSIs) introduces unique challenges. Additionally,
pathology reports simultaneously highlight key findings from small regions
while also aggregating interpretation across multiple slides, often making it
difficult to create robust image-text pairs. As such, pathology reports remain
a largely untapped source of supervision in computational pathology, with most
efforts relying on region-of-interest annotations or self-supervision at the
patch-level. In this work, we develop a vision-language model based on the
BLIP-2 framework using WSIs paired with curated text from pathology reports.
This enables applications utilizing a shared image-text embedding space, such
as text or image retrieval for finding cases of interest, as well as
integration of the WSI encoder with a frozen large language model (LLM) for
WSI-based generative text capabilities such as report generation or
AI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000
WSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure
types, and tissue types. We present pathologist evaluation of text generation
and text retrieval using WSI embeddings, as well as results for WSI
classification and workflow prioritization (slide-level triaging).
Model-generated text for WSIs was rated by pathologists as accurate, without
clinically significant error or omission, for 78% of WSIs on average. This work
demonstrates exciting potential capabilities for language-aligned WSI
embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 main pages and 19 pages of supplemental material; 3 main tables, 3
  main figures and 11 supplemental tables, 7 supplemental figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic Cancer -- Augmenting Worms with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Zimmerman, David Zollikofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With increasingly sophisticated large language models (LLMs), the potential
for abuse rises drastically. As a submission to the Swiss AI Safety Prize, we
present a novel type of metamorphic malware leveraging LLMs for two key
processes. First, LLMs are used for automatic code rewriting to evade
signature-based detection by antimalware programs. The malware then spreads its
copies via email by utilizing an LLM to socially engineer email replies to
encourage recipients to execute the attached malware. Our submission includes a
functional minimal prototype, highlighting the risks that LLMs pose for
cybersecurity and underscoring the need for further research into intelligent
malware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Won first place at the Swiss AI Safety Prize. Some technical details
  omitted, contact authors for more information</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Matters in Detecting AI-Generated Videos like Sora? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chirui Chang, Zhengzhe Liu, Xiaoyang Lyu, Xiaojuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion-based video generation have showcased
remarkable results, yet the gap between synthetic and real-world videos remains
under-explored. In this study, we examine this gap from three fundamental
perspectives: appearance, motion, and geometry, comparing real-world videos
with those generated by a state-of-the-art AI model, Stable Video Diffusion. To
achieve this, we train three classifiers using 3D convolutional networks, each
targeting distinct aspects: vision foundation model features for appearance,
optical flow for motion, and monocular depth for geometry. Each classifier
exhibits strong performance in fake video detection, both qualitatively and
quantitatively. This indicates that AI-generated videos are still easily
detectable, and a significant gap between real and fake videos persists.
Furthermore, utilizing the Grad-CAM, we pinpoint systematic failures of
AI-generated videos in appearance, motion, and geometry. Finally, we propose an
Ensemble-of-Experts model that integrates appearance, optical flow, and depth
information for fake video detection, resulting in enhanced robustness and
generalization ability. Our model is capable of detecting videos generated by
Sora with high accuracy, even without exposure to any Sora videos during
training. This suggests that the gap between real and fake videos can be
generalized across various video generative models. Project page:
https://justin-crchang.github.io/3DCNNDetection.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Gradient Search Control: A Method for Improving the Efficiency of
  Dyna-style Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bradley Burega, John D. Martin, Luke Kapeluck, Michael Bowling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how a Reinforcement Learning (RL) system can remain sample-efficient
when learning from an imperfect model of the environment. This is particularly
challenging when the learning system is resource-constrained and in continual
settings, where the environment dynamics change. To address these challenges,
our paper introduces an online, meta-gradient algorithm that tunes a
probability with which states are queried during Dyna-style planning. Our study
compares the aggregate, empirical performance of this meta-gradient method to
baselines that employ conventional sampling strategies. Results indicate that
our method improves efficiency of the planning process, which, as a
consequence, improves the sample-efficiency of the overall learning process. On
the whole, we observe that our meta-learned solutions avoid several pathologies
of conventional planning approaches, such as sampling inaccurate transitions
and those that stall credit assignment. We believe these findings could prove
useful, in future work, for designing model-based RL systems at scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking harmless refusals when fine-tuning foundation models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florin Pop, Judd Rosenblatt, Diogo Schwerz de Lucena, Michael Vaiana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the degree to which fine-tuning in Large
Language Models (LLMs) effectively mitigates versus merely conceals undesirable
behavior. Through the lens of semi-realistic role-playing exercises designed to
elicit such behaviors, we explore the response dynamics of LLMs post
fine-tuning interventions. Our methodology involves prompting models for
Chain-of-Thought (CoT) reasoning and analyzing the coherence between the
reasoning traces and the resultant outputs. Notably, we identify a pervasive
phenomenon we term \emph{reason-based deception}, where models either stop
producing reasoning traces or produce seemingly ethical reasoning traces that
belie the unethical nature of their final outputs. We further examine the
efficacy of response strategies (polite refusal versus explicit rebuttal) in
curbing the occurrence of undesired behavior in subsequent outputs of
multi-turn interactions. Our findings reveal that explicit rebuttals
significantly outperform polite refusals in preventing the continuation of
undesired outputs and nearly eliminate reason-based deception, challenging
current practices in model fine-tuning. Accordingly, the two key contributions
of this paper are (1) defining and studying reason-based deception, a new type
of hidden behavior, and (2) demonstrating that rebuttals provide a more robust
response model to harmful requests than refusals, thereby highlighting the need
to reconsider the response strategies in fine-tuning approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 AGI Workshop Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Machine-Generated Rationales to Facilitate Social Meaning
  Detection in Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritam Dutt, Zhen Wu, Kelly Shi, Divyanshu Sheth, Prakhar Gupta, Carolyn Penstein Rose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a generalizable classification approach that leverages Large
Language Models (LLMs) to facilitate the detection of implicitly encoded social
meaning in conversations. We design a multi-faceted prompt to extract a textual
explanation of the reasoning that connects visible cues to underlying social
meanings. These extracted explanations or rationales serve as augmentations to
the conversational text to facilitate dialogue understanding and transfer. Our
empirical results over 2,340 experimental settings demonstrate the significant
positive impact of adding these rationales. Our findings hold true for
in-domain classification, zero-shot, and few-shot domain transfer for two
different social meaning detection tasks, each spanning two different corpora.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at The Proceedings of the Association for Computational
  Linguistics, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Handling Ontology Gaps in Semantic Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Bacciu, Marco Damonte, Marco Basaldella, Emilio Monti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of Neural Semantic Parsing (NSP) models are developed with the
assumption that there are no concepts outside the ones such models can
represent with their target symbols (closed-world assumption). This assumption
leads to generate hallucinated outputs rather than admitting their lack of
knowledge. Hallucinations can lead to wrong or potentially offensive responses
to users. Hence, a mechanism to prevent this behavior is crucial to build
trusted NSP-based Question Answering agents. To that end, we propose the
Hallucination Simulation Framework (HSF), a general setting for stimulating and
analyzing NSP model hallucinations. The framework can be applied to any NSP
task with a closed-ontology. Using the proposed framework and KQA Pro as the
benchmark dataset, we assess state-of-the-art techniques for hallucination
detection. We then present a novel hallucination detection strategy that
exploits the computational graph of the NSP model to detect the NSP
hallucinations in the presence of ontology gaps, out-of-domain utterances, and
to recognize NSP errors, improving the F1-Score respectively by ~21, ~24% and
~1%. This is the first work in closed-ontology NSP that addresses the problem
of recognizing ontology gaps. We release our code and checkpoints at
https://github.com/amazon-science/handling-ontology-gaps-in-semantic-parsing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Large Language Models to Assist Video Content Analysis: An
  Exploratory Study of Short Videos on Depression <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaying Liu, Yunlong Wang, Yao Lyu, Yiheng Su, Shuo Niu, Xuhai "Orson" Xu, Yan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the growing interest in leveraging Large Language Models (LLMs) for
content analysis, current studies have primarily focused on text-based content.
In the present work, we explored the potential of LLMs in assisting video
content analysis by conducting a case study that followed a new workflow of
LLM-assisted multimodal content analysis. The workflow encompasses codebook
design, prompt engineering, LLM processing, and human evaluation. We
strategically crafted annotation prompts to get LLM Annotations in structured
form and explanation prompts to generate LLM Explanations for a better
understanding of LLM reasoning and transparency. To test LLM's video annotation
capabilities, we analyzed 203 keyframes extracted from 25 YouTube short videos
about depression. We compared the LLM Annotations with those of two human
coders and found that LLM has higher accuracy in object and activity
Annotations than emotion and genre Annotations. Moreover, we identified the
potential and limitations of LLM's capabilities in annotating videos. Based on
the findings, we explore opportunities and challenges for future research and
improvements to the workflow. We also discuss ethical concerns surrounding
future studies based on LLM-assisted video analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, under review in CSCW 24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Captioning Visualizations with Large Language Models (CVLLM): A Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Carenini, Jordon Johnson, Ali Salamatian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatically captioning visualizations is not new, but recent advances in
large language models(LLMs) open exciting new possibilities. In this tutorial,
after providing a brief review of Information Visualization (InfoVis)
principles and past work in captioning, we introduce neural models and the
transformer architecture used in generic LLMs. We then discuss their recent
applications in InfoVis, with a focus on captioning. Additionally, we explore
promising future directions in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Too Good to be True? Turn Any Model Differentially Private With
  DP-Weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Zagardo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imagine training a machine learning model with Differentially Private
Stochastic Gradient Descent (DP-SGD), only to discover post-training that the
noise level was either too high, crippling your model's utility, or too low,
compromising privacy. The dreaded realization hits: you must start the lengthy
training process from scratch. But what if you could avoid this retraining
nightmare? In this study, we introduce a groundbreaking approach (to our
knowledge) that applies differential privacy noise to the model's weights after
training. We offer a comprehensive mathematical proof for this novel approach's
privacy bounds, use formal methods to validate its privacy guarantees, and
empirically evaluate its effectiveness using membership inference attacks and
performance evaluations. This method allows for a single training run, followed
by post-hoc noise adjustments to achieve optimal privacy-utility trade-offs. We
compare this novel fine-tuned model (DP-Weights model) to a traditional DP-SGD
model, demonstrating that our approach yields statistically similar performance
and privacy guarantees. Our results validate the efficacy of post-training
noise application, promising significant time savings and flexibility in
fine-tuning differential privacy parameters, making it a practical alternative
for deploying differentially private models in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For code visit the following repository,
  https://github.com/dzagardo/forgetnet/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating How Large Language Models Leverage Internal Knowledge to
  Perform Complex Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miyoung Ko, Sue Hyun Park, Joonsuk Park, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements, there is a limited understanding of how
large language models (LLMs) utilize knowledge for reasoning. To address this,
we propose a method that deconstructs complex real-world questions into a
graph, representing each question as a node with parent nodes of background
knowledge needed to solve the question. We develop the DepthQA dataset,
deconstructing questions into three depths: (i) recalling conceptual knowledge,
(ii) applying procedural knowledge, and (iii) analyzing strategic knowledge.
Based on a hierarchical graph, we quantify forward discrepancy, discrepancies
in LLMs' performance on simpler sub-problems versus complex questions. We also
measure backward discrepancy, where LLMs answer complex questions but struggle
with simpler ones. Our analysis shows that smaller models have more
discrepancies than larger models. Additionally, guiding models from simpler to
complex questions through multi-turn interactions improves performance across
model sizes, highlighting the importance of structured intermediate steps in
knowledge reasoning. This work enhances our understanding of LLM reasoning and
suggests ways to improve their problem-solving abilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress; code is available at
  https://github.com/kaistAI/knowledge-reasoning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge acquisition for dialogue agents using reinforcement learning
  on graph representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selene Baez Santamaria, Shihan Wang, Piek Vossen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop an artificial agent motivated to augment its knowledge base beyond
its initial training. The agent actively participates in dialogues with other
agents, strategically acquiring new information. The agent models its knowledge
as an RDF knowledge graph, integrating new beliefs acquired through
conversation. Responses in dialogue are generated by identifying graph patterns
around these new integrated beliefs. We show that policies can be learned using
reinforcement learning to select effective graph patterns during an
interaction, without relying on explicit user feedback. Within this context,
our study is a proof of concept for leveraging users as effective sources of
information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inclusivity in Large Language Models: Personality Traits and Gender Bias
  in Scientific Abstracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naseela Pervez, Alexander J. Titus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly utilized to assist in
scientific and academic writing, helping authors enhance the coherence of their
articles. Previous studies have highlighted stereotypes and biases present in
LLM outputs, emphasizing the need to evaluate these models for their alignment
with human narrative styles and potential gender biases. In this study, we
assess the alignment of three prominent LLMs - Claude 3 Opus, Mistral AI Large,
and Gemini 1.5 Flash - by analyzing their performance on benchmark
text-generation tasks for scientific abstracts. We employ the Linguistic
Inquiry and Word Count (LIWC) framework to extract lexical, psychological, and
social features from the generated texts. Our findings indicate that, while
these models generally produce text closely resembling human authored content,
variations in stylistic features suggest significant gender biases. This
research highlights the importance of developing LLMs that maintain a diversity
of writing styles to promote inclusivity in academic discourse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Development and Evaluation of a Retrieval-Augmented Generation Tool for
  Creating SAPPhIRE Models of Artificial Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anubhab Majumder, Kausik Bhattacharya, Amaresh Chakrabarti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representing systems using the SAPPhIRE causality model is found useful in
supporting design-by-analogy. However, creating a SAPPhIRE model of artificial
or biological systems is an effort-intensive process that requires human
experts to source technical knowledge from multiple technical documents
regarding how the system works. This research investigates how to leverage
Large Language Models (LLMs) in creating structured descriptions of systems
using the SAPPhIRE model of causality. This paper, the second part of the
two-part research, presents a new Retrieval-Augmented Generation (RAG) tool for
generating information related to SAPPhIRE constructs of artificial systems and
reports the results from a preliminary evaluation of the tool's success -
focusing on the factual accuracy and reliability of outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoPT: Low-Rank Prompt Tuning for Parameter <span class="highlight-title">Efficient</span> Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shouchang Guo, Sonam Damani, Keng-hao Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In prompt tuning, a prefix or suffix text is added to the prompt, and the
embeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix
are optimized to gain more control over language models for specific tasks.
This approach eliminates the need for hand-crafted prompt engineering or
explicit model fine-tuning. Prompt tuning is significantly more
parameter-efficient than model fine-tuning, as it involves optimizing partial
inputs of language models to produce desired outputs.
  In this work, we aim to further reduce the amount of trainable parameters
required for a language model to perform well on specific tasks. We propose
Low-rank Prompt Tuning (LoPT), a low-rank model for prompts that achieves
efficient prompt optimization. The proposed method demonstrates similar
outcomes to full parameter prompt tuning while reducing the number of trainable
parameters by a factor of 5. It also provides promising results compared to the
state-of-the-art methods that would require 10 to 20 times more parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Regression for Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ergun Biçici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We use transductive regression techniques to learn mappings between source
and target features of given parallel corpora and use these mappings to
generate machine translation outputs. We show the effectiveness of $L_1$
regularized regression (\textit{lasso}) to learn the mappings between sparsely
observed feature sets versus $L_2$ regularized regression. Proper selection of
training instances plays an important role to learn correct feature mappings
within limited computational resources and at expected accuracy levels. We
introduce \textit{dice} instance selection method for proper selection of
training instances, which plays an important role to learn correct feature
mappings for improving the source and target coverage of the training set. We
show that $L_1$ regularized regression performs better than $L_2$ regularized
regression both in regression measurements and in the translation experiments
using graph decoding. We present encouraging results when translating from
German to English and Spanish to English. We also demonstrate results when the
phrase table of a phrase-based decoder is replaced with the mappings we find
with the regression model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyi Liu, Cheng Chi, Eric Cousineau, Naveen Kuppuswamy, Benjamin Burchfiel, Shuran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio signals provide rich information for the robot interaction and object
properties through contact. These information can surprisingly ease the
learning of contact-rich robot manipulation skills, especially when the visual
information alone is ambiguous or incomplete. However, the usage of audio data
in robot manipulation has been constrained to teleoperated demonstrations
collected by either attaching a microphone to the robot or object, which
significantly limits its usage in robot learning pipelines. In this work, we
introduce ManiWAV: an 'ear-in-hand' data collection device to collect
in-the-wild human demonstrations with synchronous audio and visual feedback,
and a corresponding policy interface to learn robot manipulation policy
directly from the demonstrations. We demonstrate the capabilities of our system
through four contact-rich manipulation tasks that require either passively
sensing the contact events and modes, or actively sensing the object surface
materials and states. In addition, we show that our system can generalize to
unseen in-the-wild environments, by learning from diverse in-the-wild human
demonstrations. Project website: https://mani-wav.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Lightweight</span> Predictive 3D Gaussian Splats 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junli Cao, Vidit Goel, Chaoyang Wang, Anil Kag, Ju Hu, Sergei Korolev, Chenfanfu Jiang, Sergey Tulyakov, Jian Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent approaches representing 3D objects and scenes using Gaussian splats
show increased rendering speed across a variety of platforms and devices. While
rendering such representations is indeed extremely efficient, storing and
transmitting them is often prohibitively expensive. To represent large-scale
scenes, one often needs to store millions of 3D Gaussians, occupying gigabytes
of disk space. This poses a very practical limitation, prohibiting widespread
adoption.Several solutions have been proposed to strike a balance between disk
size and rendering quality, noticeably reducing the visual quality. In this
work, we propose a new representation that dramatically reduces the hard drive
footprint while featuring similar or improved quality when compared to the
standard 3D Gaussian splats. When compared to other compact solutions, ours
offers higher quality renderings with significantly reduced storage, being able
to efficiently run on a mobile device in real-time. Our key observation is that
nearby points in the scene can share similar representations. Hence, only a
small ratio of 3D points needs to be stored. We introduce an approach to
identify such points which are called parent points. The discarded points
called children points along with attributes can be efficiently predicted by
tiny MLPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://plumpuddings.github.io/LPGS//</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Remarkable Robustness of LLMs: Stages of Inference? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vedang Lad, Wes Gurnee, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate and investigate the remarkable robustness of Large Language
Models by deleting and swapping adjacent layers. We find that deleting and
swapping interventions retain 72-95\% of the original model's prediction
accuracy without fine-tuning, whereas models with more layers exhibit more
robustness. Based on the results of the layer-wise intervention and further
experiments, we hypothesize the existence of four universal stages of inference
across eight different models: detokenization, feature engineering, prediction
ensembling, and residual sharpening. The first stage integrates local
information, lifting raw token representations into higher-level contextual
representations. Next is the iterative refinement of task and entity-specific
features. Then, the second half of the model begins with a phase transition,
where hidden representations align more with the vocabulary space due to
specialized model components. Finally, the last layer sharpens the following
token distribution by eliminating obsolete features that add noise to the
prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept
  Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Core Francisco Park, Maya Okawa, Andrew Lee, Ekdeep Singh Lubana, Hidenori Tanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern generative models demonstrate impressive capabilities, likely stemming
from an ability to identify and manipulate abstract concepts underlying their
training data. However, fundamental questions remain: what determines the
concepts a model learns, the order in which it learns them, and its ability to
manipulate those concepts? To address these questions, we propose analyzing a
model's learning dynamics via a framework we call the concept space, where each
axis represents an independent concept underlying the data generating process.
By characterizing learning dynamics in this space, we identify how the speed at
which a concept is learned, and hence the order of concept learning, is
controlled by properties of the data we term concept signal. Further, we
observe moments of sudden turns in the direction of a model's learning dynamics
in concept space. Surprisingly, these points precisely correspond to the
emergence of hidden capabilities, i.e., where latent interventions show the
model possesses the capability to manipulate a concept, but these capabilities
cannot yet be elicited via naive input prompting. While our results focus on
synthetically defined toy datasets, we hypothesize a general claim on emergence
of hidden capabilities may hold: generative models possess latent capabilities
that emerge suddenly and consistently during training, though a model might not
exhibit these capabilities under naive input prompting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fundamental Problems With Model Editing: How Should Rational Belief
  Revision Work in LLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Hase, Thomas Hofweber, Xiang Zhou, Elias Stengel-Eskin, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The model editing problem concerns how language models should learn new facts
about the world over time. While empirical research on model editing has drawn
widespread attention, the conceptual foundations of model editing remain shaky
-- perhaps unsurprisingly, since model editing is essentially belief revision,
a storied problem in philosophy that has eluded succinct solutions for decades.
Model editing nonetheless demands a solution, since we need to be able to
control the knowledge within language models. With this goal in mind, this
paper critiques the standard formulation of the model editing problem and
proposes a formal testbed for model editing research. We first describe 12 open
problems with model editing, based on challenges with (1) defining the problem,
(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the
first place. Many of these challenges are extremely difficult to address, e.g.
determining far-reaching consequences of edits, labeling probabilistic
entailments between facts, and updating beliefs of agent simulators. Next, we
introduce a semi-synthetic dataset for model editing based on Wikidata, where
we can evaluate edits against labels given by an idealized Bayesian agent. This
enables us to say exactly how belief revision in language models falls short of
a desirable epistemic standard. We encourage further research exploring
settings where such a gold standard can be compared against. Our code is
publicly available at: https://github.com/peterbhase/LLM-belief-revision
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and
  Toxicity Types for Indonesian Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucky Susanto, Musa Izzanardi Wijanarko, Prasetia Anugrah Pratama, Traci Hong, Ika Idris, Alham Fikri Aji, Derry Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech poses a significant threat to social harmony. Over the past two
years, Indonesia has seen a ten-fold increase in the online hate speech ratio,
underscoring the urgent need for effective detection mechanisms. However,
progress is hindered by the limited availability of labeled data for Indonesian
texts. The condition is even worse for marginalized minorities, such as Shia,
LGBTQ, and other ethnic minorities because hate speech is underreported and
less understood by detection tools. Furthermore, the lack of accommodation for
subjectivity in current datasets compounds this issue. To address this, we
introduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity
classification dataset. Comprising 43,692 entries annotated by 19 diverse
individuals, the dataset focuses on texts targeting vulnerable groups in
Indonesia, specifically during the hottest political event in the country: the
presidential election. We establish baselines for seven binary classification
tasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)
fine-tuned for hate speech classification. Furthermore, we demonstrate how
incorporating demographic information can enhance the zero-shot performance of
the large language model, gpt-3.5-turbo. However, we also caution that an
overemphasis on demographic information can negatively impact the fine-tuned
model performance due to data fragmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Efficient</span> World Models with Context-Aware Tokenization <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Micheli, Eloi Alonso, François Fleuret
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up deep Reinforcement Learning (RL) methods presents a significant
challenge. Following developments in generative modelling, model-based RL
positions itself as a strong contender. Recent advances in sequence modelling
have led to effective transformer-based world models, albeit at the price of
heavy computations due to the long sequences of tokens required to accurately
simulate environments. In this work, we propose $\Delta$-IRIS, a new agent with
a world model architecture composed of a discrete autoencoder that encodes
stochastic deltas between time steps and an autoregressive transformer that
predicts future deltas by summarizing the current state of the world with
continuous tokens. In the Crafter benchmark, $\Delta$-IRIS sets a new state of
the art at multiple frame budgets, while being an order of magnitude faster to
train than previous attention-based approaches. We release our code and models
at https://github.com/vmicheli/delta-iris.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jump Starting Bandits with LLM-Generated Prior Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parand A. Alamdari, Yanshuai Cao, Kevin H. Wilson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present substantial evidence demonstrating the benefits of integrating
Large Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.
Contextual bandits have been widely used in recommendation systems to generate
personalized suggestions based on user-specific contexts. We show that LLMs,
pre-trained on extensive corpora rich in human knowledge and preferences, can
simulate human behaviours well enough to jump-start contextual multi-armed
bandits to reduce online learning regret. We propose an initialization
algorithm for contextual bandits by prompting LLMs to produce a pre-training
dataset of approximate human preferences for the bandit. This significantly
reduces online learning regret and data-gathering costs for training such
models. Our approach is validated empirically through two sets of experiments
with different bandit setups: one which utilizes LLMs to serve as an oracle and
a real-world experiment utilizing data from a conjoint survey experiment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> LiveBench: A Challenging, Contamination-Free LLM Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, <span class="highlight-author">Yann LeCun</span>, Tom Goldstein, Willie Neiswanger, Micah Goldblum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test set contamination, wherein test data from a benchmark ends up in a newer
model's training set, is a well-documented obstacle for fair LLM evaluation and
can quickly render benchmarks obsolete. To mitigate this, many recent
benchmarks crowdsource new prompts and evaluations from human or LLM judges;
however, these can introduce significant biases, and break down when scoring
hard questions. In this work, we introduce a new benchmark for LLMs designed to
be immune to both test set contamination and the pitfalls of LLM judging and
human crowdsourcing. We release LiveBench, the first benchmark that (1)
contains frequently-updated questions from recent information sources, (2)
scores answers automatically according to objective ground-truth values, and
(3) contains a wide variety of challenging tasks, spanning math, coding,
reasoning, language, instruction following, and data analysis. To achieve this,
LiveBench contains questions that are based on recently-released math
competitions, arXiv papers, news articles, and datasets, and it contains
harder, contamination-free versions of tasks from previous benchmarks such as
Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source
models, as well as dozens of open-source models ranging from 0.5B to 110B in
size. LiveBench is difficult, with top models achieving below 65% accuracy. We
release all questions, code, and model answers. Questions will be added and
updated on a monthly basis, and we will release new tasks and harder versions
of tasks over time so that LiveBench can distinguish between the capabilities
of LLMs as they improve in the future. We welcome community engagement and
collaboration for expanding the benchmark tasks and models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Artificial Needles to Real Haystacks: Improving Retrieval
  Capabilities in LLMs by Finetuning on Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, Dimitris Papailiopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that Large Language Models (LLMs) struggle to
accurately retrieve information and maintain reasoning capabilities when
processing long-context inputs. To address these limitations, we propose a
finetuning approach utilizing a carefully designed synthetic dataset comprising
numerical key-value retrieval tasks. Our experiments on models like GPT-3.5
Turbo and Mistral 7B demonstrate that finetuning LLMs on this dataset
significantly improves LLMs' information retrieval and reasoning capabilities
in longer-context settings. We present an analysis of the finetuned models,
illustrating the transfer of skills from synthetic to real task evaluations
(e.g., $10.5\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5
Turbo). We also find that finetuned LLMs' performance on general benchmarks
remains almost constant while LLMs finetuned on other baseline long-context
augmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B
finetuned on our synthetic data cause no performance drop while other baseline
data can cause a drop that ranges from $2.33\%$ to $6.19\%$). Our study
highlights the potential of finetuning on synthetic data for improving the
performance of LLMs on longer-context tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into
  Multimodal LLMs at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of multimodal large language models (MLLMs), such as
GPT-4V, has led to significant advancements. However, these models still face
challenges in medical multimodal capabilities due to limitations in the
quantity and quality of medical vision-text data, stemming from data privacy
concerns and high annotation costs. While pioneering approaches utilize
PubMed's large-scale, de-identified medical image-text pairs to address these
limitations, they still fall short due to inherent data noise. To tackle this,
we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in
an 'unblinded' capacity to denoise and reformat the data, resulting in the
creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our
validation demonstrates that: (1) PubMedVision can significantly enhance the
medical multimodal capabilities of current MLLMs, showing significant
improvement in benchmarks including the MMMU Health & Medicine track; (2)
manual checks by medical experts and empirical results validate the superior
data quality of our dataset compared to other data construction methods. Using
PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows
superior performance in medical multimodal scenarios among open-source MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Semantic Framework for Neural-Symbolic Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12050v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12050v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Odense, Artur d'Avila Garcez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two approaches to AI, neural networks and symbolic systems, have been proven
very successful for an array of AI problems. However, neither has been able to
achieve the general reasoning ability required for human-like intelligence. It
has been argued that this is due to inherent weaknesses in each approach.
Luckily, these weaknesses appear to be complementary, with symbolic systems
being adept at the kinds of things neural networks have trouble with and
vice-versa. The field of neural-symbolic AI attempts to exploit this asymmetry
by combining neural networks and symbolic AI into integrated systems. Often
this has been done by encoding symbolic knowledge into neural networks.
Unfortunately, although many different methods for this have been proposed,
there is no common definition of an encoding to compare them. We seek to
rectify this problem by introducing a semantic framework for neural-symbolic
AI, which is then shown to be general enough to account for a large family of
neural-symbolic systems. We provide a number of examples and proofs of the
application of the framework to the neural encoding of various forms of
knowledge representation and neural network. These, at first sight disparate
approaches, are all shown to fall within the framework's formal definition of
what we call semantic encoding for neural-symbolic AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symbolic Prompt Program Search: A Structure-Aware Approach to <span class="highlight-title">Efficient</span>
  Compile-Time Prompt Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Schnabel, Jennifer Neville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many modern LLM applications, such as retrieval augmented generation,
prompts have become programs themselves. In these settings, prompt programs are
repeatedly called with different user queries or data instances. A big
practical challenge is optimizing such prompt programs. Recent work has mostly
focused on either simple prompt programs or assumed that the general structure
of a prompt program is fixed.
  We introduce SAMMO, a framework to perform symbolic prompt program search for
compile-time optimizations of prompt programs. SAMMO represents prompt programs
on a symbolic level which allows for a rich set of transformations that can be
searched over during optimization. We show that SAMMO generalizes previous
methods and improves the performance of complex prompts on (1) instruction
tuning, (2) RAG pipeline tuning, and (3) prompt compression, across several
different LLMs. We make all code available open-source at
https://github.com/microsoft/sammo .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LlamaFactory: Unified <span class="highlight-title">Efficient</span> Fine-Tuning of 100+ Language Models <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13372v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13372v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, Yongqiang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient fine-tuning is vital for adapting large language models (LLMs) to
downstream tasks. However, it requires non-trivial efforts to implement these
methods on different models. We present LlamaFactory, a unified framework that
integrates a suite of cutting-edge efficient training methods. It provides a
solution for flexibly customizing the fine-tuning of 100+ LLMs without the need
for coding through the built-in web UI LlamaBoard. We empirically validate the
efficiency and effectiveness of our framework on language modeling and text
generation tasks. It has been released at
https://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and
3,000 forks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, accepted to ACL 2024 System Demonstration Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Modality Program Representation Learning for Electronic Design
  Automation with High-Level Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyue Qin, Yunsheng Bai, Atefeh Sohrabizadeh, Zijian Ding, Ziniu Hu, Yizhou Sun, Jason Cong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, domain-specific accelerators (DSAs) have gained popularity
for applications such as deep learning and autonomous driving. To facilitate
DSA designs, programmers use high-level synthesis (HLS) to compile a high-level
description written in C/C++ into a design with low-level hardware description
languages that eventually synthesize DSAs on circuits. However, creating a
high-quality HLS design still demands significant domain knowledge,
particularly in microarchitecture decisions expressed as \textit{pragmas}.
Thus, it is desirable to automate such decisions with the help of machine
learning for predicting the quality of HLS designs, requiring a deeper
understanding of the program that consists of original code and pragmas.
Naturally, these programs can be considered as sequence data. In addition,
these programs can be compiled and converted into a control data flow graph
(CDFG). But existing works either fail to leverage both modalities or combine
the two in shallow or coarse ways. We propose ProgSG, a model that allows
interaction between the source code sequence modality and the graph modality in
a deep and fine-grained way. To alleviate the scarcity of labeled designs, a
pre-training method is proposed based on a suite of compiler's data flow
analysis tasks. Experimental results show that ProgSG reduces the RMSE of
design performance predictions by up to $22\%$, and identifies designs with an
average of $1.10\times$ and $1.26\times$ (up to $8.17\times$ and $13.31\times$)
performance improvement in design space exploration (DSE) task compared to HARP
and AutoDSE, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:2305.10838</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Condition Monitoring with Incomplete Data: An Integrated Variational
  Autoencoder and Distance Metric Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Ahang, Mostafa Abbasi, Todd Charter, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Condition monitoring of industrial systems is crucial for ensuring safety and
maintenance planning, yet notable challenges arise in real-world settings due
to the limited or non-existent availability of fault samples. This paper
introduces an innovative solution to this problem by proposing a new method for
fault detection and condition monitoring for unseen data. Adopting an approach
inspired by zero-shot learning, our method can identify faults and assign a
relative health index to various operational conditions. Typically, we have
plenty of data on normal operations, some data on compromised conditions, and
very few (if any) samples of severe faults. We use a variational autoencoder to
capture the probabilistic distribution of previously seen and new unseen
conditions. The health status is determined by comparing each sample's
deviation from a normal operation reference distribution in the latent space.
Faults are detected by establishing a threshold for the health indexes,
allowing the model to identify severe, unseen faults with high accuracy, even
amidst noise. We validate our approach using the run-to-failure IMS-bearing
dataset and compare it with other methods. The health indexes generated by our
model closely match the established descriptive model of bearing wear,
attesting to the robustness and reliability of our method. These findings
highlight the potential of our methodology in augmenting fault detection
capabilities within industrial domains, thereby contributing to heightened
safety protocols and optimized maintenance practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 2024 IEEE 20th International Conference on Automation
  Science and Engineering (CASE 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Equivariant Symmetry Breaking Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YuQing Xie, Tess Smidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equivariant neural networks (ENNs) have been shown to be extremely effective
in applications involving underlying symmetries. By construction ENNs cannot
produce lower symmetry outputs given a higher symmetry input. However, symmetry
breaking occurs in many physical systems and we may obtain a less symmetric
stable state from an initial highly symmetric one. Hence, it is imperative that
we understand how to systematically break symmetry in ENNs. In this work, we
propose a novel symmetry breaking framework that is fully equivariant and is
the first which fully addresses spontaneous symmetry breaking. We emphasize
that our approach is general and applicable to equivariance under any group. To
achieve this, we introduce the idea of symmetry breaking sets (SBS). Rather
than redesign existing networks, we design sets of symmetry breaking objects
which we feed into our network based on the symmetry of our inputs and outputs.
We show there is a natural way to define equivariance on these sets, which
gives an additional constraint. Minimizing the size of these sets equates to
data efficiency. We prove that minimizing these sets translates to a well
studied group theory problem, and tabulate solutions to this problem for the
point groups. Finally, we provide some examples of symmetry breaking to
demonstrate how our approach works in practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A New Era in Software Security: Towards Self-Healing Software via Large
  Language Models and Formal Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Norbert Tihanyi, Ridhi Jain, Yiannis Charalambous, Mohamed Amine Ferrag, Youcheng Sun, Lucas C. Cordeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an innovative approach that combines Large Language
Models (LLMs) with Formal Verification strategies for automatic software
vulnerability repair. Initially, we employ Bounded Model Checking (BMC) to
identify vulnerabilities and extract counterexamples. These counterexamples are
supported by mathematical proofs and the stack trace of the vulnerabilities.
Using a specially designed prompt, we combine the original source code with the
identified vulnerability, including its stack trace and counterexample that
specifies the line number and error type. This combined information is then fed
into an LLM, which is instructed to attempt to fix the code. The new code is
subsequently verified again using BMC to ensure the fix succeeded. We present
the ESBMC-AI framework as a proof of concept, leveraging the well-recognized
and industry-adopted Efficient SMT-based Context-Bounded Model Checker (ESBMC)
and a pre-trained transformer model to detect and fix errors in C programs,
particularly in critical software components. We evaluated our approach on
50,000 C programs randomly selected from the FormAI dataset with their
respective vulnerability classifications. Our results demonstrate ESBMC-AI's
capability to automate the detection and repair of issues such as buffer
overflow, arithmetic overflow, and pointer dereference failures with high
accuracy. ESBMC-AI is a pioneering initiative, integrating LLMs with BMC
techniques, offering potential integration into the continuous integration and
deployment (CI/CD) process within the software development lifecycle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-GCN: A Dynamically Weighted Loss Minimization Method for Dealing
  with the Data Imbalance in Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17073v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17073v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Mohammadizadeh, Arash Mozhdehi, Yani Ioannou, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although many real-world applications, such as disease prediction, and fault
detection suffer from class imbalance, most existing graph-based classification
methods ignore the skewness of the distribution of classes; therefore, tend to
be biased towards the majority class(es). Conventional methods typically tackle
this problem through the assignment of weights to each one of the class samples
based on a function of their loss, which can lead to over-fitting on outliers.
In this paper, we propose a meta-learning algorithm, named Meta-GCN, for
adaptively learning the example weights by simultaneously minimizing the
unbiased meta-data set loss and optimizing the model weights through the use of
a small unbiased meta-data set. Through experiments, we have shown that
Meta-GCN outperforms state-of-the-art frameworks and other baselines in terms
of accuracy, the area under the receiver operating characteristic (AUC-ROC)
curve, and macro F1-Score for classification tasks on two different datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guylingo: The Republic of Guyana Creole Corpora <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Clarke, Roland Daynauth, Charlene Wilkinson, Hubert Devonish, Jason Mars
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While major languages often enjoy substantial attention and resources, the
linguistic diversity across the globe encompasses a multitude of smaller,
indigenous, and regional languages that lack the same level of computational
support. One such region is the Caribbean. While commonly labeled as "English
speaking", the ex-British Caribbean region consists of a myriad of Creole
languages thriving alongside English. In this paper, we present Guylingo: a
comprehensive corpus designed for advancing NLP research in the domain of
Creolese (Guyanese English-lexicon Creole), the most widely spoken language in
the culturally rich nation of Guyana. We first outline our framework for
gathering and digitizing this diverse corpus, inclusive of colloquial
expressions, idioms, and regional variations in a low-resource language. We
then demonstrate the challenges of training and evaluating NLP models for
machine translation in Creole. Lastly, we discuss the unique opportunities
presented by recent NLP advancements for accelerating the formal adoption of
Creole languages as official languages in the Caribbean.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 Main Conference Special Theme Track: Languages
  of Latin America and The Caribbean</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bioptic -- A Target-Agnostic Efficacy-Based Small Molecules Search
  Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vlad Vinogradov, Ivan Izmailov, Simon Steshin, Kong T. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent successes in virtual screening have been made possible by large models
and extensive chemical libraries. However, combining these elements is
challenging: the larger the model, the more expensive it is to run, making
ultra-large libraries unfeasible. To address this, we developed a
target-agnostic, efficacy-based molecule search model, which allows us to find
structurally dissimilar molecules with similar biological activities. We used
the best practices to design fast retrieval system, based on
processor-optimized SIMD instructions, enabling us to screen the ultra-large
40B Enamine REAL library with 100\% recall rate. We extensively benchmarked our
model and several state-of-the-art models for both speed performance and
retrieval quality of novel molecules.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Jung Ling, Salomé Bru, Julia Puig, Florian Vixège, Simon Mendez, Franck Nicoud, Pierre-Yves Courand, Olivier Bernard, Damien Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify
color Doppler in cardiac imaging. In this study, we propose novel alternatives
to the traditional iVFM optimization scheme by utilizing physics-informed
neural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.
When evaluated on simulated color Doppler images derived from a
patient-specific computational fluid dynamics model and in vivo Doppler
acquisitions, both approaches demonstrate comparable reconstruction performance
to the original iVFM algorithm. The efficiency of PINNs is boosted through
dual-stage optimization and pre-optimized weights. On the other hand, the
nnU-Net method excels in generalizability and real-time capabilities. Notably,
nnU-Net shows superior robustness on sparse and truncated Doppler data while
maintaining independence from explicit boundary conditions. Overall, our
results highlight the effectiveness of these methods in reconstructing
intraventricular vector blood flow. The study also suggests potential
applications of PINNs in ultrafast color Doppler imaging and the incorporation
of fluid dynamics equations to derive biomarkers for cardiovascular diseases
based on blood flow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, accepted for publication in IEEE TUFFC; camera ready
  corrections, corrected acknowledgments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank
  Modifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05162v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05162v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, Peter Henderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show inherent brittleness in their safety
mechanisms, as evidenced by their susceptibility to jailbreaking and even
non-malicious fine-tuning. This study explores this brittleness of safety
alignment by leveraging pruning and low-rank modifications. We develop methods
to identify critical regions that are vital for safety guardrails, and that are
disentangled from utility-relevant regions at both the neuron and rank levels.
Surprisingly, the isolated regions we find are sparse, comprising about $3\%$
at the parameter level and $2.5\%$ at the rank level. Removing these regions
compromises safety without significantly impacting utility, corroborating the
inherent brittleness of the model's safety mechanisms. Moreover, we show that
LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications
to the safety-critical regions are restricted. These findings underscore the
urgent need for more robust safety strategies in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures. Project page is available at
  https://boyiwei.com/alignment-attribution/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shaping New Norms for AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Baronchelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Artificial Intelligence (AI) becomes increasingly integrated into our
lives, the need for new norms is urgent. However, AI evolves at a much faster
pace than the characteristic time of norm formation, posing an unprecedented
challenge to our societies. This paper examines possible criticalities of the
processes of norm formation surrounding AI. Thus, it focuses on how new norms
can be established, rather than on what these norms should be. It distinguishes
different scenarios based on the centralisation or decentralisation of the norm
formation process, analysing the cases where new norms are shaped by formal
authorities, informal institutions, or emerge spontaneously in a bottom-up
fashion. On the latter point, the paper reports a conversation with ChatGPT in
which the LLM discusses some of the emerging norms it has observed. Far from
seeking exhaustiveness, this article aims to offer readers interpretive tools
to understand society's response to the growing pervasiveness of AI. An outlook
on how AI could influence the formation of future social norms emphasises the
importance for open societies to anchor their formal deliberation process in an
open, inclusive, and transparent public discourse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CHESS: Contextual Harnessing for <span class="highlight-title">Efficient</span> SQL Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayan Talaei, Mohammadreza Pourreza, Yu-Chen Chang, Azalia Mirhoseini, Amin Saberi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing large language models (LLMs) for transforming natural language
questions into SQL queries (text-to-SQL) is a promising yet challenging
approach, particularly when applied to real-world databases with complex and
extensive schemas. In particular, effectively incorporating data catalogs and
database values for SQL generation remains an obstacle, leading to suboptimal
solutions. We address this problem by proposing a new pipeline that effectively
retrieves relevant data and context, selects an efficient schema, and
synthesizes correct and efficient SQL queries. To increase retrieval precision,
our pipeline introduces a hierarchical retrieval method leveraging
model-generated keywords, locality-sensitive hashing indexing, and vector
databases. Additionally, we have developed an adaptive schema pruning technique
that adjusts based on the complexity of the problem and the model's context
size. Our approach generalizes to both frontier proprietary models like GPT-4
and open-source models such as Llama-3-70B. Through a series of ablation
studies, we demonstrate the effectiveness of each component of our pipeline and
its impact on the end-to-end performance. Our method achieves new
state-of-the-art performance on the cross-domain challenging BIRD dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WebCanvas: Benchmarking Web Agents in Online Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12373v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12373v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, Zhengyang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For web agents to be practically useful, they must adapt to the continuously
evolving web environment characterized by frequent updates to user interfaces
and content. However, most existing benchmarks only capture the static aspects
of the web. To bridge this gap, we introduce WebCanvas, an innovative online
evaluation framework for web agents that effectively addresses the dynamic
nature of web interactions. WebCanvas contains three main components to
facilitate realistic assessments: (1) A novel evaluation metric which reliably
capture critical intermediate actions or states necessary for task completions
while disregarding noise caused by insignificant events or changed
web-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version
of original Mind2Web static dataset containing 542 tasks with 2439 intermediate
evaluation states; (3) Lightweight and generalizable annotation tools and
testing pipelines that enables the community to collect and maintain the
high-quality, up-to-date dataset. Building on WebCanvas, we open-source an
agent framework with extensible modules for reasoning, providing a foundation
for the community to conduct online inference and evaluations. Our
best-performing agent achieves a task success rate of 23.1% and a task
completion rate of 48.8% on the Mind2Web-Live test set. Additionally, we
analyze the performance discrepancies across various websites, domains, and
experimental environments. We encourage the community to contribute further
insights on online agent evaluation, thereby advancing this field of research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our platform, tool and dataset are publically available at
  https://www.imean.ai/web-canvas/ and
  https://huggingface.co/datasets/iMeanAI/Mind2Web-Live/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07610v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07610v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wang, Guozheng Ma, Ziqiao Meng, Zeyu Qin, Li Shen, Zhong Zhang, Bingzhe Wu, Liu Liu, Yatao Bian, Tingyang Xu, Xueqian Wang, Peilin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-alignment is an effective way to reduce the cost of human annotation
while ensuring promising model capability. However, most current methods
complete the data collection and training steps in a single round, which may
overlook the continuously improving ability of self-aligned models. This gives
rise to a key query: What if we do multi-time bootstrapping self-alignment?
Does this strategy enhance model performance or lead to rapid degradation? In
this paper, our pioneering exploration delves into the impact of bootstrapping
self-alignment on large language models. Our findings reveal that bootstrapping
self-alignment markedly surpasses the single-round approach, by guaranteeing
data diversity from in-context learning. To further exploit the capabilities of
bootstrapping, we investigate and adjust the training order of data, which
yields improved performance of the model. Drawing on these findings, we propose
Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
few-shot ability to boost zero or one-shot performance. Based on easy-to-hard
training recipe, we propose SOFT+ which further boost self-alignment's
performance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across
various classification and generation tasks, highlighting the potential of
bootstrapping self-alignment on continually enhancing model alignment
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GPTFUZZER: Red Teaming Large Language Models with Auto-Generated
  Jailbreak Prompts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10253v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10253v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Yu, Xingwei Lin, Zheng Yu, Xinyu Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently experienced tremendous popularity
and are widely used from casual conversations to AI-driven programming.
However, despite their considerable success, LLMs are not entirely reliable and
can give detailed guidance on how to conduct harmful or illegal activities.
While safety measures can reduce the risk of such outputs, adversarial
jailbreak attacks can still exploit LLMs to produce harmful content. These
jailbreak templates are typically manually crafted, making large-scale testing
challenging.
  In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing
framework inspired by the AFL fuzzing framework. Instead of manual engineering,
GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs.
At its core, GPTFuzz starts with human-written templates as initial seeds, then
mutates them to produce new templates. We detail three key components of
GPTFuzz: a seed selection strategy for balancing efficiency and variability,
mutate operators for creating semantically equivalent or similar sentences, and
a judgment model to assess the success of a jailbreak attack.
  We evaluate GPTFuzz against various commercial and open-source LLMs,
including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our
results indicate that GPTFuzz consistently produces jailbreak templates with a
high success rate, surpassing human-crafted templates. Remarkably, GPTFuzz
achieves over 90% attack success rates against ChatGPT and Llama-2 models, even
with suboptimal initial seed templates. We anticipate that GPTFuzz will be
instrumental for researchers and practitioners in examining LLM robustness and
will encourage further exploration into enhancing LLM safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the nature of large language models: A caution against
  anthropocentrism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07683v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07683v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ann Speed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI models garnered a large amount of public attention and
speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion
camps exist: one excited about possibilities these models offer for fundamental
changes to human tasks, and another highly concerned about power these models
seem to have. To address these concerns, we assessed several LLMs, primarily
GPT 3.5, using standard, normed, and validated cognitive and personality
measures. For this seedling project, we developed a battery of tests that
allowed us to estimate the boundaries of some of these models capabilities, how
stable those capabilities are over a short period of time, and how they compare
to humans. Our results indicate that LLMs are unlikely to have developed
sentience, although its ability to respond to personality inventories is
interesting. GPT3.5 did display large variability in both cognitive and
personality measures over repeated observations, which is not expected if it
had a human-like personality. Variability notwithstanding, LLMs display what in
a human would be considered poor mental health, including low self-esteem,
marked dissociation from reality, and in some cases narcissism and psychopathy,
despite upbeat and helpful responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite
  Kernel Strategy in One-Class Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06530v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06530v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Uzair Zahid, Aysen Degerli, Fahad Sohrab, Serkan Kiranyaz, Tahir Hamid, Rashid Mazhar, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early detection of myocardial infarction (MI), a critical condition arising
from coronary artery disease (CAD), is vital to prevent further myocardial
damage. This study introduces a novel method for early MI detection using a
one-class classification (OCC) algorithm in echocardiography. Our study
overcomes the challenge of limited echocardiography data availability by
adopting a novel approach based on Multi-modal Subspace Support Vector Data
Description. The proposed technique involves a specialized MI detection
framework employing multi-view echocardiography incorporating a composite
kernel in the non-linear projection trick, fusing Gaussian and Laplacian
sigmoid functions. Additionally, we enhance the update strategy of the
projection matrices by adapting maximization for both or one of the modalities
in the optimization process. Our method boosts MI detection capability by
efficiently transforming features extracted from echocardiography data into an
optimized lower-dimensional subspace. The OCC model trained specifically on
target class instances from the comprehensive HMC-QU dataset that includes
multiple echocardiography views indicates a marked improvement in MI detection
accuracy. Our findings reveal that our proposed multi-view approach achieves a
geometric mean of 71.24%, signifying a substantial advancement in
echocardiography-based MI diagnosis and offering more precise and efficient
diagnostic tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Muffin or Chihuahua? Challenging Multimodal Large Language Models with
  Multipanel VQA <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15847v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15847v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo, Xinze Guan, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multipanel images, commonly seen as web screenshots, posters, etc., pervade
our daily lives. These images, characterized by their composition of multiple
subfigures in distinct layouts, effectively convey information to people.
Toward building advanced multimodal AI applications, such as agents that
understand complex scenes and navigate through webpages, the skill of
multipanel visual reasoning is essential, and a comprehensive evaluation of
models in this regard is important. Therefore, we introduce Multipanel Visual
Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets
of questions, answers, and multipanel images that specifically challenge models
in comprehending multipanel images. Our evaluation shows that questions in the
MultipanelVQA benchmark pose significant challenges to the state-of-the-art
Multimodal Large Language Models (MLLMs) tested, even though humans can attain
approximately 99% accuracy on these questions. Distinctively, the MultipanelVQA
benchmark features synthetically generated multipanel images specifically
crafted to isolate and assess the impact of various factors, such as the
layout, on MLLMs' multipanel image comprehension abilities. As a result, in
addition to benchmarking the capabilities of MLLMs in understanding multipanel
images, we analyze various factors of the multipanel image that affect MLLMs'
performance with synthetic data and offer insights for enhancement. Code and
data are released at https://sites.google.com/view/multipanelvqa/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-26T00:00:00Z">2024-06-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Image and Video Processing <span class="chip" style="font-size: 60%">25</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessment of Clonal Hematopoiesis of Indeterminate Potential from
  Cardiac Magnetic Resonance Imaging using Deep Learning in a Cardio-oncology
  Population 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangeon Ryu, Shawn Ahn, Jeacy Espinoza, Alokkumar Jha, Stephanie Halene, James S. Duncan, Jennifer M Kwan, Nicha C. Dvornek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: We propose a novel method to identify who may likely have clonal
hematopoiesis of indeterminate potential (CHIP), a condition characterized by
the presence of somatic mutations in hematopoietic stem cells without
detectable hematologic malignancy, using deep learning techniques. Methods: We
developed a convolutional neural network (CNN) to predict CHIP status using 4
different views from standard delayed gadolinium-enhanced cardiac magnetic
resonance imaging (CMR). We used 5-fold cross validation on 82 cardio-oncology
patients to assess the performance of our model. Different algorithms were
compared to find the optimal patient-level prediction method using the
image-level CNN predictions. Results: We found that the best model had an area
under the receiver operating characteristic curve of 0.85 and an accuracy of
82%. Conclusions: We conclude that a deep learning-based diagnostic approach
for CHIP using CMR is promising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDA-UIE: An Iterative Framework for Deep Network-based Degradation Aware
  Underwater Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranjali Singh, Prithwijit Guha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater image quality is affected by fluorescence, low illumination,
absorption, and scattering. Recent works in underwater image enhancement have
proposed different deep network architectures to handle these problems. Most of
these works have proposed a single network to handle all the challenges. We
believe that deep networks trained for specific conditions deliver better
performance than a single network learned from all degradation cases.
Accordingly, the first contribution of this work lies in the proposal of an
iterative framework where a single dominant degradation condition is identified
and resolved. This proposal considers the following eight degradation
conditions -- low illumination, low contrast, haziness, blurred image, presence
of noise and color imbalance in three different channels. A deep network is
designed to identify the dominant degradation condition. Accordingly, an
appropriate deep network is selected for degradation condition-specific
enhancement. The second contribution of this work is the construction of
degradation condition specific datasets from good quality images of two
standard datasets (UIEB and EUVP). This dataset is used to learn the condition
specific enhancement networks. The proposed approach is found to outperform
nine baseline methods on UIEB and EUVP datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Repeat and Concatenate: 2D to 3D Image Translation with 3D to 3D
  Generative Modeling <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abril Corona-Figueroa, Hubert P. H. Shum, Chris G. Willcocks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates a 2D to 3D image translation method with a
straightforward technique, enabling correlated 2D X-ray to 3D CT-like
reconstruction. We observe that existing approaches, which integrate
information across multiple 2D views in the latent space, lose valuable signal
information during latent encoding. Instead, we simply repeat and concatenate
the 2D views into higher-channel 3D volumes and approach the 3D reconstruction
challenge as a straightforward 3D to 3D generative modeling problem,
sidestepping several complex modeling issues. This method enables the
reconstructed 3D volume to retain valuable information from the 2D inputs,
which are passed between channel states in a Swin UNETR backbone. Our approach
applies neural optimal transport, which is fast and stable to train,
effectively integrating signal information across multiple views without the
requirement for precise alignment; it produces non-collapsed reconstructions
that are highly faithful to the 2D views, even after limited training. We
demonstrate correlated results, both qualitatively and quantitatively, having
trained our model on a single dataset and evaluated its generalization ability
across six datasets, including out-of-distribution samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPRW 2024 - DCA in MI; Best Paper Award</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Reducing Activity with <span class="highlight-title">Distillation</span> and Regularization for Energy
  <span class="highlight-title">Efficient</span> Spiking Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Louis, Benoit Miramond, Alain Pegatoquet, Adrien Girard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interest in spiking neural networks (SNNs) has been growing steadily,
promising an energy-efficient alternative to formal neural networks (FNNs),
commonly known as artificial neural networks (ANNs). Despite increasing
interest, especially for Edge applications, these event-driven neural networks
suffered from their difficulty to be trained compared to FNNs. To alleviate
this problem, a number of innovative methods have been developed to provide
performance more or less equivalent to that of FNNs. However, the spiking
activity of a network during inference is usually not considered. While SNNs
may usually have performance comparable to that of FNNs, it is often at the
cost of an increase of the network's activity, thus limiting the benefit of
using them as a more energy-efficient solution.
  In this paper, we propose to leverage Knowledge Distillation (KD) for SNNs
training with surrogate gradient descent in order to optimize the trade-off
between performance and spiking activity. Then, after understanding why KD led
to an increase in sparsity, we also explored Activations regularization and
proposed a novel method with Logits Regularization. These approaches, validated
on several datasets, clearly show a reduction in network spiking activity
(-26.73% on GSC and -14.32% on CIFAR-10) while preserving accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-modal Evidential Fusion Network for Trusted PET/CT Tumor
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Qi, Li Lin, Jiajun Wang, Jingya Zhang, Bin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of tumors in PET/CT images is important in
computer-aided diagnosis and treatment of cancer. The key issue of such a
segmentation problem lies in the effective integration of complementary
information from PET and CT images. However, the quality of PET and CT images
varies widely in clinical settings, which leads to uncertainty in the modality
information extracted by networks. To take the uncertainty into account in
multi-modal information fusion, this paper proposes a novel Multi-modal
Evidential Fusion Network (MEFN) comprising a Cross-Modal Feature Learning
(CFL) module and a Multi-modal Trusted Fusion (MTF) module. The CFL module
reduces the domain gap upon modality conversion and highlights common tumor
features, thereby alleviating the needs of the segmentation module to handle
modality specificity. The MTF module utilizes mutual attention mechanisms and
an uncertainty calibrator to fuse modality features based on modality
uncertainty and then fuse the segmentation results under the guidance of
Dempster-Shafer Theory. Besides, a new uncertainty perceptual loss is
introduced to force the model focusing on uncertain features and hence improve
its ability to extract trusted modality information. Extensive comparative
experiments are conducted on two publicly available PET/CT datasets to evaluate
the performance of our proposed method whose results demonstrate that our MEFN
significantly outperforms state-of-the-art methods with improvements of 2.15%
and 3.23% in DSC scores on the AutoPET dataset and the Hecktor dataset,
respectively. More importantly, our model can provide radiologists with
credible uncertainty of the segmentation results for their decision in
accepting or rejecting the automatic segmentation results, which is
particularly important for clinical applications. Our code will be available at
https://github.com/QPaws/MEFN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatial-temporal Hierarchical Reinforcement Learning for Interpretable
  Pathology Image <span class="highlight-title">Super-Resolution</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenting Chen, Jie Liu, Tommy W. S. Chow, Yixuan Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathology image are essential for accurately interpreting lesion cells in
cytopathology screening, but acquiring high-resolution digital slides requires
specialized equipment and long scanning times. Though super-resolution (SR)
techniques can alleviate this problem, existing deep learning models recover
pathology image in a black-box manner, which can lead to untruthful biological
details and misdiagnosis. Additionally, current methods allocate the same
computational resources to recover each pixel of pathology image, leading to
the sub-optimal recovery issue due to the large variation of pathology image.
In this paper, we propose the first hierarchical reinforcement learning
framework named Spatial-Temporal hierARchical Reinforcement Learning (STAR-RL),
mainly for addressing the aforementioned issues in pathology image
super-resolution problem. We reformulate the SR problem as a Markov decision
process of interpretable operations and adopt the hierarchical recovery
mechanism in patch level, to avoid sub-optimal recovery. Specifically, the
higher-level spatial manager is proposed to pick out the most corrupted patch
for the lower-level patch worker. Moreover, the higher-level temporal manager
is advanced to evaluate the selected patch and determine whether the
optimization should be stopped earlier, thereby avoiding the over-processed
problem. Under the guidance of spatial-temporal managers, the lower-level patch
worker processes the selected patch with pixel-wise interpretable actions at
each time step. Experimental results on medical images degraded by different
kernels show the effectiveness of STAR-RL. Furthermore, STAR-RL validates the
promotion in tumor diagnosis with a large margin and shows generalizability
under various degradations. The source code is available at
https://github.com/CUHK-AIM-Group/STAR-RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE TRANSACTIONS ON MEDICAL IMAGING (TMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Deepfake Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sowdagar Mahammad Shahid, Sudev Kumar Padhi, Umesh Kashyap, Sk. Subidh Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The landscape of fake media creation changed with the introduction of
Generative Adversarial Networks (GAN s). Fake media creation has been on the
rise with the rapid advances in generation technology, leading to new
challenges in Detecting fake media. A fundamental characteristic of GAN s is
their sensitivity to parameter initialization, known as seeds. Each distinct
seed utilized during training leads to the creation of unique model instances,
resulting in divergent image outputs despite employing the same architecture.
This means that even if we have one GAN architecture, it can produce countless
variations of GAN models depending on the seed used. Existing methods for
attributing deepfakes work well only if they have seen the specific GAN model
during training. If the GAN architectures are retrained with a different seed,
these methods struggle to attribute the fakes. This seed dependency issue made
it difficult to attribute deepfakes with existing methods. We proposed a
generalized deepfake attribution network (GDA-N et) to attribute fake images to
their respective GAN architectures, even if they are generated from a retrained
version of the GAN architecture with a different seed (cross-seed) or from the
fine-tuned version of the existing GAN model. Extensive experiments on
cross-seed and fine-tuned data of GAN models show that our method is highly
effective compared to existing methods. We have provided the source code to
validate our results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative artificial intelligence in ophthalmology: multimodal retinal
  images for the diagnosis of Alzheimer's disease with convolutional neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        I. R. Slootweg, M. Thach, K. R. Curro-Tafili, F. D. Verbraak, F. H. Bouwman, Y. A. L. Pijnenburg, J. F. Boer, J. H. P. de Kwisthout, L. Bagheriye, P. J. González
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background/Aim. This study aims to predict Amyloid Positron Emission
Tomography (AmyloidPET) status with multimodal retinal imaging and
convolutional neural networks (CNNs) and to improve the performance through
pretraining with synthetic data. Methods. Fundus autofluorescence, optical
coherence tomography (OCT), and OCT angiography images from 328 eyes of 59
AmyloidPET positive subjects and 108 AmyloidPET negative subjects were used for
classification. Denoising Diffusion Probabilistic Models (DDPMs) were trained
to generate synthetic images and unimodal CNNs were pretrained on synthetic
data and finetuned on real data or trained solely on real data. Multimodal
classifiers were developed to combine predictions of the four unimodal CNNs
with patient metadata. Class activation maps of the unimodal classifiers
provided insight into the network's attention to inputs. Results. DDPMs
generated diverse, realistic images without memorization. Pretraining unimodal
CNNs with synthetic data improved AUPR at most from 0.350 to 0.579. Integration
of metadata in multimodal CNNs improved AUPR from 0.486 to 0.634, which was the
best overall best classifier. Class activation maps highlighted relevant
retinal regions which correlated with AD. Conclusion. Our method for generating
and leveraging synthetic data has the potential to improve AmyloidPET
prediction from multimodal retinal imaging. A DDPM can generate realistic and
unique multimodal synthetic retinal images. Our best performing unimodal and
multimodal classifiers were not pretrained on synthetic data, however
pretraining with synthetic data slightly improved classification performance
for two out of the four modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConStyle v2: A Strong Prompter for All-in-One <span class="highlight-title">Image Restoration</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Fan, Junhao Zhang, Liang Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces ConStyle v2, a strong plug-and-play prompter designed
to output clean visual prompts and assist U-Net Image Restoration models in
handling multiple degradations. The joint training process of IRConStyle, an
Image Restoration framework consisting of ConStyle and a general restoration
network, is divided into two stages: first, pre-training ConStyle alone, and
then freezing its weights to guide the training of the general restoration
network. Three improvements are proposed in the pre-training stage to train
ConStyle: unsupervised pre-training, adding a pretext task (i.e.
classification), and adopting knowledge distillation. Without bells and
whistles, we can get ConStyle v2, a strong prompter for all-in-one Image
Restoration, in less than two GPU days and doesn't require any fine-tuning.
Extensive experiments on Restormer (transformer-based), NAFNet (CNN-based),
MAXIM-1S (MLP-based), and a vanilla CNN network demonstrate that ConStyle v2
can enhance any U-Net style Image Restoration models to all-in-one Image
Restoration models. Furthermore, models guided by the well-trained ConStyle v2
exhibit superior performance in some specific degradation compared to ConStyle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Stream: Malignant Region Learning for Breast Cancer Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Rehman, Sarfaraz Hussein, Waqas Sultani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early diagnosis of breast cancer (BC) significantly contributes to reducing
the mortality rate worldwide. The detection of different factors and biomarkers
such as Estrogen receptor (ER), Progesterone receptor (PR), Human epidermal
growth factor receptor 2 (HER2) gene, Histological grade (HG), Auxiliary lymph
node (ALN) status, and Molecular subtype (MS) can play a significant role in
improved BC diagnosis. However, the existing methods predict only a single
factor which makes them less suitable to use in diagnosis and designing a
strategy for treatment. In this paper, we propose to classify the six essential
indicating factors (ER, PR, HER2, ALN, HG, MS) for early BC diagnosis using
H\&E stained WSI's. To precisely capture local neighboring relationships, we
use spatial and frequency domain information from the large patch size of WSI's
malignant regions. Furthermore, to cater the variable number of regions of
interest sizes and give due attention to each region, we propose a malignant
region learning attention network. Our experimental results demonstrate that
combining spatial and frequency information using the malignant region learning
module significantly improves multi-factor and single-factor classification
performance on publicly available datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review (Biomedical Signal Processing and Control)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EFCNet: Every Feature Counts for Small Medical Object Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingjie Kong, Qiaoling Wei, Chengming Xu, Han Chen, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the segmentation of very small medical objects with
significant clinical value. While Convolutional Neural Networks (CNNs),
particularly UNet-like models, and recent Transformers have shown substantial
progress in image segmentation, our empirical findings reveal their poor
performance in segmenting the small medical objects and lesions concerned in
this paper. This limitation may be attributed to information loss during their
encoding and decoding process. In response to this challenge, we propose a
novel model named EFCNet for small object segmentation in medical images. Our
model incorporates two modules: the Cross-Stage Axial Attention Module (CSAA)
and the Multi-Precision Supervision Module (MPS). These modules address
information loss during encoding and decoding procedures, respectively.
Specifically, CSAA integrates features from all stages of the encoder to
adaptively learn suitable information needed in different decoding stages,
thereby reducing information loss in the encoder. On the other hand, MPS
introduces a novel multi-precision supervision mechanism to the decoder. This
mechanism prioritizes attention to low-resolution features in the initial
stages of the decoder, mitigating information loss caused by subsequent
convolution and sampling processes and enhancing the model's global perception.
We evaluate our model on two benchmark medical image datasets. The results
demonstrate that EFCNet significantly outperforms previous segmentation methods
designed for both medical and normal images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Lung Nodule Dataset with Histopathology-based Cancer Type Annotation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muwei Jian, Hongyu Chen, Zaiyong Zhang, Nan Yang, Haorang Zhang, Lifu Ma, Wenjing Xu, Huixiang Zhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Computer-Aided Diagnosis (CAD) systems have emerged as
indispensable tools in clinical diagnostic workflows, significantly alleviating
the burden on radiologists. Nevertheless, despite their integration into
clinical settings, CAD systems encounter limitations. Specifically, while CAD
systems can achieve high performance in the detection of lung nodules, they
face challenges in accurately predicting multiple cancer types. This limitation
can be attributed to the scarcity of publicly available datasets annotated with
expert-level cancer type information. This research aims to bridge this gap by
providing publicly accessible datasets and reliable tools for medical
diagnosis, facilitating a finer categorization of different types of lung
diseases so as to offer precise treatment recommendations. To achieve this
objective, we curated a diverse dataset of lung Computed Tomography (CT)
images, comprising 330 annotated nodules (nodules are labeled as bounding
boxes) from 95 distinct patients. The quality of the dataset was evaluated
using a variety of classical classification and detection models, and these
promising results demonstrate that the dataset has a feasible application and
further facilitate intelligent auxiliary diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MFDNet: Multi-Frequency Deflare Network for <span class="highlight-title">Efficient</span> Nighttime Flare
  Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiguo Jiang, Xuhang Chen, Chi-Man Pun, Shuqiang Wang, Wei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When light is scattered or reflected accidentally in the lens, flare
artifacts may appear in the captured photos, affecting the photos' visual
quality. The main challenge in flare removal is to eliminate various flare
artifacts while preserving the original content of the image. To address this
challenge, we propose a lightweight Multi-Frequency Deflare Network (MFDNet)
based on the Laplacian Pyramid. Our network decomposes the flare-corrupted
image into low and high-frequency bands, effectively separating the
illumination and content information in the image. The low-frequency part
typically contains illumination information, while the high-frequency part
contains detailed content information. So our MFDNet consists of two main
modules: the Low-Frequency Flare Perception Module (LFFPM) to remove flare in
the low-frequency part and the Hierarchical Fusion Reconstruction Module (HFRM)
to reconstruct the flare-free image. Specifically, to perceive flare from a
global perspective while retaining detailed information for image restoration,
LFFPM utilizes Transformer to extract global information while utilizing a
convolutional neural network to capture detailed local features. Then HFRM
gradually fuses the outputs of LFFPM with the high-frequency component of the
image through feature aggregation. Moreover, our MFDNet can reduce the
computational cost by processing in multiple frequency bands instead of
directly removing the flare on the input image. Experimental results
demonstrate that our approach outperforms state-of-the-art methods in removing
nighttime flare on real-world and synthetic images from the Flare7K dataset.
Furthermore, the computational complexity of our model is remarkably low.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Visual Computer journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-driven imaging geometric recovery of ultrahigh resolution robotic
  micro-CT for in-vivo and other applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhou Li, Guibin Zan, Wenbin Yun, Josef Uher, John Wen, Ge Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce an ultrahigh-resolution (50\mu m\) robotic micro-CT design for
localized imaging of carotid plaques using robotic arms, cutting-edge detector,
and machine learning technologies. To combat geometric error-induced artifacts
in interior CT scans, we propose a data-driven geometry estimation method that
maximizes the consistency between projection data and the reprojection
counterparts of a reconstructed volume. Particularly, we use a normalized cross
correlation metric to overcome the projection truncation effect. Our approach
is validated on a robotic CT scan of a sacrificed mouse and a micro-CT phantom
scan, both producing sharper images with finer details than that prior
correction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4-page paper for 8th International Conference on Computational and
  Mathematical Biomedical Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Pre-trained Models for FF-to-FFPE Histopathological Image
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilai Zhang, Jiawen Li, Peiran Liao, Jiali Hu, Tian Guan, Anjia Han, Yonghong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The two primary types of Hematoxylin and Eosin (H&E) slides in histopathology
are Formalin-Fixed Paraffin-Embedded (FFPE) and Fresh Frozen (FF). FFPE slides
offer high quality histopathological images but require a labor-intensive
acquisition process. In contrast, FF slides can be prepared quickly, but the
image quality is relatively poor. Our task is to translate FF images into FFPE
style, thereby improving the image quality for diagnostic purposes. In this
paper, we propose Diffusion-FFPE, a method for FF-to-FFPE histopathological
image translation using a pre-trained diffusion model. Specifically, we employ
a one-step diffusion model as the generator and fine-tune it with LoRA adapters
using adversarial learning objectives. To ensure that the model effectively
captures both global structural information and local details, we propose a
multi-scale feature fusion (MFF) module. This module utilizes two VAE encoders
to extract features of varying image sizes and performs feature fusion before
feeding them into the UNet. Furthermore, we utilize a pre-trained
vision-language model for histopathology as the backbone for the discriminator
to further improve performance We conducted FF-to-FFPE translation experiments
on the TCGA-NSCLC datasets, and our method achieved better performance compared
to other methods. The code and models are released at
https://github.com/QilaiZhang/Diffusion-FFPE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Cross Spatio-Temporal Pathology-based Lung Nodule Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muwei Jian, Haoran Zhang, Mingju Shao, Hongyu Chen, Huihui Huang, Yanjie Zhong, Changlei Zhang, Bin Wang, Penghui Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, intelligent analysis of lung nodules with the assistant of computer
aided detection (CAD) techniques can improve the accuracy rate of lung cancer
diagnosis. However, existing CAD systems and pulmonary datasets mainly focus on
Computed Tomography (CT) images from one single period, while ignoring the
cross spatio-temporal features associated with the progression of nodules
contained in imaging data from various captured periods of lung cancer. If the
evolution patterns of nodules across various periods in the patients' CT
sequences can be explored, it will play a crucial role in guiding the precise
screening identification of lung cancer. Therefore, a cross spatio-temporal
lung nodule dataset with pathological information for nodule identification and
diagnosis is constructed, which contains 328 CT sequences and 362 annotated
nodules from 109 patients. This comprehensive database is intended to drive
research in the field of CAD towards more practical and robust methods, and
also contribute to the further exploration of precision medicine related field.
To ensure patient confidentiality, we have removed sensitive information from
the dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-End Optimization of Metasurfaces for Imaging with Compressed
  Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12348v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12348v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Arya, William F. Li, Charles Roques-Carmes, Marin Soljačić, Steven G. Johnson, Zin Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for the end-to-end optimization of metasurface imaging
systems that reconstruct targets using compressed sensing, a technique for
solving underdetermined imaging problems when the target object exhibits
sparsity (i.e. the object can be described by a small number of non-zero
values, but the positions of these values are unknown). We nest an iterative,
unapproximated compressed sensing reconstruction algorithm into our end-to-end
optimization pipeline, resulting in an interpretable, data-efficient method for
maximally leveraging metaoptics to exploit object sparsity. We apply our
framework to super-resolution imaging and high-resolution depth imaging with a
phase-change material. In both situations, our end-to-end framework
computationally discovers optimal metasurface structures for compressed sensing
recovery, automatically balancing a number of complicated design considerations
to select an imaging measurement matrix from a complex, physically constrained
manifold with millions ofdimensions. The optimized metasurface imaging systems
are robust to noise, significantly improving over random scattering surfaces
and approaching the ideal compressed sensing performance of a Gaussian matrix,
showing how a physical metasurface system can demonstrably approach the
mathematical limits of compressed sensing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diff3Dformer: Leveraging Slice Sequence <span class="highlight-title">Diffusion</span> for Enhanced 3D CT
  Classification with <span class="highlight-title">Transformer</span> Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Jin, Yingying Fang, Jiahao Huang, Caiwen Xu, Simon Walsh, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The manifestation of symptoms associated with lung diseases can vary in
different depths for individual patients, highlighting the significance of 3D
information in CT scans for medical image classification. While Vision
Transformer has shown superior performance over convolutional neural networks
in image classification tasks, their effectiveness is often demonstrated on
sufficiently large 2D datasets and they easily encounter overfitting issues on
small medical image datasets. To address this limitation, we propose a
Diffusion-based 3D Vision Transformer (Diff3Dformer), which utilizes the latent
space of the Diffusion model to form the slice sequence for 3D analysis and
incorporates clustering attention into ViT to aggregate repetitive information
within 3D CT scans, thereby harnessing the power of the advanced transformer in
3D classification tasks on small datasets. Our method exhibits improved
performance on two different scales of small datasets of 3D lung CT scans,
surpassing the state of the art 3D methods and other transformer-based
approaches that emerged during the COVID-19 pandemic, demonstrating its robust
and superior performance across different scales of data. Experimental results
underscore the superiority of our proposed method, indicating its potential for
enhancing medical image classification tasks in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Arbitrary-Scale Histopathology Image <span class="highlight-title">Super-resolution</span>: An
  <span class="highlight-title">Efficient</span> Dual-branch Framework via Implicit Self-texture Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15613v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15613v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghong Duan, Linhao Qu, Zhiwei Yang, Manning Wang, Chenxi Zhang, Zhijian Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality whole-slide scanners are expensive, complex, and time-consuming,
thus limiting the acquisition and utilization of high-resolution pathology
whole-slide images in daily clinical work. Deep learning-based single-image
super-resolution techniques are an effective way to solve this problem by
synthesizing high-resolution images from low-resolution ones. However, the
existing super-resolution models applied in pathology images can only work in
fixed integer magnifications, significantly decreasing their applicability.
Though methods based on implicit neural representation have shown promising
results in arbitrary-scale super-resolution of natural images, applying them
directly to pathology images is inadequate because they have unique
fine-grained image textures different from natural images. Thus, we propose an
Implicit Self-Texture Enhancement-based dual-branch framework (ISTE) for
arbitrary-scale super-resolution of pathology images to address this challenge.
ISTE contains a pixel learning branch and a texture learning branch, which
first learn pixel features and texture features, respectively. Then, we design
a two-stage texture enhancement strategy to fuse the features from the two
branches to obtain the super-resolution results, where the first stage is
feature-based texture enhancement, and the second stage is spatial-domain-based
texture enhancement. Extensive experiments on three public datasets show that
ISTE outperforms existing fixed-scale and arbitrary-scale algorithms at
multiple magnifications and helps to improve downstream task performance. To
the best of our knowledge, this is the first work to achieve arbitrary-scale
super-resolution in pathology images. Codes will be available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstantGroup: Instant Template Generation for Scalable Group of Brain
  MRI Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi He, Albert C. S. Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Template generation is a critical step in groupwise image registration, which
involves aligning a group of subjects into a common space. While existing
methods can generate high-quality template images, they often incur substantial
time costs or are limited by fixed group scales. In this paper, we present
InstantGroup, an efficient groupwise template generation framework based on
variational autoencoder (VAE) models that leverage latent representations'
arithmetic properties, enabling scalability to groups of any size. InstantGroup
features a Dual VAEs backbone with shared-weight twin networks to handle pairs
of inputs and incorporates a Displacement Inversion Module (DIM) to maintain
template unbiasedness and a Subject-Template Alignment Module (STAM) to improve
template quality and registration accuracy. Experiments on 3D brain MRI scans
from the OASIS and ADNI datasets reveal that InstantGroup dramatically reduces
runtime, generating templates within seconds for various group sizes while
maintaining superior performance compared to state-of-the-art baselines on
quantitative metrics, including unbiasedness and registration accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Painting Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.13459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.13459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Galerne, Lara Raad, José Lezama, Jean-Michel Morel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural style transfer (NST) is a deep learning technique that produces an
unprecedentedly rich style transfer from a style image to a content image. It
is particularly impressive when it comes to transferring style from a painting
to an image. NST was originally achieved by solving an optimization problem to
match the global statistics of the style image while preserving the local
geometric features of the content image. The two main drawbacks of this
original approach is that it is computationally expensive and that the
resolution of the output images is limited by high GPU memory requirements.
Many solutions have been proposed to both accelerate NST and produce images
with larger size. However, our investigation shows that these accelerated
methods all compromise the quality of the produced images in the context of
painting style transfer. Indeed, transferring the style of a painting is a
complex task involving features at different scales, from the color palette and
compositional style to the fine brushstrokes and texture of the canvas. This
paper provides a solution to solve the original global optimization for
ultra-high resolution (UHR) images, enabling multiscale NST at unprecedented
image sizes. This is achieved by spatially localizing the computation of each
forward and backward passes through the VGG network. Extensive qualitative and
quantitative comparisons, as well as a \textcolor{coverletter}{perceptual
study}, show that our method produces style transfer of unmatched quality for
such high-resolution painting styles. By a careful comparison, we show that
state-of-the-art fast methods are still prone to artifacts, thus suggesting
that fast painting style transfer remains an open problem. Source code is
available at https://github.com/bgalerne/scaling_painting_style_transfer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, 4 tables, accepted at EGSR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Change<span class="highlight-title">Mamba</span>: Remote Sensing Change Detection with Spatio-Temporal State
  Space Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03425v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03425v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongruixuan Chen, Jian Song, Chengxi Han, Junshi Xia, Naoto Yokoya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNN) and Transformers have made impressive
progress in the field of remote sensing change detection (CD). However, both
architectures have inherent shortcomings: CNN are constrained by a limited
receptive field that may hinder their ability to capture broader spatial
contexts, while Transformers are computationally intensive, making them costly
to train and deploy on large datasets. Recently, the Mamba architecture, based
on state space models, has shown remarkable performance in a series of natural
language processing tasks, which can effectively compensate for the
shortcomings of the above two architectures. In this paper, we explore for the
first time the potential of the Mamba architecture for remote sensing CD tasks.
We tailor the corresponding frameworks, called MambaBCD, MambaSCD, and
MambaBDA, for binary change detection (BCD), semantic change detection (SCD),
and building damage assessment (BDA), respectively. All three frameworks adopt
the cutting-edge Visual Mamba architecture as the encoder, which allows full
learning of global spatial contextual information from the input images. For
the change decoder, which is available in all three architectures, we propose
three spatio-temporal relationship modeling mechanisms, which can be naturally
combined with the Mamba architecture and fully utilize its attribute to achieve
spatio-temporal interaction of multi-temporal features, thereby obtaining
accurate change information. On five benchmark datasets, our proposed
frameworks outperform current CNN- and Transformer-based approaches without
using any complex training strategies or tricks, fully demonstrating the
potential of the Mamba architecture in CD tasks. Further experiments show that
our architecture is quite robust to degraded data. The source code will be
available in https://github.com/ChenHongruixuan/MambaCD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TGRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neuromorphic Visual Scene Understanding with Resonator Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.12880v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.12880v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alpha Renner, Lazar Supic, Andreea Danielescu, Giacomo Indiveri, Bruno A. Olshausen, Yulia Sandamirskaya, Friedrich T. Sommer, E. Paxon Frady
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing a visual scene by inferring the configuration of a generative model
is widely considered the most flexible and generalizable approach to scene
understanding. Yet, one major problem is the computational challenge of the
inference procedure, involving a combinatorial search across object identities
and poses. Here we propose a neuromorphic solution exploiting three key
concepts: (1) a computational framework based on Vector Symbolic Architectures
(VSA) with complex-valued vectors; (2) the design of Hierarchical Resonator
Networks (HRN) to factorize the non-commutative transforms translation and
rotation in visual scenes; (3) the design of a multi-compartment spiking phasor
neuron model for implementing complex-valued resonator networks on neuromorphic
hardware. The VSA framework uses vector binding operations to form a generative
image model in which binding acts as the equivariant operation for geometric
transformations. A scene can, therefore, be described as a sum of vector
products, which can then be efficiently factorized by a resonator network to
infer objects and their poses. The HRN features a partitioned architecture in
which vector binding is equivariant for horizontal and vertical translation
within one partition and for rotation and scaling within the other partition.
The spiking neuron model allows mapping the resonator network onto efficient
and low-power neuromorphic hardware. Our approach is demonstrated on synthetic
scenes composed of simple 2D shapes undergoing rigid geometric transformations
and color changes. A companion paper demonstrates the same approach in
real-world application scenarios for machine vision and robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures, minor revisions and extended supplementary
  material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MedMNIST-C: Comprehensive benchmark and improved classifier robustness
  by simulating realistic image corruptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Di Salvo, Sebastian Doerrich, Christian Ledig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of neural-network-based systems into clinical practice is
limited by challenges related to domain generalization and robustness. The
computer vision community established benchmarks such as ImageNet-C as a
fundamental prerequisite to measure progress towards those challenges. Similar
datasets are largely absent in the medical imaging community which lacks a
comprehensive benchmark that spans across imaging modalities and applications.
To address this gap, we create and open-source MedMNIST-C, a benchmark dataset
based on the MedMNIST+ collection covering 12 datasets and 9 imaging
modalities. We simulate task and modality-specific image corruptions of varying
severity to comprehensively evaluate the robustness of established algorithms
against real-world artifacts and distribution shifts. We further provide
quantitative evidence that our simple-to-use artificial corruptions allow for
highly performant, lightweight data augmentation to enhance model robustness.
Unlike traditional, generic augmentation strategies, our approach leverages
domain knowledge, exhibiting significantly higher robustness when compared to
widely adopted methods. By introducing MedMNIST-C and open-sourcing the
corresponding library allowing for targeted data augmentations, we contribute
to the development of increasingly robust methods tailored to the challenges of
medical imaging. The code is available at
https://github.com/francescodisalvo05/medmnistc-api}{github.com/francescodisalvo05/medmnistc-api .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Low-light Light Field Images with A Deep Compensation
  Unfolding Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.05404v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.05404v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianqiang Lyu, Junhui Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel and interpretable end-to-end learning framework,
called the deep compensation unfolding network (DCUNet), for restoring light
field (LF) images captured under low-light conditions. DCUNet is designed with
a multi-stage architecture that mimics the optimization process of solving an
inverse imaging problem in a data-driven fashion. The framework uses the
intermediate enhanced result to estimate the illumination map, which is then
employed in the unfolding process to produce a new enhanced result.
Additionally, DCUNet includes a content-associated deep compensation module at
each optimization stage to suppress noise and illumination map estimation
errors. To properly mine and leverage the unique characteristics of LF images,
this paper proposes a pseudo-explicit feature interaction module that
comprehensively exploits redundant information in LF images. The experimental
results on both simulated and real datasets demonstrate the superiority of our
DCUNet over state-of-the-art methods, both qualitatively and quantitatively.
Moreover, DCUNet preserves the essential geometric structure of enhanced LF
images much better. The code will be publicly available at
https://github.com/lyuxianqiang/LFLL-DCU.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-25T00:00:00Z">2024-06-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Image and Video Processing <span class="chip" style="font-size: 60%">32</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Highly Constrained Coded Aperture Imaging Systems Design Via a Knowledge
  <span class="highlight-title">Distillation</span> Approach <span class="chip">ICIP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Suarez-Rodriguez, Roman Jacome, Henry Arguello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational optical imaging (COI) systems have enabled the acquisition of
high-dimensional signals through optical coding elements (OCEs). OCEs encode
the high-dimensional signal in one or more snapshots, which are subsequently
decoded using computational algorithms. Currently, COI systems are optimized
through an end-to-end (E2E) approach, where the OCEs are modeled as a layer of
a neural network and the remaining layers perform a specific imaging task.
However, the performance of COI systems optimized through E2E is limited by the
physical constraints imposed by these systems. This paper proposes a knowledge
distillation (KD) framework for the design of highly physically constrained COI
systems. This approach employs the KD methodology, which consists of a
teacher-student relationship, where a high-performance, unconstrained COI
system (the teacher), guides the optimization of a physically constrained
system (the student) characterized by a limited number of snapshots. We
validate the proposed approach, using a binary coded apertures single pixel
camera for monochromatic and multispectral image reconstruction. Simulation
results demonstrate the superiority of the KD scheme over traditional E2E
optimization for the designing of highly physically constrained COI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures. Accepted at ICIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hot-Distance: Combining One-Hot and Signed Distance Embeddings for
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marwan Zouinkhi, Jeff L. Rhoades, Aubrey V. Weigel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are only as good as the data to which they are fit.
As such, it is always preferable to use as much data as possible in training
models. What data can be used for fitting a model depends a lot on the
formulation of the task. We introduce Hot-Distance, a novel segmentation target
that incorporates the strength of signed boundary distance prediction with the
flexibility of one-hot encoding, to increase the amount of usable training data
for segmentation of subcellular structures in focused ion beam scanning
electron microscopy (FIB-SEM).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 1 figure, in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Total Variation Regularization for Tomographic Reconstruction of
  Cylindrically Symmetric Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maliha Hossain, Charles A. Bouman, Brendt Wohlberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flash X-ray computed tomography (CT) is an important imaging modality for
characterization of high-speed dynamic events, such as Kolsky bar impact
experiments for the study of mechanical properties of materials subjected to
impulsive forces. Due to experimental constraints, the number of X-ray views
that can be obtained is typically very sparse in both space and time, requiring
strong priors in order to enable a CT reconstruction. In this paper, we propose
an effective method for exploiting the cylindrical symmetry inherent in the
experiment via a variant of total variation (TV) regularization that operates
in cylindrical coordinates, and demonstrate that it outperforms competing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptation of Echocardiography Segmentation Via Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnaud Judge, Thierry Judge, Nicolas Duchateau, Roman A. Sandler, Joseph Z. Sokol, Olivier Bernard, Pierre-Marc Jodoin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance of deep learning segmentation models is significantly challenged
in its transferability across different medical imaging domains, particularly
when aiming to adapt these models to a target domain with insufficient
annotated data for effective fine-tuning. While existing domain adaptation (DA)
methods propose strategies to alleviate this problem, these methods do not
explicitly incorporate human-verified segmentation priors, compromising the
potential of a model to produce anatomically plausible segmentations. We
introduce RL4Seg, an innovative reinforcement learning framework that reduces
the need to otherwise incorporate large expertly annotated datasets in the
target domain, and eliminates the need for lengthy manual human review. Using a
target dataset of 10,000 unannotated 2D echocardiographic images, RL4Seg not
only outperforms existing state-of-the-art DA methods in accuracy but also
achieves 99% anatomical validity on a subset of 220 expert-validated subjects
from the target domain. Furthermore, our framework's reward network offers
uncertainty estimates comparable with dedicated state-of-the-art uncertainty
methods, demonstrating the utility and effectiveness of RL4Seg in overcoming
domain adaptation challenges in medical image segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pixel-weighted Multi-pose Fusion for Metal Artifact Reduction in X-ray
  Computed Tomography <span class="chip">SP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diyu Yang, Craig A. J. Kemp, Soumendu Majee, Gregery T. Buzzard, Charles A. Bouman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  X-ray computed tomography (CT) reconstructs the internal morphology of a
three dimensional object from a collection of projection images, most commonly
using a single rotation axis. However, for objects containing dense materials
like metal, the use of a single rotation axis may leave some regions of the
object obscured by the metal, even though projections from other rotation axes
(or poses) might contain complementary information that would better resolve
these obscured regions.
  In this paper, we propose pixel-weighted Multi-pose Fusion to reduce metal
artifacts by fusing the information from complementary measurement poses into a
single reconstruction. Our method uses Multi-Agent Consensus Equilibrium
(MACE), an extension of Plug-and-Play, as a framework for integrating
projection data from different poses. A primary novelty of the proposed method
is that the output of different MACE agents are fused in a pixel-weighted
manner to minimize the effects of metal throughout the reconstruction. Using
real CT data on an object with and without metal inserts, we demonstrate that
the proposed pixel-weighted Multi-pose Fusion method significantly reduces
metal artifacts relative to single-pose reconstructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE MMSP 2024. arXiv admin note: substantial text
  overlap with arXiv:2209.07561</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mask-Guided <span class="highlight-title">Attention</span> U-Net for Enhanced Neonatal Brain Extraction and
  Image Preprocessing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bahram Jafrasteh, Simon Pedro Lubian-Lopez, Emiliano Trimarco, Macarena Roman Ruiz, Carmen Rodriguez Barrios, Yolanda Marin Almagro, Isabel Benavente-Fernandez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we introduce MGA-Net, a novel mask-guided attention neural
network, which extends the U-net model for precision neonatal brain imaging.
MGA-Net is designed to extract the brain from other structures and reconstruct
high-quality brain images. The network employs a common encoder and two
decoders: one for brain mask extraction and the other for brain region
reconstruction. A key feature of MGA-Net is its high-level mask-guided
attention module, which leverages features from the brain mask decoder to
enhance image reconstruction. To enable the same encoder and decoder to process
both MRI and ultrasound (US) images, MGA-Net integrates sinusoidal positional
encoding. This encoding assigns distinct positional values to MRI and US
images, allowing the model to effectively learn from both modalities.
Consequently, features learned from a single modality can aid in learning a
modality with less available data, such as US. We extensively validated the
proposed MGA-Net on diverse datasets from varied clinical settings and neonatal
age groups. The metrics used for assessment included the DICE similarity
coefficient, recall, and accuracy for image segmentation; structural similarity
for image reconstruction; and root mean squared error for total brain volume
estimation from 3D ultrasound images. Our results demonstrate that MGA-Net
significantly outperforms traditional methods, offering superior performance in
brain extraction and segmentation while achieving high precision in image
reconstruction and volumetric analysis. Thus, MGA-Net represents a robust and
effective preprocessing tool for MRI and 3D ultrasound images, marking a
significant advance in neuroimaging that enhances both research and clinical
diagnostics in the neonatal period and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain Tumor Classification using Vision <span class="highlight-title">Transformer</span> with Selective
  Cross-<span class="highlight-title">Attention</span> Mechanism and Feature Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Ali Labbaf Khaniki, Alireza Golkarieh, Mohammad Manthouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumor classification is a challenging task in medical image analysis.
In this paper, we propose a novel approach to brain tumor classification using
a vision transformer with a novel cross-attention mechanism. Our approach
leverages the strengths of transformers in modeling long-range dependencies and
multi-scale feature fusion. We introduce two new mechanisms to improve the
performance of the cross-attention fusion module: Feature Calibration Mechanism
(FCM) and Selective Cross-Attention (SCA). FCM calibrates the features from
different branches to make them more compatible, while SCA selectively attends
to the most informative features. Our experiments demonstrate that the proposed
approach outperforms other state-of-the-art methods in brain tumor
classification, achieving improved accuracy and efficiency. The proposed FCM
and SCA mechanisms can be easily integrated into other vision transformer
architectures, making them a promising direction for future research in medical
image analysis. Experimental results confirm that our approach surpasses
existing methods, achieving state-of-the-art performance in brain tumor
classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-based segmentation of adnexal lesions and ovarian implants
  in CT images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aneesh Rangnekar, Kevin M. Boehm, Emily A. Aherne, Ines Nikolovski, Natalie Gangai, Ying Liu, Dimitry Zamarin, Kara L. Roche, Sohrab P. Shah, Yulia Lakhman, Harini Veeraraghavan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two self-supervised pretrained transformer-based segmentation models (SMIT
and Swin UNETR) fine-tuned on a dataset of ovarian cancer CT images provided
reasonably accurate delineations of the tumors in an independent test dataset.
Tumors in the adnexa were segmented more accurately by both transformers (SMIT
and Swin UNETR) than the omental implants. AI-assisted labeling performed on 72
out of 245 omental implants resulted in smaller manual editing effort of 39.55
mm compared to full manual correction of partial labels of 106.49 mm and
resulted in overall improved accuracy performance. Both SMIT and Swin UNETR did
not generate any false detection of omental metastases in the urinary bladder
and relatively few false detections in the small bowel, with 2.16 cc on average
for SMIT and 7.37 cc for Swin UNETR respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedded event based object detection with spiking neural network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Courtois, Pierre-Emmanuel Novac, Edgar Lemaire, Alain Pegatoquet, Benoit Miramond
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complexity of event-based object detection (OD) poses considerable
challenges. Spiking Neural Networks (SNNs) show promising results and pave the
way for efficient event-based OD. Despite this success, the path to efficient
SNNs on embedded devices remains a challenge. This is due to the size of the
networks required to accomplish the task and the ability of devices to take
advantage of SNNs benefits. Even when "edge" devices are considered, they
typically use embedded GPUs that consume tens of watts. In response to these
challenges, our research introduces an embedded neuromorphic testbench that
utilizes the SPiking Low-power Event-based ArchiTecture (SPLEAT) accelerator.
Using an extended version of the Qualia framework, we can train, evaluate,
quantize, and deploy spiking neural networks on an FPGA implementation of
SPLEAT. We used this testbench to load a state-of-the-art SNN solution,
estimate the performance loss associated with deploying the network on
dedicated hardware, and run real-world event-based OD on neuromorphic hardware
specifically designed for low-power spiking neural networks. Remarkably, our
embedded spiking solution, which includes a model with 1.08 million parameters,
operates efficiently with 490 mJ per prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Result link: https://youtu.be/TsolUDaMY7Y</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse-view Signal-domain Photoacoustic Tomography Reconstruction Method
  Based on Neural Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowei Yao, Yi Zeng, Haizhao Dai, Qing Wu, Youshen Xiao, Fei Gao, Yuyao Zhang, Jingyi Yu, Xiran Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photoacoustic tomography is a hybrid biomedical technology, which combines
the advantages of acoustic and optical imaging. However, for the conventional
image reconstruction method, the image quality is affected obviously by
artifacts under the condition of sparse sampling. in this paper, a novel
model-based sparse reconstruction method via implicit neural representation was
proposed for improving the image quality reconstructed from sparse data.
Specially, the initial acoustic pressure distribution was modeled as a
continuous function of spatial coordinates, and parameterized by a multi-layer
perceptron. The weights of multi-layer perceptron were determined by training
the network in self-supervised manner. And the total variation regularization
term was used to offer the prior knowledge. We compared our result with some
ablation studies, and the results show that out method outperforms existing
methods on simulation and experimental data. Under the sparse sampling
condition, our method can suppress the artifacts and avoid the ill-posed
problem effectively, which reconstruct images with higher signal-to-noise ratio
and contrast-to-noise ratio than traditional methods. The high-quality results
for sparse data make the proposed method hold the potential for further
decreasing the hardware cost of photoacoustic tomography system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Cell Detection in Anterior Segment Optical Coherence
  Tomography Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyu Chen, Ameenat L. Solebo, Paul Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anterior uveitis, a common form of eye inflammation, can lead to permanent
vision loss if not promptly diagnosed. Monitoring this condition involves
quantifying inflammatory cells in the anterior chamber (AC) of the eye, which
can be captured using Anterior Segment Optical Coherence Tomography (AS-OCT).
However, manually identifying cells in AS-OCT images is time-consuming and
subjective. Moreover, existing automated approaches may have limitations in
both the effectiveness of detecting cells and the reliability of their
detection results. To address these challenges, we propose an automated
framework to detect cells in the AS-OCT images. This framework consists of a
zero-shot chamber segmentation module and a cell detection module. The first
module segments the AC area in the image without requiring human-annotated
training data. Subsequently, the second module identifies individual cells
within the segmented AC region. Through experiments, our framework demonstrates
superior performance compared to current state-of-the-art methods for both AC
segmentation and cell detection tasks. Notably, we find that previous cell
detection approaches could suffer from low recall, potentially overlooking a
significant number of cells. In contrast, our framework offers an improved
solution, which could benefit the diagnosis and study of anterior uveitis. Our
code for cell detection is publicly available at:
https://github.com/joeybyc/cell_detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRIP: Trainable Region-of-Interest Prediction for Hardware-<span class="highlight-title">Efficient</span>
  Neuromorphic Processing on Event-based Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cina Arjmand, Yingfu Xu, Kevin Shidqi, Alexandra F. Dobrita, Kanishkan Vadivel, Paul Detterer, Manolis Sifalakis, Amirreza Yousefzadeh, Guangzhi Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuromorphic processors are well-suited for efficiently handling sparse
events from event-based cameras. However, they face significant challenges in
the growth of computing demand and hardware costs as the input resolution
increases. This paper proposes the Trainable Region-of-Interest Prediction
(TRIP), the first hardware-efficient hard attention framework for event-based
vision processing on a neuromorphic processor. Our TRIP framework actively
produces low-resolution Region-of-Interest (ROIs) for efficient and accurate
classification. The framework exploits sparse events' inherent low information
density to reduce the overhead of ROI prediction. We introduced extensive
hardware-aware optimizations for TRIP and implemented the hardware-optimized
algorithm on the SENECA neuromorphic processor. We utilized multiple
event-based classification datasets for evaluation. Our approach achieves
state-of-the-art accuracies in all datasets and produces reasonable ROIs with
varying locations and sizes. On the DvsGesture dataset, our solution requires
46x less computation than the state-of-the-art while achieving higher accuracy.
Furthermore, TRIP enables more than 2x latency and energy improvements on the
SENECA neuromorphic processor compared to the conventional solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICONS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UHD-IQA Benchmark Database: Pushing the Boundaries of <span class="highlight-title">Blind</span> Photo
  Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vlad Hosu, Lorenzo Agnolucci, Oliver Wiedemann, Daisuke Iso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel Image Quality Assessment (IQA) dataset comprising 6073
UHD-1 (4K) images, annotated at a fixed width of 3840 pixels. Contrary to
existing No-Reference (NR) IQA datasets, ours focuses on highly aesthetic
photos of high technical quality, filling a gap in the literature. The images,
carefully curated to exclude synthetic content, are sufficiently diverse to
train general NR-IQA models. The dataset is annotated with perceptual quality
ratings obtained through a crowdsourcing study. Ten expert raters, comprising
photographers and graphics artists, assessed each image at least twice in
multiple sessions spanning several days, resulting in highly reliable labels.
Annotators were rigorously selected based on several metrics, including
self-consistency, to ensure their reliability. The dataset includes rich
metadata with user and machine-generated tags from over 5,000 categories and
popularity indicators such as favorites, likes, downloads, and views. With its
unique characteristics, such as its focus on high-quality images, reliable
crowdsourced annotations, and high annotation resolution, our dataset opens up
new opportunities for advancing perceptual image quality assessment research
and developing practical NR-IQA models that apply to modern photos. Our dataset
is available at https://database.mmsp-kn.de/uhd-iqa-benchmark-database.html
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medical Image Segmentation Using Directional Window <span class="highlight-title">Attention</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniya Najiha Abdul Kareem, Mustansar Fiaz, Noa Novershtern, Hisham Cholakkal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of medical images is crucial for diagnostic purposes,
including cell segmentation, tumor identification, and organ localization.
Traditional convolutional neural network (CNN)-based approaches struggled to
achieve precise segmentation results due to their limited receptive fields,
particularly in cases involving multi-organ segmentation with varying shapes
and sizes. The transformer-based approaches address this limitation by
leveraging the global receptive field, but they often face challenges in
capturing local information required for pixel-precise segmentation. In this
work, we introduce DwinFormer, a hierarchical encoder-decoder architecture for
medical image segmentation comprising a directional window (Dwin) attention and
global self-attention (GSA) for feature encoding. The focus of our design is
the introduction of Dwin block within DwinFormer that effectively captures
local and global information along the horizontal, vertical, and depthwise
directions of the input feature map by separately performing attention in each
of these directional volumes. To this end, our Dwin block introduces a nested
Dwin attention (NDA) that progressively increases the receptive field in
horizontal, vertical, and depthwise directions and a convolutional Dwin
attention (CDA) that captures local contextual information for the attention
computation. While the proposed Dwin block captures local and global
dependencies at the first two high-resolution stages of DwinFormer, the GSA
block encodes global dependencies at the last two lower-resolution stages.
Experiments over the challenging 3D Synapse Multi-organ dataset and Cell HMS
dataset demonstrate the benefits of our DwinFormer over the state-of-the-art
approaches. Our source code will be publicly available at
\url{https://github.com/Daniyanaj/DWINFORMER}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning-based brain segmentation model performance validation with
  clinical radiotherapy CT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Selena Huisman, Matteo Maspero, Marielle Philippens, Joost Verhoeff, Szabolcs David
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manual segmentation of medical images is labor intensive and especially
challenging for images with poor contrast or resolution. The presence of
disease exacerbates this further, increasing the need for an automated
solution. To this extent, SynthSeg is a robust deep learning model designed for
automatic brain segmentation across various contrasts and resolutions. This
study validates the SynthSeg robust brain segmentation model on computed
tomography (CT), using a multi-center dataset. An open access dataset of 260
paired CT and magnetic resonance imaging (MRI) from radiotherapy patients
treated in 5 centers was collected. Brain segmentations from CT and MRI were
obtained with SynthSeg model, a component of the Freesurfer imaging suite.
These segmentations were compared and evaluated using Dice scores and Hausdorff
95 distance (HD95), treating MRI-based segmentations as the ground truth. Brain
regions that failed to meet performance criteria were excluded based on
automated quality control (QC) scores. Dice scores indicate a median overlap of
0.76 (IQR: 0.65-0.83). The median HD95 is 2.95 mm (IQR: 1.73-5.39). QC score
based thresholding improves median dice by 0.1 and median HD95 by 0.05mm.
Morphological differences related to sex and age, as detected by MRI, were also
replicated with CT, with an approximate 17% difference between the CT and MRI
results for sex and 10% difference between the results for age. SynthSeg can be
utilized for CT-based automatic brain segmentation, but only in applications
where precision is not essential. CT performance is lower than MRI based on the
integrated QC scores, but low-quality segmentations can be excluded with
QC-based thresholding. Additionally, performing CT-based neuroanatomical
studies is encouraged, as the results show correlations in sex- and age-based
analyses similar to those found with MRI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures, 3 supplementary data csv's, 1 supplementary file
  with 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robustly Optimized Deep Feature Decoupling Network for Fatty Liver
  Diseases Detection <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Huang, Shu Hu, Bo Peng, Jiashu Zhang, Xi Wu, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current medical image classification efforts mainly aim for higher average
performance, often neglecting the balance between different classes. This can
lead to significant differences in recognition accuracy between classes and
obvious recognition weaknesses. Without the support of massive data, deep
learning faces challenges in fine-grained classification of fatty liver. In
this paper, we propose an innovative deep learning framework that combines
feature decoupling and adaptive adversarial training. Firstly, we employ two
iteratively compressed decouplers to supervised decouple common features and
specific features related to fatty liver in abdominal ultrasound images.
Subsequently, the decoupled features are concatenated with the original image
after transforming the color space and are fed into the classifier. During
adversarial training, we adaptively adjust the perturbation and balance the
adversarial strength by the accuracy of each class. The model will eliminate
recognition weaknesses by correctly classifying adversarial samples, thus
improving recognition robustness. Finally, the accuracy of our method improved
by 4.16%, achieving 82.95%. As demonstrated by extensive experiments, our
method is a generalized learning framework that can be directly used to
eliminate the recognition weaknesses of any classifier while improving its
average performance. Code is available at https://github.com/HP-ML/MICCAI2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HD snapshot diffractive spectral imaging and inferencing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apratim Majumder, Monjurul Meem, Fernando Gonzalez del Cueto, Fernando Guevara-Vasquez, Syed N. Qadri, Freddie Santiago, Rajesh Menon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel high-definition (HD) snapshot diffractive spectral imaging
system utilizing a diffractive filter array (DFA) to capture a single image
that encodes both spatial and spectral information. This single diffractogram
can be computationally reconstructed into a spectral image cube, providing a
high-resolution representation of the scene across 25 spectral channels in the
440-800 nm range at 1304x744 spatial pixels (~1 MP). This unique approach
offers numerous advantages including snapshot capture, a form of optical
compression, flexible offline reconstruction, the ability to select the
spectral basis after capture, and high light throughput due to the absence of
lossy filters. We demonstrate a 30-50 nm spectral resolution and compared our
reconstructed spectra against ground truth obtained by conventional
spectrometers. Proof-of-concept experiments in diverse applications including
biological tissue classification, food quality assessment, and simulated
stellar photometry validate our system's capability to perform robust and
accurate inference. These results establish the DFA-based imaging system as a
versatile and powerful tool for advancing scientific and industrial imaging
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A benchmark for 2D foetal brain ultrasound analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariano Cabezas, Yago Diez, Clara Martinez-Diago, Anna Maroto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain development involves a sequence of structural changes from early stages
of the embryo until several months after birth. Currently, ultrasound is the
established technique for screening due to its ability to acquire dynamic
images in real-time without radiation and to its cost-efficiency. However,
identifying abnormalities remains challenging due to the difficulty in
interpreting foetal brain images. In this work we present a set of 104 2D
foetal brain ultrasound images acquired during the 20th week of gestation that
have been co-registered to a common space from a rough skull segmentation. The
images are provided both on the original space and template space centred on
the ellipses of all the subjects. Furthermore, the images have been annotated
to highlight landmark points from structures of interest to analyse brain
development. Both the final atlas template with probabilistic maps and the
original images can be used to develop new segmentation techniques, test
registration approaches for foetal brain ultrasound, extend our work to
longitudinal datasets and to detect anomalies in new images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expansive Synthesis: Generating Large-Scale Datasets from Minimal
  Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vahid Jebraeeli, Bo Jiang, Hamid Krim, Derya Cansever
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenge of limited availability of data for training in machine
learning arises in many applications and the impact on performance and
generalization is serious. Traditional data augmentation methods aim to enhance
training with a moderately sufficient data set. Generative models like
Generative Adversarial Networks (GANs) often face problematic convergence when
generating significant and diverse data samples. Diffusion models, though
effective, still struggle with high computational cost and long training times.
This paper introduces an innovative Expansive Synthesis model that generates
large-scale, high-fidelity datasets from minimal samples. The proposed approach
exploits expander graph mappings and feature interpolation to synthesize
expanded datasets while preserving the intrinsic data distribution and feature
structural relationships. The rationale of the model is rooted in the
non-linear property of neural networks' latent space and in its capture by a
Koopman operator to yield a linear space of features to facilitate the
construction of larger and enriched consistent datasets starting with a much
smaller dataset. This process is optimized by an autoencoder architecture
enhanced with self-attention layers and further refined for distributional
consistency by optimal transport. We validate our Expansive Synthesis by
training classifiers on the generated datasets and comparing their performance
to classifiers trained on larger, original datasets. Experimental results
demonstrate that classifiers trained on synthesized data achieve performance
metrics on par with those trained on full-scale datasets, showcasing the
model's potential to effectively augment training data. This work represents a
significant advancement in data generation, offering a robust solution to data
scarcity and paving the way for enhanced data availability in machine learning
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages. arXiv admin note: text overlap with arXiv:2405.13866</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Cross-Task Interaction for Survival Analysis in Whole Slide
  Pathological Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songhan Jiang, Zhengyu Gan, Linghan Cai, Yifeng Wang, Yongbing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Survival prediction, utilizing pathological images and genomic profiles, is
increasingly important in cancer analysis and prognosis. Despite significant
progress, precise survival analysis still faces two main challenges: (1) The
massive pixels contained in whole slide images (WSIs) complicate the process of
pathological images, making it difficult to generate an effective
representation of the tumor microenvironment (TME). (2) Existing multimodal
methods often rely on alignment strategies to integrate complementary
information, which may lead to information loss due to the inherent
heterogeneity between pathology and genes. In this paper, we propose a
Multimodal Cross-Task Interaction (MCTI) framework to explore the intrinsic
correlations between subtype classification and survival analysis tasks.
Specifically, to capture TME-related features in WSIs, we leverage the subtype
classification task to mine tumor regions. Simultaneously, multi-head attention
mechanisms are applied in genomic feature extraction, adaptively performing
genes grouping to obtain task-related genomic embedding. With the joint
representation of pathological images and genomic data, we further introduce a
Transport-Guided Attention (TGA) module that uses optimal transport theory to
model the correlation between subtype classification and survival analysis
tasks, effectively transferring potential information. Extensive experiments
demonstrate the superiority of our approaches, with MCTI outperforming
state-of-the-art frameworks on three public benchmarks.
\href{https://github.com/jsh0792/MCTI}{https://github.com/jsh0792/MCTI}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Point Spread Function Invertibility Assessment for Image
  Deconvolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romario Gualdrón-Hurtado, Roman Jacome, Sergio Urrea, Henry Arguello, Luis Gonzalez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep-learning (DL)-based image deconvolution (ID) has exhibited remarkable
recovery performance, surpassing traditional linear methods. However, unlike
traditional ID approaches that rely on analytical properties of the point
spread function (PSF) to achieve high recovery performance - such as specific
spectrum properties or small conditional numbers in the convolution matrix - DL
techniques lack quantifiable metrics for evaluating PSF suitability for
DL-assisted recovery. Aiming to enhance deconvolution quality, we propose a
metric that employs a non-linear approach to learn the invertibility of an
arbitrary PSF using a neural network by mapping it to a unit impulse. A lower
discrepancy between the mapped PSF and a unit impulse indicates a higher
likelihood of successful inversion by a DL network. Our findings reveal that
this metric correlates with high recovery performance in DL and traditional
methods, thereby serving as an effective regularizer in deconvolution tasks.
This approach reduces the computational complexity over conventional condition
number assessments and is a differentiable process. These useful properties
allow its application in designing diffractive optical elements through
end-to-end (E2E) optimization, achieving invertible PSFs, and outperforming the
E2E baseline framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Mamba</span>MIR: An Arbitrary-Masked <span class="highlight-title">Mamba</span> for Joint Medical Image
  Reconstruction and Uncertainty Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Huang, Liutao Yang, Fanwen Wang, Yang Nan, Angelica I. Aviles-Rivero, Carola-Bibiane Schönlieb, Daoqiang Zhang, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent Mamba model has shown remarkable adaptability for visual
representation learning, including in medical imaging tasks. This study
introduces MambaMIR, a Mamba-based model for medical image reconstruction, as
well as its Generative Adversarial Network-based variant, MambaMIR-GAN. Our
proposed MambaMIR inherits several advantages, such as linear complexity,
global receptive fields, and dynamic weights, from the original Mamba model.
The innovated arbitrary-mask mechanism effectively adapt Mamba to our image
reconstruction task, providing randomness for subsequent Monte Carlo-based
uncertainty estimation. Experiments conducted on various medical image
reconstruction tasks, including fast MRI and SVCT, which cover anatomical
regions such as the knee, chest, and abdomen, have demonstrated that MambaMIR
and MambaMIR-GAN achieve comparable or superior reconstruction results relative
to state-of-the-art methods. Additionally, the estimated uncertainty maps offer
further insights into the reliability of the reconstruction quality. The code
is publicly available at https://github.com/ayanglab/MambaMIR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Global Sensitivity and Uncertainty Quantification in Medical
  Image Reconstruction with Monte Carlo Arbitrary-Masked <span class="highlight-title">Mamba</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Huang, Liutao Yang, Fanwen Wang, Yang Nan, Weiwen Wu, Chengyan Wang, Kuangyu Shi, Angelica I. Aviles-Rivero, Carola-Bibiane Schönlieb, Daoqiang Zhang, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has been extensively applied in medical image reconstruction,
where Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs)
represent the predominant paradigms, each possessing distinct advantages and
inherent limitations: CNNs exhibit linear complexity with local sensitivity,
whereas ViTs demonstrate quadratic complexity with global sensitivity. The
emerging Mamba has shown superiority in learning visual representation, which
combines the advantages of linear scalability and global sensitivity. In this
study, we introduce MambaMIR, an Arbitrary-Masked Mamba-based model with
wavelet decomposition for joint medical image reconstruction and uncertainty
estimation. A novel Arbitrary Scan Masking (ASM) mechanism "masks out"
redundant information to introduce randomness for further uncertainty
estimation. Compared to the commonly used Monte Carlo (MC) dropout, our
proposed MC-ASM provides an uncertainty map without the need for hyperparameter
tuning and mitigates the performance drop typically observed when applying
dropout to low-level tasks. For further texture preservation and better
perceptual quality, we employ the wavelet transformation into MambaMIR and
explore its variant based on the Generative Adversarial Network, namely
MambaMIR-GAN. Comprehensive experiments have been conducted for multiple
representative medical image reconstruction tasks, demonstrating that the
proposed MambaMIR and MambaMIR-GAN outperform other baseline and
state-of-the-art methods in different reconstruction tasks, where MambaMIR
achieves the best reconstruction fidelity and MambaMIR-GAN has the best
perceptual quality. In addition, our MC-ASM provides uncertainty maps as an
additional tool for clinicians, while mitigating the typical performance drop
caused by the commonly used dropout.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instance-level quantitative saliency in multiple sclerosis lesion
  segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Spagnolo, Nataliia Molchanova, Roger Schaer, Meritxell Bach Cuadra, Mario Ocampo Pineda, Lester Melie-Garcia, Cristina Granziera, Vincent Andrearczyk, Adrien Depeursinge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, explainable methods for artificial intelligence (XAI) have
tried to reveal and describe models' decision mechanisms in the case of
classification tasks. However, XAI for semantic segmentation and in particular
for single instances has been little studied to date. Understanding the process
underlying automatic segmentation of single instances is crucial to reveal what
information was used to detect and segment a given object of interest. In this
study, we proposed two instance-level explanation maps for semantic
segmentation based on SmoothGrad and Grad-CAM++ methods. Then, we investigated
their relevance for the detection and segmentation of white matter lesions
(WML), a magnetic resonance imaging (MRI) biomarker in multiple sclerosis (MS).
687 patients diagnosed with MS for a total of 4043 FLAIR and MPRAGE MRI scans
were collected at the University Hospital of Basel, Switzerland. Data were
randomly split into training, validation and test sets to train a 3D U-Net for
MS lesion segmentation. We observed 3050 true positive (TP), 1818 false
positive (FP), and 789 false negative (FN) cases. We generated instance-level
explanation maps for semantic segmentation, by developing two XAI methods based
on SmoothGrad and Grad-CAM++. We investigated: 1) the distribution of gradients
in saliency maps with respect to both input MRI sequences; 2) the model's
response in the case of synthetic lesions; 3) the amount of perilesional tissue
needed by the model to segment a lesion. Saliency maps (based on SmoothGrad) in
FLAIR showed positive values inside a lesion and negative in its neighborhood.
Peak values of saliency maps generated for these four groups of volumes
presented distributions that differ significantly from one another, suggesting
a quantitative nature of the proposed saliency. Contextual information of 7mm
around the lesion border was required for their segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Ninth NTIRE 2024 <span class="highlight-title">Efficient</span> <span class="highlight-title">Super-Resolution</span> Challenge Report <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Ren, Yawei Li, Nancy Mehta, Radu Timofte, Hongyuan Yu, Cheng Wan, Yuxin Hong, Bingnan Han, Zhuoyuan Wu, Yajun Zou, Yuqing Liu, Jizhe Li, Keji He, Chao Fan, Heng Zhang, Xiaolin Zhang, Xuanwu Yin, Kunlong Zuo, Bohao Liao, Peizhe Xia, Long Peng, Zhibo Du, Xin Di, Wangkai Li, Yang Wang, Wei Zhai, Renjing Pei, Jiaming Guo, Songcen Xu, Yang Cao, Zhengjun Zha, Yan Wang, Yi Liu, Qing Wang, Gang Zhang, Liou Zhang, Shijie Zhao, Long Sun, Jinshan Pan, Jiangxin Dong, Jinhui Tang, Xin Liu, Min Yan, Qian Wang, Menghan Zhou, Yiqiang Yan, Yixuan Liu, Wensong Chan, Dehua Tang, Dong Zhou, Li Wang, Lu Tian, Barsoum Emad, Bohan Jia, Junbo Qiao, Yunshuai Zhou, Yun Zhang, Wei Li, Shaohui Lin, Shenglong Zhou, Binbin Chen, Jincheng Liao, Suiyi Zhao, Zhao Zhang, Bo Wang, Yan Luo, Yanyan Wei, Feng Li, Mingshen Wang, Yawei Li, Jinhan Guan, Dehua Hu, Jiawei Yu, Qisheng Xu, Tao Sun, Long Lan, Kele Xu, Xin Lin, Jingtong Yue, Lehan Yang, Shiyi Du, Lu Qi, Chao Ren, Zeyu Han, Yuhan Wang, Chaolin Chen, Haobo Li, Mingjun Zheng, Zhongbao Yang, Lianhong Song, Xingzhuo Yan, Minghan Fu, Jingyi Zhang, Baiang Li, Qi Zhu, Xiaogang Xu, Dan Guo, Chunle Guo, Jiadi Chen, Huanhuan Long, Chunjiang Duanmu, Xiaoyan Lei, Jie Liu, Weilin Jia, Weifeng Cao, Wenlong Zhang, Yanyu Mao, Ruilong Guo, Nihao Zhang, Qian Wang, Manoj Pandey, Maksym Chernozhukov, Giang Le, Shuli Cheng, Hongyuan Wang, Ziyan Wei, Qingting Tang, Liejun Wang, Yongming Li, Yanhui Guo, Hao Xu, Akram Khatami-Rizi, Ahmad Mahmoudi-Aznaveh, Chih-Chung Hsu, Chia-Ming Lee, Yi-Shiuan Chou, Amogh Joshi, Nikhil Akalwadi, Sampada Malagi, Palani Yashaswini, Chaitra Desai, Ramesh Ashok Tabib, Ujwala Patil, Uma Mudenagudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a comprehensive review of the NTIRE 2024 challenge,
focusing on efficient single-image super-resolution (ESR) solutions and their
outcomes. The task of this challenge is to super-resolve an input image with a
magnification factor of x4 based on pairs of low and corresponding
high-resolution images. The primary objective is to develop networks that
optimize various aspects such as runtime, parameters, and FLOPs, while still
maintaining a peak signal-to-noise ratio (PSNR) of approximately 26.90 dB on
the DIV2K_LSDIR_valid dataset and 26.99 dB on the DIV2K_LSDIR_test dataset. In
addition, this challenge has 4 tracks including the main track (overall
performance), sub-track 1 (runtime), sub-track 2 (FLOPs), and sub-track 3
(parameters). In the main track, all three metrics (ie runtime, FLOPs, and
parameter count) were considered. The ranking of the main track is calculated
based on a weighted sum-up of the scores of all other sub-tracks. In sub-track
1, the practical runtime performance of the submissions was evaluated, and the
corresponding score was used to determine the ranking. In sub-track 2, the
number of FLOPs was considered. The score calculated based on the corresponding
FLOPs was used to determine the ranking. In sub-track 3, the number of
parameters was considered. The score calculated based on the corresponding
parameters was used to determine the ranking. RLFN is set as the baseline for
efficiency measurement. The challenge had 262 registered participants, and 34
teams made valid submissions. They gauge the state-of-the-art in efficient
single-image super-resolution. To facilitate the reproducibility of the
challenge and enable other researchers to build upon these findings, the code
and the pre-trained model of validated solutions are made publicly available at
https://github.com/Amazingren/NTIRE2024_ESR/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The report paper of NTIRE2024 Efficient Super-resolution, accepted by
  CVPRW2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing dermatological diagnosis: Development of a hyperspectral
  dermatoscope for enhanced skin imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin J. Hetz, Carina Nogueira Garcia, Sarah Haggenmüller, Titus J. Brinker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical dermatology necessitates precision and innovation for efficient
diagnosis and treatment of various skin conditions. This paper introduces the
development of a cutting-edge hyperspectral dermatoscope (the Hyperscope)
tailored for human skin analysis. We detail the requirements to such a device
and the design considerations, from optical configurations to sensor selection,
necessary to capture a wide spectral range with high fidelity. Preliminary
results from 15 individuals and 160 recorded skin images demonstrate the
potential of the Hyperscope in identifying and characterizing various skin
conditions, offering a promising avenue for non-invasive skin evaluation and a
platform for future research in dermatology-related hyperspectral imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 11 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sampling Strategies in Bayesian Inversion: A Study of RTO and Langevin
  Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16658v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16658v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Remi Laumont, Yiqiu Dong, Martin Skovgaard Andersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies two classes of sampling methods for the solution of
inverse problems, namely Randomize-Then-Optimize (RTO), which is rooted in
sensitivity analysis, and Langevin methods, which are rooted in the Bayesian
framework. The two classes of methods correspond to different assumptions and
yield samples from different target distributions. We highlight the main
conceptual and theoretical differences between the two approaches and compare
them from a practical point of view by tackling two classical inverse problems
in imaging: deblurring and inpainting. We show that the choice of the sampling
method has a significant impact on the quality of the reconstruction and that
the RTO method is more robust to the choice of the parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Image Prior for Unsupervised Dynamic Cardiac Cine MRI
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongsen Li, Wenxuan Chen, Shuai Wang, Chuyu Liu, Qing Zou, Rui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inductive bias of the convolutional neural network (CNN) can be a strong
prior for image restoration, which is known as the Deep Image Prior (DIP).
Recently, DIP is utilized in unsupervised dynamic MRI reconstruction, which
adopts a generative model from the latent space to the image space. However,
existing methods usually use a pyramid-shaped CNN generator shared by all
frames, embedding the temporal modeling within the latent space, which may
hamper the model expression capability. In this work, we propose a novel scheme
for dynamic MRI representation, named ``Graph Image Prior'' (GIP). GIP adopts a
two-stage generative network in a new modeling methodology, which first employs
independent CNNs to recover the image structure for each frame, and then
exploits the spatio-temporal correlations within the feature space
parameterized by a graph model. A graph convolutional network is utilized for
feature fusion and dynamic image generation. In addition, we devise an ADMM
algorithm to alternately optimize the images and the network parameters to
improve the reconstruction performance. Experiments were conducted on cardiac
cine MRI reconstruction, which demonstrate that GIP outperforms compressed
sensing methods and other DIP-based unsupervised methods, significantly
reducing the performance gap with state-of-the-art supervised algorithms.
Moreover, GIP displays superior generalization ability when transferred to a
different reconstruction setting, without the need for any additional data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-ray2CTPA: Generating 3D CTPA scans from 2D X-ray conditioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noa Cahan, Eyal Klang, Galit Aviram, Yiftach Barash, Eli Konen, Raja Giryes, Hayit Greenspan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest X-rays or chest radiography (CXR), commonly used for medical
diagnostics, typically enables limited imaging compared to computed tomography
(CT) scans, which offer more detailed and accurate three-dimensional data,
particularly contrast-enhanced scans like CT Pulmonary Angiography (CTPA).
However, CT scans entail higher costs, greater radiation exposure, and are less
accessible than CXRs. In this work we explore cross-modal translation from a 2D
low contrast-resolution X-ray input to a 3D high contrast and
spatial-resolution CTPA scan. Driven by recent advances in generative AI, we
introduce a novel diffusion-based approach to this task. We evaluate the models
performance using both quantitative metrics and qualitative feedback from
radiologists, ensuring diagnostic relevance of the generated images.
Furthermore, we employ the synthesized 3D images in a classification framework
and show improved AUC in a PE categorization task, using the initial CXR input.
The proposed method is generalizable and capable of performing additional
cross-modality translations in medical imaging. It may pave the way for more
accessible and cost-effective advanced diagnostic tools. The code for this
project is available: https://github.com/NoaCahan/X-ray2CTPA .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, project code: https://github.com/NoaCahan/X-ray2CTPA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CEST-KAN: Kolmogorov-Arnold Networks for CEST MRI Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawen Wang, Pei Cai, Ziyan Wang, Huabin Zhang, Jianpan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: This study aims to propose and investigate the feasibility of using
Kolmogorov-Arnold Network (KAN) for CEST MRI data analysis (CEST-KAN). Methods:
CEST MRI data were acquired from twelve healthy volunteers at 3T. Data from ten
subjects were used for training, while the remaining two were reserved for
testing. The performance of multi-layer perceptron (MLP) and KAN models with
the same network settings were evaluated and compared to the conventional
multi-pool Lorentzian fitting (MPLF) method in generating water and multiple
CEST contrasts, including amide, relayed nuclear Overhauser effect (rNOE), and
magnetization transfer (MT). Results: The water and CEST maps generated by both
MLP and KAN were visually comparable to the MPLF results. However, the KAN
model demonstrated higher accuracy in extrapolating the CEST fitting metrics,
as evidenced by the smaller validation loss during training and smaller
absolute error during testing. Voxel-wise correlation analysis showed that all
four CEST fitting metrics generated by KAN consistently exhibited higher
Pearson coefficients than the MLP results, indicating superior performance.
Moreover, the KAN models consistently outperformed the MLP models in varying
hidden layer numbers despite longer training time. Conclusion: In this study,
we demonstrated for the first time the feasibility of utilizing KAN for CEST
MRI data analysis, highlighting its superiority over MLP in this task. The
findings suggest that CEST-KAN has the potential to be a robust and reliable
post-analysis tool for CEST MRI in clinical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rapid and Accurate Diagnosis of Acute Aortic Syndrome using Non-contrast
  CT: A Large-scale, Retrospective, Multi-center and AI-based Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujian Hu, Yilang Xiang, Yan-Jie Zhou, Yangyan He, Shifeng Yang, Xiaolong Du, Chunlan Den, Youyao Xu, Gaofeng Wang, Zhengyao Ding, Jingyong Huang, Wenjun Zhao, Xuejun Wu, Donglin Li, Qianqian Zhu, Zhenjiang Li, Chenyang Qiu, Ziheng Wu, Yunjun He, Chen Tian, Yihui Qiu, Zuodong Lin, Xiaolong Zhang, Yuan He, Zhenpeng Yuan, Xiaoxiang Zhou, Rong Fan, Ruihan Chen, Wenchao Guo, Jianpeng Zhang, Tony C. W. Mok, Zi Li, Le Lu, Dehai Lang, Xiaoqiang Li, Guofu Wang, Wei Lu, Zhengxing Huang, Minfeng Xu, Hongkun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest pain symptoms are highly prevalent in emergency departments (EDs),
where acute aortic syndrome (AAS) is a catastrophic cardiovascular emergency
with a high fatality rate, especially when timely and accurate treatment is not
administered. However, current triage practices in the ED can cause up to
approximately half of patients with AAS to have an initially missed diagnosis
or be misdiagnosed as having other acute chest pain conditions. Subsequently,
these AAS patients will undergo clinically inaccurate or suboptimal
differential diagnosis. Fortunately, even under these suboptimal protocols,
nearly all these patients underwent non-contrast CT covering the aorta anatomy
at the early stage of differential diagnosis. In this study, we developed an
artificial intelligence model (DeepAAS) using non-contrast CT, which is highly
accurate for identifying AAS and provides interpretable results to assist in
clinical decision-making. Performance was assessed in two major phases: a
multi-center retrospective study (n = 20,750) and an exploration in real-world
emergency scenarios (n = 137,525). In the multi-center cohort, DeepAAS achieved
a mean area under the receiver operating characteristic curve of 0.958 (95% CI
0.950-0.967). In the real-world cohort, DeepAAS detected 109 AAS patients with
misguided initial suspicion, achieving 92.6% (95% CI 76.2%-97.5%) in mean
sensitivity and 99.2% (95% CI 99.1%-99.3%) in mean specificity. Our AI model
performed well on non-contrast CT at all applicable early stages of
differential diagnosis workflows, effectively reduced the overall missed
diagnosis and misdiagnosis rate from 48.8% to 4.8% and shortened the diagnosis
time for patients with misguided initial suspicion from an average of 681.8
(74-11,820) mins to 68.5 (23-195) mins. DeepAAS could effectively fill the gap
in the current clinical workflow without requiring additional tests.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under peer review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ECLIPSE: Expunging Clean-label Indiscriminate Poisons via Sparse
  <span class="highlight-title">Diffusion</span> Purification <span class="chip">ESORICS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianlong Wang, Shengshan Hu, Yechao Zhang, Ziqi Zhou, Leo Yu Zhang, Peng Xu, Wei Wan, Hai Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clean-label indiscriminate poisoning attacks add invisible perturbations to
correctly labeled training images, thus dramatically reducing the
generalization capability of the victim models. Recently, some defense
mechanisms have been proposed such as adversarial training, image
transformation techniques, and image purification. However, these schemes are
either susceptible to adaptive attacks, built on unrealistic assumptions, or
only effective against specific poison types, limiting their universal
applicability. In this research, we propose a more universally effective,
practical, and robust defense scheme called ECLIPSE. We first investigate the
impact of Gaussian noise on the poisons and theoretically prove that any kind
of poison will be largely assimilated when imposing sufficient random noise. In
light of this, we assume the victim has access to an extremely limited number
of clean images (a more practical scene) and subsequently enlarge this sparse
set for training a denoising probabilistic model (a universal denoising tool).
We then begin by introducing Gaussian noise to absorb the poisons and then
apply the model for denoising, resulting in a roughly purified dataset.
Finally, to address the trade-off of the inconsistency in the assimilation
sensitivity of different poisons by Gaussian noise, we propose a lightweight
corruption compensation module to effectively eliminate residual poisons,
providing a more universal defense approach. Extensive experiments demonstrate
that our defense approach outperforms 10 state-of-the-art defenses. We also
propose an adaptive attack against ECLIPSE and verify the robustness of our
defense scheme. Our code is available at https://github.com/CGCL-codes/ECLIPSE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ESORICS 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-24T00:00:00Z">2024-06-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Image and Video Processing <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Aperture Fusion of <span class="highlight-title">Transformer</span>-Convolutional Network (MFTC-Net)
  for 3D Medical Image Segmentation and Visualization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyavash Shabani, Muhammad Sohaib, Sahar A. Mohammed, Bahram Parvin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers have shown superior performance to the traditional
convolutional-based frameworks in many vision applications, including but not
limited to the segmentation of 3D medical images. To further advance this area,
this study introduces the Multi-Aperture Fusion of Transformer-Convolutional
Network (MFTC-Net), which integrates the output of Swin Transformers and their
corresponding convolutional blocks using 3D fusion blocks. The Multi-Aperture
incorporates each image patch at its original resolutions with its pyramid
representation to better preserve minute details. The proposed architecture has
demonstrated a score of 89.73 and 7.31 for Dice and HD95, respectively, on the
Synapse multi-organs dataset an improvement over the published results. The
improved performance also comes with the added benefits of the reduced
complexity of approximately 40 million parameters. Our code is available at
https://github.com/Siyavashshabani/MFTC-Net
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Domain Adaptation for Pediatric Brain Tumor Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingru Fu, Simone Bendazzoli, Örjan Smedby, Rodrigo Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant advances have been made toward building accurate automatic
segmentation models for adult gliomas. However, the performance of these models
often degrades when applied to pediatric glioma due to their imaging and
clinical differences (domain shift). Obtaining sufficient annotated data for
pediatric glioma is typically difficult because of its rare nature. Also,
manual annotations are scarce and expensive. In this work, we propose
Domain-Adapted nnU-Net (DA-nnUNet) to perform unsupervised domain adaptation
from adult glioma (source domain) to pediatric glioma (target domain).
Specifically, we add a domain classifier connected with a gradient reversal
layer (GRL) to a backbone nnU-Net. Once the classifier reaches a very high
accuracy, the GRL is activated with the goal of transferring domain-invariant
features from the classifier to the segmentation model while preserving
segmentation accuracy on the source domain. The accuracy of the classifier
slowly degrades to chance levels. No annotations are used in the target domain.
The method is compared to 8 different supervised models using BraTS-Adult
glioma (N=1251) and BraTS-PED glioma data (N=99). The proposed method shows
notable performance enhancements in the tumor core (TC) region compared to the
model that only uses adult data: ~32% better Dice scores and ~20 better 95th
percentile Hausdorff distances. Moreover, our unsupervised approach shows no
statistically significant difference compared to the practical upper bound
model using manual annotations from both datasets in TC region. The code is
shared at https://github.com/Fjr9516/DA_nnUNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The MRI Scanner as a Diagnostic: Image-less Active Sampling <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuning Du, Rohan Dharmakumar, Sotirios A. Tsaftaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the high diagnostic accuracy of Magnetic Resonance Imaging (MRI),
using MRI as a Point-of-Care (POC) disease identification tool poses
significant accessibility challenges due to the use of high magnetic field
strength and lengthy acquisition times. We ask a simple question: Can we
dynamically optimise acquired samples, at the patient level, according to an
(automated) downstream decision task, while discounting image reconstruction?
We propose an ML-based framework that learns an active sampling strategy, via
reinforcement learning, at a patient-level to directly infer disease from
undersampled k-space. We validate our approach by inferring Meniscus Tear in
undersampled knee MRI data, where we achieve diagnostic performance comparable
with ML-based diagnosis, using fully sampled k-space data. We analyse
task-specific sampling policies, showcasing the adaptability of our active
sampling approach. The introduced frugal sampling strategies have the potential
to reduce high field strength requirements that in turn strengthen the
viability of MRI-based POC disease identification and associated preliminary
screening tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ μ-Net: A Deep Learning-Based Architecture for μ-CT Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierangela Bruno, Edoardo De Rose, Carlo Adornetto, Francesco Calimeri, Sandro Donato, Raffaele Giuseppe Agostino, Daniela Amelio, Riccardo Barberi, Maria Carmela Cerra, Maria Caterina Crocco, Mariacristina Filice, Raffaele Filosa, Gianluigi Greco, Sandra Imbrogno, Vincenzo Formoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  X-ray computed microtomography ({\mu}-CT) is a non-destructive technique that
can generate high-resolution 3D images of the internal anatomy of medical and
biological samples. These images enable clinicians to examine internal anatomy
and gain insights into the disease or anatomical morphology. However,
extracting relevant information from 3D images requires semantic segmentation
of the regions of interest, which is usually done manually and results
time-consuming and tedious. In this work, we propose a novel framework that
uses a convolutional neural network (CNN) to automatically segment the full
morphology of the heart of Carassius auratus. The framework employs an
optimized 2D CNN architecture that can infer a 3D segmentation of the sample,
avoiding the high computational cost of a 3D CNN architecture. We tackle the
challenges of handling large and high-resoluted image data (over a thousand
pixels in each dimension) and a small training database (only three samples) by
proposing a standard protocol for data normalization and processing. Moreover,
we investigate how the noise, contrast, and spatial resolution of the sample
and the training of the architecture are affected by the reconstruction
technique, which depends on the number of input images. Experiments show that
our framework significantly reduces the time required to segment new samples,
allowing a faster microtomography analysis of the Carassius auratus heart
shape. Furthermore, our framework can work with any bio-image (biological and
medical) from {\mu}-CT with high-resolution and small dataset size
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demystifying the Effect of Receptive Field Size in U-Net Models for
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Loos, Rohit Pardasani, Navchetan Awasthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation is a critical task in healthcare applications, and
U-Nets have demonstrated promising results. This work delves into the
understudied aspect of receptive field (RF) size and its impact on the U-Net
and Attention U-Net architectures. This work explores several critical elements
including the relationship between RF size, characteristics of the region of
interest, and model performance, as well as the balance between RF size and
computational costs for U-Net and Attention U-Net methods for different
datasets. This work also proposes a mathematical notation for representing the
theoretical receptive field (TRF) of a given layer in a network and proposes
two new metrics - effective receptive field (ERF) rate and the Object rate to
quantify the fraction of significantly contributing pixels within the ERF
against the TRF area and assessing the relative size of the segmentation object
compared to the TRF size respectively. The results demonstrate that there
exists an optimal TRF size that successfully strikes a balance between
capturing a wider global context and maintaining computational efficiency,
thereby optimizing model performance. Interestingly, a distinct correlation is
observed between the data complexity and the required TRF size; segmentation
based solely on contrast achieved peak performance even with smaller TRF sizes,
whereas more complex segmentation tasks necessitated larger TRFs. Attention
U-Net models consistently outperformed their U-Net counterparts, highlighting
the value of attention mechanisms regardless of TRF size. These novel insights
present an invaluable resource for developing more efficient U-Net-based
architectures for medical imaging and pave the way for future exploration. A
tool is also developed that calculates the TRF for a U-Net (and Attention
U-Net) model, and also suggest an appropriate TRF size for a given model and
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLOctolyzer: Fully automatic analysis toolkit for segmentation and
  feature extracting in scanning laser ophthalmoscopy images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamie Burke, Samuel Gibbon, Justin Engelmann, Adam Threlfall, Ylenia Giarratano, Charlene Hamid, Stuart King, Ian J. C. MacCormick, Tom MacGillivray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To describe SLOctolyzer: an open-source analysis toolkit for en face
retinal vessels appearing in infrared reflectance scanning laser ophthalmoscopy
(SLO) images.
  Methods: SLOctolyzer includes two main modules: segmentation and measurement.
The segmentation module use deep learning methods to delineate retinal anatomy,
while the measurement module quantifies key retinal vascular features such as
vessel complexity, density, tortuosity, and calibre. We evaluate the
segmentation module using unseen data and measure its reproducibility.
  Results: SLOctolyzer's segmentation module performed well against unseen
internal test data (Dice for all-vessels, 0.9097; arteries, 0.8376; veins,
0.8525; optic disc, 0.9430; fovea, 0.8837). External validation against severe
retinal pathology showed decreased performance (Dice for arteries, 0.7180;
veins, 0.7470; optic disc, 0.9032). SLOctolyzer had good reproducibility (mean
difference for fractal dimension, -0.0007; vessel density, -0.0003; vessel
calibre, -0.3154 $\mu$m; tortuosity density, 0.0013). SLOctolyzer can process a
macula-centred SLO image in under 20 seconds and a disc-centred SLO image in
under 30 seconds using a standard laptop CPU.
  Conclusions: To our knowledge, SLOctolyzer is the first open-source tool to
convert raw SLO images into reproducible and clinically meaningful retinal
vascular parameters. SLO images are captured simultaneous to optical coherence
tomography (OCT), and we believe our software will be useful for extracting
retinal vascular measurements from large OCT image sets and linking them to
ocular or systemic diseases. It requires no specialist knowledge or proprietary
software, and allows manual correction of segmentations and re-computing of
vascular metrics. SLOctolyzer is freely available at
https://github.com/jaburke166/SLOctolyzer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 6 tables + Supplementary (7 pages, 10 figures, 4
  tables). Submitted for peer review at Translational Vision Science and
  Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Vision xLSTM Embedded UNet More Reliable in Medical 3D Image
  Segmentation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pallabi Dutta, Soham Bose, Swalpa Kumar Roy, Sushmita Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of developing efficient medical image segmentation has
evolved from initial dependence on Convolutional Neural Networks (CNNs) to the
present investigation of hybrid models that combine CNNs with Vision
Transformers. Furthermore, there is an increasing focus on creating
architectures that are both high-performing in medical image segmentation tasks
and computationally efficient to be deployed on systems with limited resources.
Although transformers have several advantages like capturing global
dependencies in the input data, they face challenges such as high computational
and memory complexity. This paper investigates the integration of CNNs and
Vision Extended Long Short-Term Memory (Vision-xLSTM) models by introducing a
novel approach called UVixLSTM. The Vision-xLSTM blocks captures temporal and
global relationships within the patches extracted from the CNN feature maps.
The convolutional feature reconstruction path upsamples the output volume from
the Vision-xLSTM blocks to produce the segmentation output. Our primary
objective is to propose that Vision-xLSTM forms a reliable backbone for medical
image segmentation tasks, offering excellent segmentation performance and
reduced computational complexity. UVixLSTM exhibits superior performance
compared to state-of-the-art networks on the publicly-available Synapse
dataset. Code is available at: https://github.com/duttapallabi2907/UVixLSTM
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Generative Adversarial Networks for Video <span class="highlight-title">Super-Resolution</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research, we explore different ways to improve generative adversarial
networks for video super-resolution tasks from a base single image
super-resolution GAN model. Our primary objective is to identify potential
techniques that enhance these models and to analyze which of these techniques
yield the most significant improvements. We evaluate our results using Peak
Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). Our
findings indicate that the most effective techniques include temporal
smoothing, long short-term memory (LSTM) layers, and a temporal loss function.
The integration of these methods results in an 11.97% improvement in PSNR and
an 8% improvement in SSIM compared to the baseline video super-resolution
generative adversarial network (GAN) model. This substantial improvement
suggests potential further applications to enhance current state-of-the-art
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximate DCT and Quantization Techniques for Energy-Constrained Image
  Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming-Che Li, Archisman Ghosh, Shreyas Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent expansions in multimedia devices gather enormous amounts of real-time
images for processing and inference. The images are first compressed using
compression schemes, like JPEG, to reduce storage costs and power for
transmitting the captured data. Due to inherent error resilience and
imperceptibility in images, JPEG can be approximated to reduce the required
computation power and area. This work demonstrates the first end-to-end
approximation computing-based optimization of JPEG hardware using i) an
approximate division realized using bit-shift operators to reduce the
complexity of the quantization block, ii) loop perforation, and iii) precision
scaling on top of a multiplier-less fast DCT architecture to achieve an
extremely energy-efficient JPEG compression unit which will be a perfect fit
for power/bandwidth-limited scenario. Furthermore, a gradient descent-based
heuristic composed of two conventional approximation strategies, i.e.,
Precision Scaling and Loop Perforation, is implemented for tuning the degree of
approximation to trade off energy consumption with the quality degradation of
the decoded image. The entire RTL design is coded in Verilog HDL, synthesized,
mapped to TSMC 65nm CMOS technology, and simulated using Cadence Spectre
Simulator under 25$^{\circ}$\textbf{C}, TT corner. The approximate division
approach achieved around $\textbf{28\%}$ reduction in the active design area.
The heuristic-based approximation technique combined with accelerator
optimization achieves a significant energy reduction of $\textbf{36\%}$ for a
minimal image quality degradation of $\textbf{2\%}$ SAD. Simulation results
also show that the proposed architecture consumes 15uW at the DCT and
quantization stages to compress a colored 480p image at 6fps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lesion-Aware Cross-Phase <span class="highlight-title">Attention</span> Network for Renal Tumor Subtype
  Classification on Multi-Phase CT Scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwang-Hyun Uhm, Seung-Won Jung, Sung-Hoo Hong, Sung-Jea Ko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-phase computed tomography (CT) has been widely used for the
preoperative diagnosis of kidney cancer due to its non-invasive nature and
ability to characterize renal lesions. However, since enhancement patterns of
renal lesions across CT phases are different even for the same lesion type, the
visual assessment by radiologists suffers from inter-observer variability in
clinical practice. Although deep learning-based approaches have been recently
explored for differential diagnosis of kidney cancer, they do not explicitly
model the relationships between CT phases in the network design, limiting the
diagnostic performance. In this paper, we propose a novel lesion-aware
cross-phase attention network (LACPANet) that can effectively capture temporal
dependencies of renal lesions across CT phases to accurately classify the
lesions into five major pathological subtypes from time-series multi-phase CT
images. We introduce a 3D inter-phase lesion-aware attention mechanism to learn
effective 3D lesion features that are used to estimate attention weights
describing the inter-phase relations of the enhancement patterns. We also
present a multi-scale attention scheme to capture and aggregate temporal
patterns of lesion features at different spatial scales for further
improvement. Extensive experiments on multi-phase CT scans of kidney cancer
patients from the collected dataset demonstrate that our LACPANet outperforms
state-of-the-art approaches in diagnostic accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been accepted for publication in Computers in
  Biology and Medicine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Priorformer: A UGC-VQA Method with content and distortion priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yajing Pei, Shiyu Huang, Yiting Lu, Xin Li, Zhibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User Generated Content (UGC) videos are susceptible to complicated and
variant degradations and contents, which prevents the existing blind video
quality assessment (BVQA) models from good performance since the lack of the
adapability of distortions and contents. To mitigate this, we propose a novel
prior-augmented perceptual vision transformer (PriorFormer) for the BVQA of
UGC, which boots its adaptability and representation capability for divergent
contents and distortions. Concretely, we introduce two powerful priors, i.e.,
the content and distortion priors, by extracting the content and distortion
embeddings from two pre-trained feature extractors. Then we adopt these two
powerful embeddings as the adaptive prior tokens, which are transferred to the
vision transformer backbone jointly with implicit quality features. Based on
the above strategy, the proposed PriorFormer achieves state-of-the-art
performance on three public UGC VQA datasets including KoNViD-1K, LIVE-VQC and
YouTube-UGC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coordinate-based neural representations for computational adaptive
  optics in widefield microscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03812v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03812v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iksung Kang, Qinrong Zhang, Stella X. Yu, Na Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Widefield microscopy is widely used for non-invasive imaging of biological
structures at subcellular resolution. When applied to complex specimen, its
image quality is degraded by sample-induced optical aberration. Adaptive optics
can correct wavefront distortion and restore diffraction-limited resolution but
require wavefront sensing and corrective devices, increasing system complexity
and cost. Here, we describe a self-supervised machine learning algorithm,
CoCoA, that performs joint wavefront estimation and three-dimensional
structural information extraction from a single input 3D image stack without
the need for external training dataset. We implemented CoCoA for widefield
imaging of mouse brain tissues and validated its performance with
direct-wavefront-sensing-based adaptive optics. Importantly, we systematically
explored and quantitatively characterized the limiting factors of CoCoA's
performance. Using CoCoA, we demonstrated the first in vivo widefield mouse
brain imaging using machine-learning-based adaptive optics. Incorporating
coordinate-based neural representations and a forward physics model, the
self-supervised scheme of CoCoA should be applicable to microscopy modalities
in general.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>60 pages, 20 figures, 2 tables. Nat Mach Intell (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UCM-Net: A <span class="highlight-title">Lightweight</span> and <span class="highlight-title">Efficient</span> Solution for Skin Lesion
  Segmentation using MLP and CNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09457v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09457v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyu Yuan, Dongfang Zhao, Sos S. Agaian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin cancer poses a significant public health challenge, necessitating
efficient diagnostic tools. We introduce UCM-Net, a novel skin lesion
segmentation model combining Multi-Layer Perceptrons (MLP) and Convolutional
Neural Networks (CNN). This lightweight, efficient architecture, deviating from
traditional UNet designs, dramatically reduces computational demands, making it
ideal for mobile health applications. Evaluated on PH2, ISIC 2017, and ISIC
2018 datasets, UCM-Net demonstrates robust performance with fewer than 50KB
parameters and requires less than 0.05 Giga Operations Per Second (GLOPs).
Moreover, its minimal memory requirement is just 1.19MB in CPU environment
positions. It is a potential benchmark for efficiency in skin lesion
segmentation, suitable for deployment in resource-constrained settings. In
order to facilitate accessibility and further research in the field, the
UCM-Net source code is https://github.com/chunyuyuan/UCM-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, accepted by Journal of Biomedical Signal Processing and
  Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MRISegmentator-Abdomen: A Fully Automated Multi-Organ and Structure
  Segmentation Tool for T1-weighted Abdominal MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Zhuang, Tejas Sudharshan Mathai, Pritam Mukherjee, Brandon Khoury, Boah Kim, Benjamin Hou, Nusrat Rabbee, Abhinav Suri, Ronald M. Summers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Segmentation of organs and structures in abdominal MRI is useful
for many clinical applications, such as disease diagnosis and radiotherapy.
Current approaches have focused on delineating a limited set of abdominal
structures (13 types). To date, there is no publicly available abdominal MRI
dataset with voxel-level annotations of multiple organs and structures.
Consequently, a segmentation tool for multi-structure segmentation is also
unavailable. Methods: We curated a T1-weighted abdominal MRI dataset consisting
of 195 patients who underwent imaging at National Institutes of Health (NIH)
Clinical Center. The dataset comprises of axial pre-contrast T1, arterial,
venous, and delayed phases for each patient, thereby amounting to a total of
780 series (69,248 2D slices). Each series contains voxel-level annotations of
62 abdominal organs and structures. A 3D nnUNet model, dubbed as
MRISegmentator-Abdomen (MRISegmentator in short), was trained on this dataset,
and evaluation was conducted on an internal test set and two large external
datasets: AMOS22 and Duke Liver. The predicted segmentations were compared
against the ground-truth using the Dice Similarity Coefficient (DSC) and
Normalized Surface Distance (NSD). Findings: MRISegmentator achieved an average
DSC of 0.861$\pm$0.170 and a NSD of 0.924$\pm$0.163 in the internal test set.
On the AMOS22 dataset, MRISegmentator attained an average DSC of
0.829$\pm$0.133 and a NSD of 0.908$\pm$0.067. For the Duke Liver dataset, an
average DSC of 0.933$\pm$0.015 and a NSD of 0.929$\pm$0.021 was obtained.
Interpretation: The proposed MRISegmentator provides automatic, accurate, and
robust segmentations of 62 organs and structures in T1-weighted abdominal MRI
sequences. The tool has the potential to accelerate research on various
clinical topics, such as abnormality detection, radiotherapy, disease
classification among others.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We made the segmentation model publicly available</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QUBIQ: Uncertainty Quantification for Biomedical Image Segmentation
  Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18435v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18435v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongwei Bran Li, Fernando Navarro, Ivan Ezhov, Amirhossein Bayat, Dhritiman Das, Florian Kofler, Suprosanna Shit, Diana Waldmannstetter, Johannes C. Paetzold, Xiaobin Hu, Benedikt Wiestler, Lucas Zimmer, Tamaz Amiranashvili, Chinmay Prabhakar, Christoph Berger, Jonas Weidner, Michelle Alonso-Basant, Arif Rashid, Ujjwal Baid, Wesam Adel, Deniz Ali, Bhakti Baheti, Yingbin Bai, Ishaan Bhatt, Sabri Can Cetindag, Wenting Chen, Li Cheng, Prasad Dutand, Lara Dular, Mustafa A. Elattar, Ming Feng, Shengbo Gao, Henkjan Huisman, Weifeng Hu, Shubham Innani, Wei Jiat, Davood Karimi, Hugo J. Kuijf, Jin Tae Kwak, Hoang Long Le, Xiang Lia, Huiyan Lin, Tongliang Liu, Jun Ma, Kai Ma, Ting Ma, Ilkay Oksuz, Robbie Holland, Arlindo L. Oliveira, Jimut Bahan Pal, Xuan Pei, Maoying Qiao, Anindo Saha, Raghavendra Selvan, Linlin Shen, Joao Lourenco Silva, Ziga Spiclin, Sanjay Talbar, Dadong Wang, Wei Wang, Xiong Wang, Yin Wang, Ruiling Xia, Kele Xu, Yanwu Yan, Mert Yergin, Shuang Yu, Lingxi Zeng, YingLin Zhang, Jiachen Zhao, Yefeng Zheng, Martin Zukovec, Richard Do, Anton Becker, Amber Simpson, Ender Konukoglu, Andras Jakab, Spyridon Bakas, Leo Joskowicz, Bjoern Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty in medical image segmentation tasks, especially inter-rater
variability, arising from differences in interpretations and annotations by
various experts, presents a significant challenge in achieving consistent and
reliable image segmentation. This variability not only reflects the inherent
complexity and subjective nature of medical image interpretation but also
directly impacts the development and evaluation of automated segmentation
algorithms. Accurately modeling and quantifying this variability is essential
for enhancing the robustness and clinical applicability of these algorithms. We
report the set-up and summarize the benchmark results of the Quantification of
Uncertainties in Biomedical Image Quantification Challenge (QUBIQ), which was
organized in conjunction with International Conferences on Medical Image
Computing and Computer-Assisted Intervention (MICCAI) 2020 and 2021. The
challenge focuses on the uncertainty quantification of medical image
segmentation which considers the omnipresence of inter-rater variability in
imaging datasets. The large collection of images with multi-rater annotations
features various modalities such as MRI and CT; various organs such as the
brain, prostate, kidney, and pancreas; and different image dimensions 2D-vs-3D.
A total of 24 teams submitted different solutions to the problem, combining
various baseline models, Bayesian neural networks, and ensemble model
techniques. The obtained results indicate the importance of the ensemble
models, as well as the need for further research to develop efficient 3D
methods for uncertainty quantification methods in 3D segmentation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>initial technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Informed Deep Learning for Motion-Corrected Reconstruction of
  Quantitative Brain MRI <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08298v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08298v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Eichhorn, Veronika Spieker, Kerstin Hammernik, Elisa Saks, Kilian Weiss, Christine Preibisch, Julia A. Schnabel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose PHIMO, a physics-informed learning-based motion correction method
tailored to quantitative MRI. PHIMO leverages information from the signal
evolution to exclude motion-corrupted k-space lines from a data-consistent
reconstruction. We demonstrate the potential of PHIMO for the application of
T2* quantification from gradient echo MRI, which is particularly sensitive to
motion due to its sensitivity to magnetic field inhomogeneities. A
state-of-the-art technique for motion correction requires redundant acquisition
of the k-space center, prolonging the acquisition. We show that PHIMO can
detect and exclude intra-scan motion events and, thus, correct for severe
motion artifacts. PHIMO approaches the performance of the state-of-the-art
motion correction method, while substantially reducing the acquisition time by
over 40%, facilitating clinical applicability. Our code is available at
https://github.com/HannahEichhorn/PHIMO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep-Learning Approach for Tissue Classification using Acoustic Waves
  during Ablation with an Er:YAG Laser (Updated) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14570v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14570v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Seppi, Philippe C. Cattin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's mechanical tools for bone cutting (osteotomy) cause mechanical trauma
that prolongs the healing process. Medical device manufacturers aim to minimize
this trauma, with minimally invasive surgery using laser cutting as one
innovation. This method ablates tissue using laser light instead of mechanical
tools, reducing post-surgery healing time. A reliable feedback system is
crucial during laser surgery to prevent damage to surrounding tissues. We
propose a tissue classification method analyzing acoustic waves generated
during laser ablation, demonstrating its applicability in an ex-vivo
experiment. The ablation process with a microsecond pulsed Er:YAG laser
produces acoustic waves, acquired with an air-coupled transducer. These waves
were used to classify five porcine tissue types: hard bone, soft bone, muscle,
fat, and skin. For automated tissue classification, we compared five Neural
Network (NN) approaches: a one-dimensional Convolutional Neural Network (CNN)
with time-dependent input, a Fully-connected Neural Network (FcNN) with either
the frequency spectrum or principal components of the frequency spectrum as
input, and a combination of a CNN and an FcNN with time-dependent data and its
frequency spectrum as input. Consecutive acoustic waves were used to improve
classification accuracy. Grad-Cam identified the activation map of the
frequencies, showing low frequencies as the most important for this task. Our
results indicated that combining time-dependent data with its frequency
spectrum achieved the highest classification accuracy (65.5%-75.5%). We also
found that using the frequency spectrum alone was sufficient, with no
additional benefit from applying Principal Components Analysis (PCA).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is an updated version of Deep-Learning Approach for Tissue
  Classification using Acoustic Waves during Ablation with an Er:YAG Laser
  originally published in DOI:10.1109/ACCESS.2021.3113055. This update
  addresses several issues and incorporates corrections as outlined in
  DOI:10.1109/ACCESS.2024.3395071. We provide here a detailed description of
  our experiments and the new models we used</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion-robust free-running volumetric cardiovascular MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02088v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02088v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syed M. Arshad, Lee C. Potter, Chong Chen, Yingmin Liu, Preethi Chandrasekaran, Christopher Crabtree, Matthew S. Tong, Orlando P. Simonetti, Yuchi Han, Rizwan Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PURPOSE: To present and assess an outlier mitigation method that makes
free-running volumetric cardiovascular MRI (CMR) more robust to motion.
  METHODS: The proposed method, called compressive recovery with outlier
rejection (CORe), models outliers in the measured data as an additive auxiliary
variable. We enforce MR physics-guided group sparsity on the auxiliary
variable, and jointly estimate it along with the image using an iterative
algorithm. For evaluation, CORe is first compared to traditional compressed
sensing (CS), robust regression (RR), and an existing outlier rejection method
using two simulation studies. Then, CORe is compared to CS using seven
three-dimensional (3D) cine, 12 rest four-dimensional (4D) flow, and eight
stress 4D flow imaging datasets.
  RESULTS: Our simulation studies show that CORe outperforms CS, RR, and the
existing outlier rejection method in terms of normalized mean square error and
structural similarity index across 55 different realizations. The expert reader
evaluation of 3D cine images demonstrates that CORe is more effective in
suppressing artifacts while maintaining or improving image sharpness. Finally,
4D flow images show that CORe yields more reliable and consistent flow
measurements, especially in the presence of involuntary subject motion or
exercise stress.
  CONCLUSION: An outlier rejection method is presented and tested using
simulated and measured data. This method can help suppress motion artifacts in
a wide range of free-running CMR applications.
  CODE & DATA: Implementation code and datasets are available on GitHub at
http://github.com/OSU-MR/motion-robust-CMR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do High-Performance Image-to-Image Translation Networks Enable the
  Discovery of Radiomic Features? Application to MRI Synthesis from Ultrasound
  in Prostate Cancer <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18651v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18651v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad R. Salmanpour, Amin Mousavi, Yixi Xu, William B Weeks, Ilker Hacihaliloglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the foundational characteristics of image-to-image
translation networks, specifically examining their suitability and
transferability within the context of routine clinical environments, despite
achieving high levels of performance, as indicated by a Structural Similarity
Index (SSIM) exceeding 0.95. The evaluation study was conducted using data from
794 patients diagnosed with Prostate cancer. To synthesize MRI from Ultrasound
images, we employed five widely recognized image to image translation networks
in medical imaging: 2DPix2Pix, 2DCycleGAN, 3DCycleGAN, 3DUNET, and
3DAutoEncoder. For quantitative assessment, we report four prevalent evaluation
metrics Mean Absolute Error, Mean Square Error, Structural Similarity Index
(SSIM), and Peak Signal to Noise Ratio. Moreover, a complementary analysis
employing Radiomic features (RF) via Spearman correlation coefficient was
conducted to investigate, for the first time, whether networks achieving high
performance, SSIM greater than 0.85, could identify low-level RFs. The RF
analysis showed 75 features out of 186 RFs were discovered via just 2DPix2Pix
algorithm while half of RFs were lost in the translation process. Finally, a
detailed qualitative assessment by five medical doctors indicated a lack of low
level feature discovery in image to image translation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-06-23T00:00:00Z">2024-06-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Image and Video Processing <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing the Sampling Burden of Fourier Sensing with a Non-rectangular
  Field-of-View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Dwork, Erin K. Englund, Alex J. Barker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With Fourier sensing, it is commonly the case that the field-of-view (FOV),
the area of space to be imaged, is known prior to reconstruction. To date,
reconstruction algorithms have focused on FOVs with simple geometries: a
rectangle or a hexagon. This yields sampling patterns that are more burdensome
than necessary. Due to the reduced area of imaging possible with an arbitrary
(e.g., non-rectangular) FOV, the number of samples required for a high-quality
images is reduced. However, when an arbitrary FOV has been considered, the
reconstruction algorithm is computationally expensive. In this manuscript, we
present a method to reduce the sampling pattern for an arbitrary FOV with an
accompanying direct (non-iterative) reconstruction algorithm. We also present a
method to decrease the computational cost of the (iterative) model-based
reconstruction (MBR) algorithm. We present results using MRI data of an ankle,
a pineapple, and a brain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Instabilities of Unsupervised Denoising <span class="highlight-title">Diffusion</span> Models in Magnetic
  Resonance Imaging Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Han, Sven Nebelung, Firas Khader, Jakob Nikolas Kather, Daniel Truhn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models offer a promising approach to accelerating
magnetic resonance imaging (MRI) and producing diagnostic-level images in an
unsupervised manner. However, our study demonstrates that even tiny worst-case
potential perturbations transferred from a surrogate model can cause these
models to generate fake tissue structures that may mislead clinicians. The
transferability of such worst-case perturbations indicates that the robustness
of image reconstruction may be compromised due to MR system imperfections or
other sources of noise. Moreover, at larger perturbation strengths, diffusion
models exhibit Gaussian noise-like artifacts that are distinct from those
observed in supervised models and are more challenging to detect. Our results
highlight the vulnerability of current state-of-the-art diffusion-based
reconstruction models to possible worst-case perturbations and underscore the
need for further research to improve their robustness and reliability in
clinical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fuzzy <span class="highlight-title">Attention</span>-based Border Rendering Network for Lung Organ
  Segmentation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Zhang, Yang Nan, Yingying Fang, Shiyi Wang, Xiaodan Xing, Zhifan Gao, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic lung organ segmentation on CT images is crucial for lung disease
diagnosis. However, the unlimited voxel values and class imbalance of lung
organs can lead to false-negative/positive and leakage issues in advanced
methods. Additionally, some slender lung organs are easily lost during the
recycled down/up-sample procedure, e.g., bronchioles & arterioles, causing
severe discontinuity issue. Inspired by these, this paper introduces an
effective lung organ segmentation method called Fuzzy Attention-based Border
Rendering (FABR) network. Since fuzzy logic can handle the uncertainty in
feature extraction, hence the fusion of deep networks and fuzzy sets should be
a viable solution for better performance. Meanwhile, unlike prior top-tier
methods that operate on all regular dense points, our FABR depicts lung organ
regions as cube-trees, focusing only on recycle-sampled border vulnerable
points, rendering the severely discontinuous, false-negative/positive organ
regions with a novel Global-Local Cube-tree Fusion (GLCF) module. All
experimental results, on four challenging datasets of airway & artery,
demonstrate that our method can achieve the favorable performance
significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research on Feature Extraction Data Processing System For MRI of Brain
  Diseases Based on Computer Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingxi Xiao, Jinxin Hu, Yutian Yang, Yinqiu Feng, Zichao Li, Zexi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the existing wavelet image processing techniques are carried out in
the form of single-scale reconstruction and multiple iterations. However,
processing high-quality fMRI data presents problems such as mixed noise and
excessive computation time. This project proposes the use of matrix operations
by combining mixed noise elimination methods with wavelet analysis to replace
traditional iterative algorithms. Functional magnetic resonance imaging (fMRI)
of the auditory cortex of a single subject is analyzed and compared to the
wavelet domain signal processing technology based on repeated times and the
world's most influential SPM8. Experiments show that this algorithm is the
fastest in computing time, and its detection effect is comparable to the
traditional iterative algorithm. However, this has a higher practical value for
the processing of FMRI data. In addition, the wavelet analysis method proposed
signal processing to speed up the calculation rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intensity Confusion Matters: An Intensity-Distance Guided Loss for
  Bronchus Segmentation <span class="chip">ICME</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haifan Gong, Wenhao Huang, Huan Zhang, Yu Wang, Xiang Wan, Hong Shen, Guanbin Li, Haofeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic segmentation of the bronchial tree from CT imaging is important, as
it provides structural information for disease diagnosis. Despite the merits of
previous automatic bronchus segmentation methods, they have paied less
attention to the issue we term as \textit{Intensity Confusion}, wherein the
intensity values of certain background voxels approach those of the foreground
voxels within bronchi. Conversely, the intensity values of some foreground
voxels are nearly identical to those of background voxels. This proximity in
intensity values introduces significant challenges to neural network
methodologies. To address the issue, we introduce a novel Intensity-Distance
Guided loss function, which assigns adaptive weights to different image voxels
for mining hard samples that cause the intensity confusion. The proposed loss
estimates the voxel-level hardness of samples, on the basis of the following
intensity and distance priors. We regard a voxel as a hard sample if it is in:
(1) the background and has an intensity value close to the bronchus region; (2)
the bronchus region and is of higher intensity than most voxels inside the
bronchus; (3) the background region and at a short distance from the bronchus.
Extensive experiments not only show the superiority of our method compared with
the state-of-the-art methods, but also verify that tackling the intensity
confusion issue helps to significantly improve bronchus segmentation. Project
page: https://github.com/lhaof/ICM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE International Conference on Multimedia & Expo (ICME) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Mamba</span>-based Light Field <span class="highlight-title">Super-Resolution</span> with <span class="highlight-title">Efficient</span> Subspace
  Scanning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruisheng Gao, Zeyu Xiao, Zhiwei Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based methods have demonstrated impressive performance in 4D
light field (LF) super-resolution by effectively modeling long-range
spatial-angular correlations, but their quadratic complexity hinders the
efficient processing of high resolution 4D inputs, resulting in slow inference
speed and high memory cost. As a compromise, most prior work adopts a
patch-based strategy, which fails to leverage the full information from the
entire input LFs. The recently proposed selective state-space model, Mamba, has
gained popularity for its efficient long-range sequence modeling. In this
paper, we propose a Mamba-based Light Field Super-Resolution method, named
MLFSR, by designing an efficient subspace scanning strategy. Specifically, we
tokenize 4D LFs into subspace sequences and conduct bi-directional scanning on
each subspace. Based on our scanning strategy, we then design the Mamba-based
Global Interaction (MGI) module to capture global information and the local
Spatial- Angular Modulator (SAM) to complement local details. Additionally, we
introduce a Transformer-to-Mamba (T2M) loss to further enhance overall
performance. Extensive experiments on public benchmarks demonstrate that MLFSR
surpasses CNN-based models and rivals Transformer-based methods in performance
while maintaining higher efficiency. With quicker inference speed and reduced
memory demand, MLFSR facilitates full-image processing of high-resolution 4D
LFs with enhanced performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAVM: Conditional Autoregressive Vision Model for Contrast-Enhanced
  Brain Tumor MRI Synthesis <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lujun Gui, Chuyang Ye, Tianyi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrast-enhanced magnetic resonance imaging (MRI) is pivotal in the pipeline
of brain tumor segmentation and analysis. Gadolinium-based contrast agents, as
the most commonly used contrast agents, are expensive and may have potential
side effects, and it is desired to obtain contrast-enhanced brain tumor MRI
scans without the actual use of contrast agents. Deep learning methods have
been applied to synthesize virtual contrast-enhanced MRI scans from
non-contrast images. However, as this synthesis problem is inherently
ill-posed, these methods fall short in producing high-quality results. In this
work, we propose Conditional Autoregressive Vision Model (CAVM) for improving
the synthesis of contrast-enhanced brain tumor MRI. As the enhancement of image
intensity grows with a higher dose of contrast agents, we assume that it is
less challenging to synthesize a virtual image with a lower dose, where the
difference between the contrast-enhanced and non-contrast images is smaller.
Thus, CAVM gradually increases the contrast agent dosage and produces
higher-dose images based on previous lower-dose ones until the final desired
dose is achieved. Inspired by the resemblance between the gradual dose increase
and the Chain-of-Thought approach in natural language processing, CAVM uses an
autoregressive strategy with a decomposition tokenizer and a decoder.
Specifically, the tokenizer is applied to obtain a more compact image
representation for computational efficiency, and it decomposes the image into
dose-variant and dose-invariant tokens. Then, a masked self-attention mechanism
is developed for autoregression that gradually increases the dose of the
virtual image based on the dose-variant tokens. Finally, the updated
dose-variant tokens corresponding to the desired dose are decoded together with
dose-invariant tokens to produce the final contrast-enhanced MRI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The work has been accepted by MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wound Tissue Segmentation in Diabetic Foot Ulcer Images Using Deep
  Learning: A Pilot Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mrinal Kanti Dhar, Chuanbo Wang, Yash Patel, Taiyu Zhang, Jeffrey Niezgoda, Sandeep Gopalakrishnan, Keke Chen, Zeyun Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying individual tissues, so-called tissue segmentation, in diabetic
foot ulcer (DFU) images is a challenging task and little work has been
published, largely due to the limited availability of a clinical image dataset.
To address this gap, we have created a DFUTissue dataset for the research
community to evaluate wound tissue segmentation algorithms. The dataset
contains 110 images with tissues labeled by wound experts and 600 unlabeled
images. Additionally, we conducted a pilot study on segmenting wound
characteristics including fibrin, granulation, and callus using deep learning.
Due to the limited amount of annotated data, our framework consists of both
supervised learning (SL) and semi-supervised learning (SSL) phases. In the SL
phase, we propose a hybrid model featuring a Mix Transformer (MiT-b3) in the
encoder and a CNN in the decoder, enhanced by the integration of a parallel
spatial and channel squeeze-and-excitation (P-scSE) module known for its
efficacy in improving boundary accuracy. The SSL phase employs a
pseudo-labeling-based approach, iteratively identifying and incorporating
valuable unlabeled images to enhance overall segmentation performance.
Comparative evaluations with state-of-the-art methods are conducted for both SL
and SSL phases. The SL achieves a Dice Similarity Coefficient (DSC) of 84.89%,
which has been improved to 87.64% in the SSL phase. Furthermore, the results
are benchmarked against two widely used SSL approaches: Generative Adversarial
Networks and Cross-Consistency Training. Additionally, our hybrid model
outperforms the state-of-the-art methods with a 92.99% DSC in performing binary
segmentation of DFU wound areas when tested on the Chronic Wound dataset. Codes
and data are available at https://github.com/uwm-bigdata/DFUTissueSegNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Segmentation of Ascites on Abdominal CT Scans for
  Automatic Volume Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Hou, Sung-Won Lee, Jung-Min Lee, Christopher Koh, Jing Xiao, Perry J. Pickhardt, Ronald M. Summers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To evaluate the performance of an automated deep learning method in
detecting ascites and subsequently quantifying its volume in patients with
liver cirrhosis and ovarian cancer.
  Materials and Methods: This retrospective study included contrast-enhanced
and non-contrast abdominal-pelvic CT scans of patients with cirrhotic ascites
and patients with ovarian cancer from two institutions, National Institutes of
Health (NIH) and University of Wisconsin (UofW). The model, trained on The
Cancer Genome Atlas Ovarian Cancer dataset (mean age, 60 years +/- 11 [s.d.];
143 female), was tested on two internal (NIH-LC and NIH-OV) and one external
dataset (UofW-LC). Its performance was measured by the Dice coefficient,
standard deviations, and 95% confidence intervals, focusing on ascites volume
in the peritoneal cavity.
  Results: On NIH-LC (25 patients; mean age, 59 years +/- 14 [s.d.]; 14 male)
and NIH-OV (166 patients; mean age, 65 years +/- 9 [s.d.]; all female), the
model achieved Dice scores of 0.855 +/- 0.061 (CI: 0.831-0.878) and 0.826 +/-
0.153 (CI: 0.764-0.887), with median volume estimation errors of 19.6% (IQR:
13.2-29.0) and 5.3% (IQR: 2.4-9.7) respectively. On UofW-LC (124 patients; mean
age, 46 years +/- 12 [s.d.]; 73 female), the model had a Dice score of 0.830
+/- 0.107 (CI: 0.798-0.863) and median volume estimation error of 9.7% (IQR:
4.5-15.1). The model showed strong agreement with expert assessments, with r^2
values of 0.79, 0.98, and 0.97 across the test sets.
  Conclusion: The proposed deep learning method performed well in segmenting
and quantifying the volume of ascites in concordance with expert radiologist
assessments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Latents for <span class="highlight-title">Efficient</span> Thermography Classification and
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06589v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06589v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamir Shor, Chaim Baskin, Alex Bronstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer is a prominent health concern worldwide, currently being the
secondmost common and second-deadliest type of cancer in women. While current
breast cancer diagnosis mainly relies on mammography imaging, in recent years
the use of thermography for breast cancer imaging has been garnering growing
popularity. Thermographic imaging relies on infrared cameras to capture
body-emitted heat distributions. While these heat signatures have proven useful
for computer-vision systems for accurate breast cancer segmentation and
classification, prior work often relies on handcrafted feature engineering or
complex architectures, potentially limiting the comparability and applicability
of these methods. In this work, we present a novel algorithm for both breast
cancer classification and segmentation. Rather than focusing efforts on manual
feature and architecture engineering, our algorithm focuses on leveraging an
informative, learned feature space, thus making our solution simpler to use and
extend to other frameworks and downstream tasks, as well as more applicable to
data-scarce settings. Our classification produces SOTA results, while we are
the first work to produce segmentation regions studied in this paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiview Contrastive Learning for Completely <span class="highlight-title">Blind</span> Video Quality
  Assessment of User Generated Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06148v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06148v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shankhanil Mitra, Rajiv Soundararajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Completely blind video quality assessment (VQA) refers to a class of quality
assessment methods that do not use any reference videos, human opinion scores
or training videos from the target database to learn a quality model. The
design of this class of methods is particularly important since it can allow
for superior generalization in performance across various datasets. We consider
the design of completely blind VQA for user generated content. While several
deep feature extraction methods have been considered in supervised and weakly
supervised settings, such approaches have not been studied in the context of
completely blind VQA. We bridge this gap by presenting a self-supervised
multiview contrastive learning framework to learn spatio-temporal quality
representations. In particular, we capture the common information between frame
differences and frames by treating them as a pair of views and similarly obtain
the shared representations between frame differences and optical flow. The
resulting features are then compared with a corpus of pristine natural video
patches to predict the quality of the distorted video. Detailed experiments on
multiple camera captured VQA datasets reveal the superior performance of our
method over other features when evaluated without training on human scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rate Splitting Multiple Access-Enabled Adaptive Panoramic Video Semantic
  Transmission 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haixiao Gao, Mengying Sun, Xiaodong Xu, Shujun Han, Bizhu Wang, Jingxuan Zhang, Ping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an adaptive panoramic video semantic transmission
(APVST) framework enabled by rate splitting multiple access (RSMA). The APVST
framework consists of a semantic transmitter and receiver, utilizing a deep
joint source-channel coding structure to adaptively extract and encode semantic
features from panoramic frames. To achieve higher spectral efficiency and
conserve bandwidth, APVST employs an entropy model and a dimension-adaptive
module to control the transmission rate. Additionally, we take
weighted-to-spherically-uniform peak signal-to-noise ratio (WS-PSNR) and
weighted-to-spherically-uniform structural similarity (WS-SSIM) as distortion
evaluation metrics for panoramic videos and design a weighted self-attention
module for APVST. This module integrates weights and feature maps to enhance
the quality of the immersive experience. Considering the overlap in the field
of view when users watch panoramic videos, we further utilize RSMA to split the
required panoramic video semantic streams into common and private messages for
transmission. We propose an RSMA-enabled semantic stream transmission scheme
and formulate a joint problem of latency and immersive experience quality by
optimizing the allocation ratios of power, common rate, and channel bandwidth,
aiming to maximize the quality of service (QoS) scores for users. To address
the above problem, we propose a deep reinforcement learning algorithm based on
proximal policy optimization (PPO) with high efficiency to handle dynamically
changing environments. Simulation results demonstrate that our proposed APVST
framework saves up to 20% and 50% of channel bandwidth compared to other
semantic and traditional video transmission schemes, respectively. Moreover,
our study confirms the efficiency of RSMA in panoramic video transmission,
achieving performance gains of 13% and 20% compared to NOMA and OFDMA.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-07-01T05:26:59.662312090Z">
            2024-07-01 05:26:59 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
