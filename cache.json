{"2024-06-28T00:00:00Z":{"Image and Video Processing":[{"id":"http://arxiv.org/abs/2406.20005v1","updated":"2024-06-28T15:44:55Z","published":"2024-06-28T15:44:55Z","title":"Malaria Cell Detection Using Deep Neural Networks","summary":"  Malaria remains one of the most pressing public health concerns globally,\ncausing significant morbidity and mortality, especially in sub-Saharan Africa.\nRapid and accurate diagnosis is crucial for effective treatment and disease\nmanagement. Traditional diagnostic methods, such as microscopic examination of\nblood smears, are labor-intensive and require significant expertise, which may\nnot be readily available in resource-limited settings. This project aims to\nautomate the detection of malaria-infected cells using a deep learning\napproach. We employed a convolutional neural network (CNN) based on the\nResNet50 architecture, leveraging transfer learning to enhance performance. The\nMalaria Cell Images Dataset from Kaggle, containing 27,558 images categorized\ninto infected and uninfected cells, was used for training and evaluation. Our\nmodel demonstrated high accuracy, precision, and recall, indicating its\npotential as a reliable tool for assisting in malaria diagnosis. Additionally,\na web application was developed using Streamlit to allow users to upload cell\nimages and receive predictions about malaria infection, making the technology\naccessible and user-friendly. This paper provides a comprehensive overview of\nthe methodology, experiments, and results, highlighting the effectiveness of\ndeep learning in medical image analysis.\n","authors":["Saurabh Sawant","Anurag Singh"],"pdf_url":"https://arxiv.org/pdf/2406.20005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19943v1","updated":"2024-06-28T14:22:30Z","published":"2024-06-28T14:22:30Z","title":"Impact of Initialization on Intra-subject Pediatric Brain MR Image\n  Registration: A Comparative Analysis between SyN ANTs and Deep Learning-Based\n  Approaches","summary":"  This study evaluates the performance of conventional SyN ANTs and\nlearning-based registration methods in the context of pediatric neuroimaging,\nspecifically focusing on intrasubject deformable registration. The comparison\ninvolves three approaches: without (NR), with rigid (RR), and with rigid and\naffine (RAR) initializations. In addition to initialization, performances are\nevaluated in terms of accuracy, speed, and the impact of age intervals and sex\nper pair. Data consists of the publicly available MRI scans from the Calgary\nPreschool dataset, which includes 63 children aged 2-7 years, allowing for 431\nregistration pairs. We implemented the unsupervised DL framework with a U-Net\narchitecture using DeepReg and it was 5-fold cross-validated. Evaluation\nincludes Dice scores for tissue segmentation from 18 smaller regions obtained\nby SynthSeg, analysis of log Jacobian determinants, and registration pro-rated\ntraining and inference times. Learning-based approaches, with or without linear\ninitializations, exhibit slight superiority over SyN ANTs in terms of Dice\nscores. Indeed, DL-based implementations with RR and RAR initializations\nsignificantly outperform SyN ANTs. Both SyN ANTs and DL-based registration\ninvolve parameter optimization, but the choice between these methods depends on\nthe scale of registration: network-based for broader coverage or SyN ANTs for\nspecific structures. Both methods face challenges with larger age intervals due\nto greater growth changes. The main takeaway is that while DL-based methods\nshow promise with faster and more accurate registrations, SyN ANTs remains\nrobust and generalizable without the need for extensive training, highlighting\nthe importance of method selection based on specific registration needs in the\npediatric context. Our code is available at\nhttps://github.com/neuropoly/pediatric-DL-registration\n","authors":["Andjela Dimitrijevic","Vincent Noblet","Benjamin De Leener"],"pdf_url":"https://arxiv.org/pdf/2406.19943v1.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:013"},{"id":"http://arxiv.org/abs/2406.19870v1","updated":"2024-06-28T12:22:31Z","published":"2024-06-28T12:22:31Z","title":"Deep Unfolding-Aided Parameter Tuning for Plug-and-Play Based Video\n  Snapshot Compressive Imaging","summary":"  Snapshot compressive imaging (SCI) captures high-dimensional data efficiently\nby compressing it into two-dimensional observations and reconstructing\nhigh-dimensional data from two-dimensional observations with various\nalgorithms. Plug-and-play (PnP) is a promising approach for the video SCI\nreconstruction because it can leverage both the observation model and denoising\nmethods for videos. This paper proposes a deep unfolding-based method for\ntuning noise level parameters in PnP-based video SCI, which significantly\naffects the reconstruction accuracy. For the training of the parameters, we\nprepare training data from the densely annotated video segmentation (DAVIS)\ndataset, reparametrize the noise level parameters, and apply the checkpointing\ntechnique to reduce the required memory. Simulation results show that the\ntrained noise level parameters significantly improve the reconstruction\naccuracy and exhibit a non-monotonic pattern, which is different from the\nassumptions in the conventional convergence analyses of PnP-based algorithms.\n","authors":["Takashi Matsuda","Ryo Hayakawa","Youji Iiguni"],"pdf_url":"https://arxiv.org/pdf/2406.19870v1.pdf","comment":"This work will be submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2406.19796v1","updated":"2024-06-28T10:05:58Z","published":"2024-06-28T10:05:58Z","title":"Comprehensive Generative Replay for Task-Incremental Segmentation with\n  Concurrent Appearance and Semantic Forgetting","summary":"  Generalist segmentation models are increasingly favored for diverse tasks\ninvolving various objects from different image sources. Task-Incremental\nLearning (TIL) offers a privacy-preserving training paradigm using tasks\narriving sequentially, instead of gathering them due to strict data sharing\npolicies. However, the task evolution can span a wide scope that involves\nshifts in both image appearance and segmentation semantics with intricate\ncorrelation, causing concurrent appearance and semantic forgetting. To solve\nthis issue, we propose a Comprehensive Generative Replay (CGR) framework that\nrestores appearance and semantic knowledge by synthesizing image-mask pairs to\nmimic past task data, which focuses on two aspects: modeling image-mask\ncorrespondence and promoting scalability for diverse tasks. Specifically, we\nintroduce a novel Bayesian Joint Diffusion (BJD) model for high-quality\nsynthesis of image-mask pairs with their correspondence explicitly preserved by\nconditional denoising. Furthermore, we develop a Task-Oriented Adapter (TOA)\nthat recalibrates prompt embeddings to modulate the diffusion model, making the\ndata synthesis compatible with different tasks. Experiments on incremental\ntasks (cardiac, fundus and prostate segmentation) show its clear advantage for\nalleviating concurrent appearance and semantic forgetting. Code is available at\nhttps://github.com/jingyzhang/CGR.\n","authors":["Wei Li","Jingyang Zhang","Pheng-Ann Heng","Lixu Gu"],"pdf_url":"https://arxiv.org/pdf/2406.19796v1.pdf","comment":"Accepted by MICCAI24"},{"id":"http://arxiv.org/abs/2404.09666v2","updated":"2024-06-28T09:25:25Z","published":"2024-04-15T10:57:16Z","title":"Deformable MRI Sequence Registration for AI-based Prostate Cancer\n  Diagnosis","summary":"  The PI-CAI (Prostate Imaging: Cancer AI) challenge led to expert-level\ndiagnostic algorithms for clinically significant prostate cancer detection. The\nalgorithms receive biparametric MRI scans as input, which consist of\nT2-weighted and diffusion-weighted scans. These scans can be misaligned due to\nmultiple factors in the scanning process. Image registration can alleviate this\nissue by predicting the deformation between the sequences. We investigate the\neffect of image registration on the diagnostic performance of AI-based prostate\ncancer diagnosis. First, the image registration algorithm, developed in\nMeVisLab, is analyzed using a dataset with paired lesion annotations. Second,\nthe effect on diagnosis is evaluated by comparing case-level cancer diagnosis\nperformance between using the original dataset, rigidly aligned\ndiffusion-weighted scans, or deformably aligned diffusion-weighted scans. Rigid\nregistration showed no improvement. Deformable registration demonstrated a\nsubstantial improvement in lesion overlap (+10% median Dice score) and a\npositive yet non-significant improvement in diagnostic performance (+0.3%\nAUROC, p=0.18). Our investigation shows that a substantial improvement in\nlesion alignment does not directly lead to a significant improvement in\ndiagnostic performance. Qualitative analysis indicated that jointly developing\nimage registration methods and diagnostic AI algorithms could enhance\ndiagnostic accuracy and patient outcomes.\n","authors":["Alessa Hering","Sarah de Boer","Anindo Saha","Jasper J. Twilt","Mattias P. Heinrich","Derya Yakar","Maarten de Rooij","Henkjan Huisman","Joeran S. Bosma"],"pdf_url":"https://arxiv.org/pdf/2404.09666v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19336v2","updated":"2024-06-28T09:20:01Z","published":"2024-06-27T17:10:10Z","title":"LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver\n  with a Few Partial Ultrasound Scans","summary":"  3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver\nvisibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver\nreconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no\nsignificant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our\nknowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.\n","authors":["Kaushalya Sivayogaraj","Sahan T. Guruge","Udari Liyanage","Jeevani Udupihille","Saroj Jayasinghe","Gerard Fernando","Ranga Rodrigo","M. Rukshani Liyanaarachchi"],"pdf_url":"https://arxiv.org/pdf/2406.19336v2.pdf","comment":"10 pages, Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2304.10839v4","updated":"2024-06-28T09:01:03Z","published":"2023-04-21T09:30:22Z","title":"Cross-domain Denoising for Low-dose Multi-frame Spiral Computed\n  Tomography","summary":"  Computed tomography (CT) has been used worldwide as a non-invasive test to\nassist in diagnosis. However, the ionizing nature of X-ray exposure raises\nconcerns about potential health risks such as cancer. The desire for lower\nradiation doses has driven researchers to improve reconstruction quality.\nAlthough previous studies on low-dose computed tomography (LDCT) denoising have\ndemonstrated the effectiveness of learning-based methods, most were developed\non the simulated data. However, the real-world scenario differs significantly\nfrom the simulation domain, especially when using the multi-slice spiral\nscanner geometry. This paper proposes a two-stage method for the commercially\navailable multi-slice spiral CT scanners that better exploits the complete\nreconstruction pipeline for LDCT denoising across different domains. Our\napproach makes good use of the high redundancy of multi-slice projections and\nthe volumetric reconstructions while leveraging the over-smoothing problem in\nconventional cascaded frameworks caused by aggressive denoising. The dedicated\ndesign also provides a more explicit interpretation of the data flow. Extensive\nexperiments on various datasets showed that the proposed method could remove up\nto 70\\% of noise without compromised spatial resolution, and subjective\nevaluations by two experienced radiologists further supported its superior\nperformance against state-of-the-art methods in clinical practice.\n","authors":["Yucheng Lu","Zhixin Xu","Moon Hyung Choi","Jimin Kim","Seung-Won Jung"],"pdf_url":"https://arxiv.org/pdf/2304.10839v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19749v1","updated":"2024-06-28T08:48:14Z","published":"2024-06-28T08:48:14Z","title":"SPIRONet: Spatial-Frequency Learning and Topological Channel Interaction\n  Network for Vessel Segmentation","summary":"  Automatic vessel segmentation is paramount for developing next-generation\ninterventional navigation systems. However, current approaches suffer from\nsuboptimal segmentation performances due to significant challenges in\nintraoperative images (i.e., low signal-to-noise ratio, small or slender\nvessels, and strong interference). In this paper, a novel spatial-frequency\nlearning and topological channel interaction network (SPIRONet) is proposed to\naddress the above issues. Specifically, dual encoders are utilized to\ncomprehensively capture local spatial and global frequency vessel features.\nThen, a cross-attention fusion module is introduced to effectively fuse spatial\nand frequency features, thereby enhancing feature discriminability.\nFurthermore, a topological channel interaction module is designed to filter out\ntask-irrelevant responses based on graph neural networks. Extensive\nexperimental results on several challenging datasets (CADSA, CAXF, DCA1, and\nXCAD) demonstrate state-of-the-art performances of our method. Moreover, the\ninference speed of SPIRONet is 21 FPS with a 512x512 input size, surpassing\nclinical real-time requirements (6~12FPS). These promising outcomes indicate\nSPIRONet's potential for integration into vascular interventional navigation\nsystems. Code is available at https://github.com/Dxhuang-CASIA/SPIRONet.\n","authors":["De-Xing Huang","Xiao-Hu Zhou","Xiao-Liang Xie","Shi-Qi Liu","Shuang-Yi Wang","Zhen-Qiu Feng","Mei-Jiang Gui","Hao Li","Tian-Yu Xiang","Bo-Xian Yao","Zeng-Guang Hou"],"pdf_url":"https://arxiv.org/pdf/2406.19749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19686v1","updated":"2024-06-28T06:51:38Z","published":"2024-06-28T06:51:38Z","title":"Enhancing Radiological Diagnosis: A Collaborative Approach Integrating\n  AI and Human Expertise for Visual Miss Correction","summary":"  Human-AI collaboration to identify and correct perceptual errors in chest\nradiographs has not been previously explored. This study aimed to develop a\ncollaborative AI system, CoRaX, which integrates eye gaze data and radiology\nreports to enhance diagnostic accuracy in chest radiology by pinpointing\nperceptual errors and refining the decision-making process. Using public\ndatasets REFLACX and EGD-CXR, the study retrospectively developed CoRaX,\nemploying a large multimodal model to analyze image embeddings, eye gaze data,\nand radiology reports. The system's effectiveness was evaluated based on its\nreferral-making process, the quality of referrals, and performance in\ncollaborative diagnostic settings. CoRaX was tested on a simulated error\ndataset of 271 samples with 28% (93 of 332) missed abnormalities. The system\ncorrected 21% (71 of 332) of these errors, leaving 7% (22 of 312) unresolved.\nThe Referral-Usefulness score, indicating the accuracy of predicted regions for\nall true referrals, was 0.63 (95% CI 0.59, 0.68). The Total-Usefulness score,\nreflecting the diagnostic accuracy of CoRaX's interactions with radiologists,\nshowed that 84% (237 of 280) of these interactions had a score above 0.40. In\nconclusion, CoRaX efficiently collaborates with radiologists to address\nperceptual errors across various abnormalities, with potential applications in\nthe education and training of novice radiologists.\n","authors":["Akash Awasthi","Ngan Le","Zhigang Deng","Carol C. Wu","Hien Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2406.19686v1.pdf","comment":"Under Review in Journal"},{"id":"http://arxiv.org/abs/2406.17051v2","updated":"2024-06-28T05:50:11Z","published":"2024-06-24T18:13:09Z","title":"Leveraging Knowledge Distillation for Lightweight Skin Cancer\n  Classification: Balancing Accuracy and Computational Efficiency","summary":"  Skin cancer is a major concern to public health, accounting for one-third of\nthe reported cancers. If not detected early, the cancer has the potential for\nsevere consequences. Recognizing the critical need for effective skin cancer\nclassification, we address the limitations of existing models, which are often\ntoo large to deploy in areas with limited computational resources. In response,\nwe present a knowledge distillation based approach for creating a lightweight\nyet high-performing classifier. The proposed solution involves fusing three\nmodels, namely ResNet152V2, ConvNeXtBase, and ViT Base, to create an effective\nteacher model. The teacher model is then employed to guide a lightweight\nstudent model of size 2.03 MB. This student model is further compressed to\n469.77 KB using 16-bit quantization, enabling smooth incorporation into edge\ndevices. With six-stage image preprocessing, data augmentation, and a rigorous\nablation study, the model achieves an impressive accuracy of 98.75% on the\nHAM10000 dataset and 98.94% on the Kaggle dataset in classifying benign and\nmalignant skin cancers. With its high accuracy and compact size, our model\nappears to be a potential choice for accurate skin cancer classification,\nparticularly in resource-constrained settings.\n","authors":["Niful Islam","Khan Md Hasib","Fahmida Akter Joti","Asif Karim","Sami Azam"],"pdf_url":"https://arxiv.org/pdf/2406.17051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19666v1","updated":"2024-06-28T05:25:57Z","published":"2024-06-28T05:25:57Z","title":"CSAKD: Knowledge Distillation with Cross Self-Attention for\n  Hyperspectral and Multispectral Image Fusion","summary":"  Hyperspectral imaging, capturing detailed spectral information for each\npixel, is pivotal in diverse scientific and industrial applications. Yet, the\nacquisition of high-resolution (HR) hyperspectral images (HSIs) often needs to\nbe addressed due to the hardware limitations of existing imaging systems. A\nprevalent workaround involves capturing both a high-resolution multispectral\nimage (HR-MSI) and a low-resolution (LR) HSI, subsequently fusing them to yield\nthe desired HR-HSI. Although deep learning-based methods have shown promising\nin HR-MSI/LR-HSI fusion and LR-HSI super-resolution (SR), their substantial\nmodel complexities hinder deployment on resource-constrained imaging devices.\nThis paper introduces a novel knowledge distillation (KD) framework for\nHR-MSI/LR-HSI fusion to achieve SR of LR-HSI. Our KD framework integrates the\nproposed Cross-Layer Residual Aggregation (CLRA) block to enhance efficiency\nfor constructing Dual Two-Streamed (DTS) network structure, designed to extract\njoint and distinct features from LR-HSI and HR-MSI simultaneously. To fully\nexploit the spatial and spectral feature representations of LR-HSI and HR-MSI,\nwe propose a novel Cross Self-Attention (CSA) fusion module to adaptively fuse\nthose features to improve the spatial and spectral quality of the reconstructed\nHR-HSI. Finally, the proposed KD-based joint loss function is employed to\nco-train the teacher and student networks. Our experimental results demonstrate\nthat the student model not only achieves comparable or superior LR-HSI SR\nperformance but also significantly reduces the model-size and computational\nrequirements. This marks a substantial advancement over existing\nstate-of-the-art methods. The source code is available at\nhttps://github.com/ming053l/CSAKD.\n","authors":["Chih-Chung Hsu","Chih-Chien Ni","Chia-Ming Lee","Li-Wei Kang"],"pdf_url":"https://arxiv.org/pdf/2406.19666v1.pdf","comment":"Submitted to TIP 2024"},{"id":"http://arxiv.org/abs/2406.19649v1","updated":"2024-06-28T04:38:12Z","published":"2024-06-28T04:38:12Z","title":"AstMatch: Adversarial Self-training Consistency Framework for\n  Semi-Supervised Medical Image Segmentation","summary":"  Semi-supervised learning (SSL) has shown considerable potential in medical\nimage segmentation, primarily leveraging consistency regularization and\npseudo-labeling. However, many SSL approaches only pay attention to low-level\nconsistency and overlook the significance of pseudo-label reliability.\nTherefore, in this work, we propose an adversarial self-training consistency\nframework (AstMatch). Firstly, we design an adversarial consistency\nregularization (ACR) approach to enhance knowledge transfer and strengthen\nprediction consistency under varying perturbation intensities. Second, we apply\na feature matching loss for adversarial training to incorporate high-level\nconsistency regularization. Additionally, we present the pyramid channel\nattention (PCA) and efficient channel and spatial attention (ECSA) modules to\nimprove the discriminator's performance. Finally, we propose an adaptive\nself-training (AST) approach to ensure the pseudo-labels' quality. The proposed\nAstMatch has been extensively evaluated with cutting-edge SSL methods on three\npublic-available datasets. The experimental results under different labeled\nratios indicate that AstMatch outperforms other existing methods, achieving new\nstate-of-the-art performance. Our code will be available at\nhttps://github.com/GuanghaoZhu663/AstMatch.\n","authors":["Guanghao Zhu","Jing Zhang","Juanxiu Liu","Xiaohui Du","Ruqian Hao","Yong Liu","Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2406.19649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14534v2","updated":"2024-06-28T02:12:20Z","published":"2024-06-20T17:47:30Z","title":"Epicardium Prompt-guided Real-time Cardiac Ultrasound Frame-to-volume\n  Registration","summary":"  A comprehensive guidance view for cardiac interventional surgery can be\nprovided by the real-time fusion of the intraoperative 2D images and\npreoperative 3D volume based on the ultrasound frame-to-volume registration.\nHowever, cardiac ultrasound images are characterized by a low signal-to-noise\nratio and small differences between adjacent frames, coupled with significant\ndimension variations between 2D frames and 3D volumes to be registered,\nresulting in real-time and accurate cardiac ultrasound frame-to-volume\nregistration being a very challenging task. This paper introduces a lightweight\nend-to-end Cardiac Ultrasound frame-to-volume Registration network, termed\nCU-Reg. Specifically, the proposed model leverages epicardium prompt-guided\nanatomical clues to reinforce the interaction of 2D sparse and 3D dense\nfeatures, followed by a voxel-wise local-global aggregation of enhanced\nfeatures, thereby boosting the cross-dimensional matching effectiveness of\nlow-quality ultrasound modalities. We further embed an inter-frame\ndiscriminative regularization term within the hybrid supervised learning to\nincrease the distinction between adjacent slices in the same ultrasound volume\nto ensure registration stability. Experimental results on the reprocessed CAMUS\ndataset demonstrate that our CU-Reg surpasses existing methods in terms of\nregistration accuracy and efficiency, meeting the guidance requirements of\nclinical cardiac interventional surgery.\n","authors":["Long Lei","Jun Zhou","Jialun Pei","Baoliang Zhao","Yueming Jin","Yuen-Chun Jeremy Teoh","Jing Qin","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2406.14534v2.pdf","comment":"This paper has been accepted by MICCAI 2024"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.20099v1","updated":"2024-06-28T17:59:51Z","published":"2024-06-28T17:59:51Z","title":"Odd-One-Out: Anomaly Detection by Comparing with Neighbors","summary":"  This paper introduces a novel anomaly detection (AD) problem that focuses on\nidentifying `odd-looking' objects relative to the other instances within a\nscene. Unlike the traditional AD benchmarks, in our setting, anomalies in this\ncontext are scene-specific, defined by the regular instances that make up the\nmajority. Since object instances are often partly visible from a single\nviewpoint, our setting provides multiple views of each scene as input. To\nprovide a testbed for future research in this task, we introduce two\nbenchmarks, ToysAD-8K and PartsAD-15K. We propose a novel method that generates\n3D object-centric representations for each instance and detects the anomalous\nones through a cross-examination between the instances. We rigorously analyze\nour method quantitatively and qualitatively in the presented benchmarks.\n","authors":["Ankan Bhunia","Changjian Li","Hakan Bilen"],"pdf_url":"https://arxiv.org/pdf/2406.20099v1.pdf","comment":"Codes & Dataset at https://github.com/VICO-UoE/OddOneOutAD"},{"id":"http://arxiv.org/abs/2406.20098v1","updated":"2024-06-28T17:59:46Z","published":"2024-06-28T17:59:46Z","title":"Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework\n  for Multimodal LLMs","summary":"  Multimodal large language models (MLLMs) have shown impressive success across\nmodalities such as image, video, and audio in a variety of understanding and\ngeneration tasks. However, current MLLMs are surprisingly poor at understanding\nwebpage screenshots and generating their corresponding HTML code. To address\nthis problem, we propose Web2Code, a benchmark consisting of a new large-scale\nwebpage-to-code dataset for instruction tuning and an evaluation framework for\nthe webpage understanding and HTML code translation abilities of MLLMs. For\ndataset construction, we leverage pretrained LLMs to enhance existing\nwebpage-to-code datasets as well as generate a diverse pool of new webpages\nrendered into images. Specifically, the inputs are webpage images and\ninstructions, while the responses are the webpage's HTML code. We further\ninclude diverse natural language QA pairs about the webpage content in the\nresponses to enable a more comprehensive understanding of the web content. To\nevaluate model performance in these tasks, we develop an evaluation framework\nfor testing MLLMs' abilities in webpage understanding and web-to-code\ngeneration. Extensive experiments show that our proposed dataset is beneficial\nnot only to our proposed tasks but also in the general visual domain, while\nprevious datasets result in worse performance. We hope our work will contribute\nto the development of general MLLMs suitable for web-based content generation\nand task automation. Our data and code will be available at\nhttps://github.com/MBZUAI-LLM/web2code.\n","authors":["Sukmin Yun","Haokun Lin","Rusiru Thushara","Mohammad Qazim Bhat","Yongxin Wang","Zutao Jiang","Mingkai Deng","Jinhong Wang","Tianhua Tao","Junbo Li","Haonan Li","Preslav Nakov","Timothy Baldwin","Zhengzhong Liu","Eric P. Xing","Xiaodan Liang","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2406.20098v1.pdf","comment":"Website at https://mbzuai-llm.github.io/webpage2code/"},{"id":"http://arxiv.org/abs/2406.20095v1","updated":"2024-06-28T17:59:12Z","published":"2024-06-28T17:59:12Z","title":"LLaRA: Supercharging Robot Learning Data for Vision-Language Policy","summary":"  Large Language Models (LLMs) equipped with extensive world knowledge and\nstrong reasoning skills can tackle diverse tasks across domains, often by\nposing them as conversation-style instruction-response pairs. In this paper, we\npropose LLaRA: Large Language and Robotics Assistant, a framework which\nformulates robot action policy as conversations, and provides improved\nresponses when trained with auxiliary data that complements policy learning.\nLLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity\nto process state information as visual-textual prompts and generate optimal\npolicy decisions in text. To train such action policy VLMs, we first introduce\nan automated pipeline to generate diverse high-quality robotics instruction\ndata from existing behavior cloning data. A VLM finetuned with the resulting\ncollection of datasets based on a conversation-style formulation tailored for\nrobotics tasks, can generate meaningful robot action policy decisions. Our\nexperiments across multiple simulated and real-world environments demonstrate\nthe state-of-the-art performance of the proposed LLaRA framework. The code,\ndatasets, and pretrained models are available at\nhttps://github.com/LostXine/LLaRA.\n","authors":["Xiang Li","Cristina Mata","Jongwoo Park","Kumara Kahatapitiya","Yoo Sung Jang","Jinghuan Shang","Kanchana Ranasinghe","Ryan Burgert","Mu Cai","Yong Jae Lee","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2406.20095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20092v1","updated":"2024-06-28T17:57:14Z","published":"2024-06-28T17:57:14Z","title":"LLaVolta: Efficient Multi-modal Models via Stage-wise Visual Context\n  Compression","summary":"  While significant advancements have been made in compressed representations\nfor text embeddings in large language models (LLMs), the compression of visual\ntokens in large multi-modal models (LMMs) has remained a largely overlooked\narea. In this work, we present the study on the analysis of redundancy\nconcerning visual tokens and efficient training within these models. Our\ninitial experiments show that eliminating up to 70% of visual tokens at the\ntesting stage by simply average pooling only leads to a minimal 3% reduction in\nvisual question answering accuracy on the GQA benchmark, indicating significant\nredundancy in visual context. Addressing this, we introduce Visual Context\nCompressor, which reduces the number of visual tokens during training to\nenhance training efficiency without sacrificing performance. To minimize\ninformation loss caused by the compression on visual tokens while maintaining\ntraining efficiency, we develop LLaVolta as a lite training scheme. LLaVolta\nincorporates stage-wise visual context compression to progressively compress\nthe visual tokens from heavily to lightly, and finally no compression at the\nend of training, yielding no loss of information when testing. Extensive\nexperiments demonstrate that our approach enhances the performance of MLLMs in\nboth image-language and video-language understanding, while also significantly\ncutting training costs. Code is available at\nhttps://github.com/Beckschen/LLaVolta\n","authors":["Jieneng Chen","Luoxin Ye","Ju He","Zhao-Yang Wang","Daniel Khashabi","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2406.20092v1.pdf","comment":"Code is available at https://github.com/Beckschen/LLaVolta"},{"id":"http://arxiv.org/abs/2406.20085v1","updated":"2024-06-28T17:53:18Z","published":"2024-06-28T17:53:18Z","title":"Auto Cherry-Picker: Learning from High-quality Generative Data Driven by\n  Language","summary":"  Diffusion-based models have shown great potential in generating high-quality\nimages with various layouts, which can benefit downstream perception tasks.\nHowever, a fully automatic layout generation driven only by language and a\nsuitable metric for measuring multiple generated instances has not been well\nexplored. In this work, we present Auto Cherry-Picker (ACP), a novel framework\nthat generates high-quality multi-modal training examples to augment perception\nand multi-modal training. Starting with a simple list of natural language\nconcepts, we prompt large language models (LLMs) to generate a detailed\ndescription and design reasonable layouts. Next, we use an off-the-shelf\ntext-to-image model to generate multiple images. Then, the generated data are\nrefined using a comprehensively designed metric to ensure quality. In\nparticular, we present a new metric, Composite Layout and Image Score (CLIS),\nto evaluate the generated images fairly. Our synthetic high-quality examples\nboost performance in various scenarios by customizing the initial concept list,\nespecially in addressing challenges associated with long-tailed distribution\nand imbalanced datasets. Experiment results on downstream tasks demonstrate\nthat Auto Cherry-Picker can significantly improve the performance of existing\nmodels. In addition, we have thoroughly investigated the correlation between\nCLIS and performance gains in downstream tasks, and we find that a better CLIS\nscore results in better performance. This finding shows the potential for\nevaluation metrics as the role for various visual perception and MLLM tasks.\nCode will be available.\n","authors":["Yicheng Chen","Xiangtai Li","Yining Li","Yanhong Zeng","Jianzong Wu","Xiangyu Zhao","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.20085v1.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.20083v1","updated":"2024-06-28T17:51:10Z","published":"2024-06-28T17:51:10Z","title":"PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful\n  Navigators","summary":"  We present PoliFormer (Policy Transformer), an RGB-only indoor navigation\nagent trained end-to-end with reinforcement learning at scale that generalizes\nto the real-world without adaptation despite being trained purely in\nsimulation. PoliFormer uses a foundational vision transformer encoder with a\ncausal transformer decoder enabling long-term memory and reasoning. It is\ntrained for hundreds of millions of interactions across diverse environments,\nleveraging parallelized, multi-machine rollouts for efficient training with\nhigh throughput. PoliFormer is a masterful navigator, producing\nstate-of-the-art results across two distinct embodiments, the LoCoBot and\nStretch RE-1 robots, and four navigation benchmarks. It breaks through the\nplateaus of previous work, achieving an unprecedented 85.5% success rate in\nobject goal navigation on the CHORES-S benchmark, a 28.5% absolute improvement.\nPoliFormer can also be trivially extended to a variety of downstream\napplications such as object tracking, multi-object navigation, and\nopen-vocabulary navigation with no finetuning.\n","authors":["Kuo-Hao Zeng","Zichen Zhang","Kiana Ehsani","Rose Hendrix","Jordi Salvador","Alvaro Herrasti","Ross Girshick","Aniruddha Kembhavi","Luca Weihs"],"pdf_url":"https://arxiv.org/pdf/2406.20083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20081v1","updated":"2024-06-28T17:47:32Z","published":"2024-06-28T17:47:32Z","title":"Segment Anything without Supervision","summary":"  The Segmentation Anything Model (SAM) requires labor-intensive data labeling.\nWe present Unsupervised SAM (UnSAM) for promptable and automatic whole-image\nsegmentation that does not require human annotations. UnSAM utilizes a\ndivide-and-conquer strategy to \"discover\" the hierarchical structure of visual\nscenes. We first leverage top-down clustering methods to partition an unlabeled\nimage into instance/semantic level segments. For all pixels within a segment, a\nbottom-up clustering method is employed to iteratively merge them into larger\ngroups, thereby forming a hierarchical structure. These unsupervised\nmulti-granular masks are then utilized to supervise model training. Evaluated\nacross seven popular datasets, UnSAM achieves competitive results with the\nsupervised counterpart SAM, and surpasses the previous state-of-the-art in\nunsupervised segmentation by 11% in terms of AR. Moreover, we show that\nsupervised SAM can also benefit from our self-supervised labels. By integrating\nour unsupervised pseudo masks into SA-1B's ground-truth masks and training\nUnSAM with only 1% of SA-1B, a lightly semi-supervised UnSAM can often segment\nentities overlooked by supervised SAM, exceeding SAM's AR by over 6.7% and AP\nby 3.9% on SA-1B.\n","authors":["XuDong Wang","Jingfeng Yang","Trevor Darrell"],"pdf_url":"https://arxiv.org/pdf/2406.20081v1.pdf","comment":"Code: https://github.com/frank-xwang/UnSAM"},{"id":"http://arxiv.org/abs/2406.20078v1","updated":"2024-06-28T17:42:08Z","published":"2024-06-28T17:42:08Z","title":"GM-DF: Generalized Multi-Scenario Deepfake Detection","summary":"  Existing face forgery detection usually follows the paradigm of training\nmodels in a single domain, which leads to limited generalization capacity when\nunseen scenarios and unknown attacks occur. In this paper, we elaborately\ninvestigate the generalization capacity of deepfake detection models when\njointly trained on multiple face forgery detection datasets. We first find a\nrapid degradation of detection accuracy when models are directly trained on\ncombined datasets due to the discrepancy across collection scenarios and\ngeneration methods. To address the above issue, a Generalized Multi-Scenario\nDeepfake Detection framework (GM-DF) is proposed to serve multiple real-world\nscenarios by a unified model. First, we propose a hybrid expert modeling\napproach for domain-specific real/forgery feature extraction. Besides, as for\nthe commonality representation, we use CLIP to extract the common features for\nbetter aligning visual and textual features across domains. Meanwhile, we\nintroduce a masked image reconstruction mechanism to force models to capture\nrich forged details. Finally, we supervise the models via a domain-aware\nmeta-learning strategy to further enhance their generalization capacities.\nSpecifically, we design a novel domain alignment loss to strongly align the\ndistributions of the meta-test domains and meta-train domains. Thus, the\nupdated models are able to represent both specific and common real/forgery\nfeatures across multiple datasets. In consideration of the lack of study of\nmulti-dataset training, we establish a new benchmark leveraging multi-source\ndata to fairly evaluate the models' generalization capacity on unseen\nscenarios. Both qualitative and quantitative experiments on five datasets\nconducted on traditional protocols as well as the proposed benchmark\ndemonstrate the effectiveness of our approach.\n","authors":["Yingxin Lai","Zitong Yu","Jing Yang","Bin Li","Xiangui Kang","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2406.20078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20077v1","updated":"2024-06-28T17:39:38Z","published":"2024-06-28T17:39:38Z","title":"HouseCrafter: Lifting Floorplans to 3D Scenes with 2D Diffusion Model","summary":"  We introduce HouseCrafter, a novel approach that can lift a floorplan into a\ncomplete large 3D indoor scene (e.g., a house). Our key insight is to adapt a\n2D diffusion model, which is trained on web-scale images, to generate\nconsistent multi-view color (RGB) and depth (D) images across different\nlocations of the scene. Specifically, the RGB-D images are generated\nautoregressively in a batch-wise manner along sampled locations based on the\nfloorplan, where previously generated images are used as condition to the\ndiffusion model to produce images at nearby locations. The global floorplan and\nattention design in the diffusion model ensures the consistency of the\ngenerated images, from which a 3D scene can be reconstructed. Through extensive\nevaluation on the 3D-Front dataset, we demonstrate that HouseCraft can generate\nhigh-quality house-scale 3D scenes. Ablation studies also validate the\neffectiveness of different design choices. We will release our code and model\nweights. Project page: https://neu-vi.github.io/houseCrafter/\n","authors":["Hieu T. Nguyen","Yiwen Chen","Vikram Voleti","Varun Jampani","Huaizu Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.20077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20076v1","updated":"2024-06-28T17:38:18Z","published":"2024-06-28T17:38:18Z","title":"EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything\n  Model","summary":"  Segment Anything Model (SAM) has attracted widespread attention for its\nsuperior interactive segmentation capabilities with visual prompts while\nlacking further exploration of text prompts. In this paper, we empirically\ninvestigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting\nSAM for referring expression segmentation and introduce the Early\nVision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective\nreferring segmentation method which exploits multimodal prompts (i.e., image\nand text) and comprises a pre-trained vision-language model to generate\nreferring prompts and a SAM model for segmentation. Surprisingly, we observe\nthat: (1) multimodal prompts and (2) vision-language models with early fusion\n(e.g., BEIT-3) are beneficial for prompting SAM for accurate referring\nsegmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3\ncan obtain state-of-the-art performance on RefCOCO/+/g for referring expression\nsegmentation and demonstrate the superiority of prompting SAM with early\nvision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters\nachieves remarkably higher performance while reducing nearly 82% of parameters\ncompared to previous SAM methods based on large multimodal models.\n","authors":["Yuxuan Zhang","Tianheng Cheng","Rui Hu","ei Liu","Heng Liu","Longjin Ran","Xiaoxin Chen","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2406.20076v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.20066v1","updated":"2024-06-28T17:22:33Z","published":"2024-06-28T17:22:33Z","title":"ASSR-NeRF: Arbitrary-Scale Super-Resolution on Voxel Grid for\n  High-Quality Radiance Fields Reconstruction","summary":"  NeRF-based methods reconstruct 3D scenes by building a radiance field with\nimplicit or explicit representations. While NeRF-based methods can perform\nnovel view synthesis (NVS) at arbitrary scale, the performance in\nhigh-resolution novel view synthesis (HRNVS) with low-resolution (LR)\noptimization often results in oversmoothing. On the other hand, single-image\nsuper-resolution (SR) aims to enhance LR images to HR counterparts but lacks\nmulti-view consistency. To address these challenges, we propose Arbitrary-Scale\nSuper-Resolution NeRF (ASSR-NeRF), a novel framework for super-resolution novel\nview synthesis (SRNVS). We propose an attention-based VoxelGridSR model to\ndirectly perform 3D super-resolution (SR) on the optimized volume. Our model is\ntrained on diverse scenes to ensure generalizability. For unseen scenes trained\nwith LR views, we then can directly apply our VoxelGridSR to further refine the\nvolume and achieve multi-view consistent SR. We demonstrate quantitative and\nqualitatively that the proposed method achieves significant performance in\nSRNVS.\n","authors":["Ding-Jiun Huang","Zi-Ting Chou","Yu-Chiang Frank Wang","Cheng Sun"],"pdf_url":"https://arxiv.org/pdf/2406.20066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.07015v4","updated":"2024-06-28T17:14:13Z","published":"2023-05-11T17:55:25Z","title":"Exploiting Diffusion Prior for Real-World Image Super-Resolution","summary":"  We present a novel approach to leverage prior knowledge encapsulated in\npre-trained text-to-image diffusion models for blind super-resolution (SR).\nSpecifically, by employing our time-aware encoder, we can achieve promising\nrestoration results without altering the pre-trained synthesis model, thereby\npreserving the generative prior and minimizing training cost. To remedy the\nloss of fidelity caused by the inherent stochasticity of diffusion models, we\nemploy a controllable feature wrapping module that allows users to balance\nquality and fidelity by simply adjusting a scalar value during the inference\nprocess. Moreover, we develop a progressive aggregation sampling strategy to\novercome the fixed-size constraints of pre-trained diffusion models, enabling\nadaptation to resolutions of any size. A comprehensive evaluation of our method\nusing both synthetic and real-world benchmarks demonstrates its superiority\nover current state-of-the-art approaches. Code and models are available at\nhttps://github.com/IceClear/StableSR.\n","authors":["Jianyi Wang","Zongsheng Yue","Shangchen Zhou","Kelvin C. K. Chan","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2305.07015v4.pdf","comment":"Accepted by IJCV'2024. Some Figs are compressed due to size limits.\n  Uncompressed ver.:\n  https://github.com/IceClear/StableSR/releases/download/UncompressedPDF/StableSR_IJCV_Uncompressed.pdf.\n  Project page: https://iceclear.github.io/projects/stablesr/"},{"id":"http://arxiv.org/abs/2406.20055v1","updated":"2024-06-28T17:07:11Z","published":"2024-06-28T17:07:11Z","title":"SpotlessSplats: Ignoring Distractors in 3D Gaussian Splatting","summary":"  3D Gaussian Splatting (3DGS) is a promising technique for 3D reconstruction,\noffering efficient training and rendering speeds, making it suitable for\nreal-time applications.However, current methods require highly controlled\nenvironments (no moving people or wind-blown elements, and consistent lighting)\nto meet the inter-view consistency assumption of 3DGS. This makes\nreconstruction of real-world captures problematic. We present SpotlessSplats,\nan approach that leverages pre-trained and general-purpose features coupled\nwith robust optimization to effectively ignore transient distractors. Our\nmethod achieves state-of-the-art reconstruction quality both visually and\nquantitatively, on casual captures.\n","authors":["Sara Sabour","Lily Goli","George Kopanas","Mark Matthews","Dmitry Lagun","Leonidas Guibas","Alec Jacobson","David J. Fleet","Andrea Tagliasacchi"],"pdf_url":"https://arxiv.org/pdf/2406.20055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20042v1","updated":"2024-06-28T16:40:57Z","published":"2024-06-28T16:40:57Z","title":"HAITCH: A Framework for Distortion and Motion Correction in Fetal\n  Multi-Shell Diffusion-Weighted MRI","summary":"  Diffusion magnetic resonance imaging (dMRI) is pivotal for probing the\nmicrostructure of the rapidly-developing fetal brain. However, fetal motion\nduring scans and its interaction with magnetic field inhomogeneities result in\nartifacts and data scattering across spatial and angular domains. The effects\nof those artifacts are more pronounced in high-angular resolution fetal dMRI,\nwhere signal-to-noise ratio is very low. Those effects lead to biased estimates\nand compromise the consistency and reliability of dMRI analysis. This work\npresents HAITCH, the first and the only publicly available tool to correct and\nreconstruct multi-shell high-angular resolution fetal dMRI data. HAITCH offers\nseveral technical advances that include a blip-reversed dual-echo acquisition\nfor dynamic distortion correction, advanced motion correction for model-free\nand robust reconstruction, optimized multi-shell design for enhanced\ninformation capture and increased tolerance to motion, and outlier detection\nfor improved reconstruction fidelity. The framework is open-source, flexible,\nand can be used to process any type of fetal dMRI data including single-echo or\nsingle-shell acquisitions, but is most effective when used with multi-shell\nmulti-echo fetal dMRI data that cannot be processed with any of the existing\ntools. Validation experiments on real fetal dMRI scans demonstrate significant\nimprovements and accurate correction across diverse fetal ages and motion\nlevels. HAITCH successfully removes artifacts and reconstructs high-fidelity\nfetal dMRI data suitable for advanced diffusion modeling, including fiber\norientation distribution function estimation. These advancements pave the way\nfor more reliable analysis of the fetal brain microstructure and tractography\nunder challenging imaging conditions.\n","authors":["Haykel Snoussi","Davood Karimi","Onur Afacan","Mustafa Utkur","Ali Gholipour"],"pdf_url":"https://arxiv.org/pdf/2406.20042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15180v2","updated":"2024-06-28T16:40:39Z","published":"2023-07-27T20:19:11Z","title":"EnSolver: Uncertainty-Aware Ensemble CAPTCHA Solvers with Theoretical\n  Guarantees","summary":"  The popularity of text-based CAPTCHA as a security mechanism to protect\nwebsites from automated bots has prompted researches in CAPTCHA solvers, with\nthe aim of understanding its failure cases and subsequently making CAPTCHAs\nmore secure. Recently proposed solvers, built on advances in deep learning, are\nable to crack even the very challenging CAPTCHAs with high accuracy. However,\nthese solvers often perform poorly on out-of-distribution samples that contain\nvisual features different from those in the training set. Furthermore, they\nlack the ability to detect and avoid such samples, making them susceptible to\nbeing locked out by defense systems after a certain number of failed attempts.\nIn this paper, we propose EnSolver, a family of CAPTCHA solvers that use deep\nensemble uncertainty to detect and skip out-of-distribution CAPTCHAs, making it\nharder to be detected. We prove novel theoretical bounds on the effectiveness\nof our solvers and demonstrate their use with state-of-the-art CAPTCHA solvers.\nOur experiments show that the proposed approaches perform well when cracking\nCAPTCHA datasets that contain both in-distribution and out-of-distribution\nsamples.\n","authors":["Duc C. Hoang","Behzad Ousat","Amin Kharraz","Cuong V. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2307.15180v2.pdf","comment":"A previous version of this paper was presented at the Epistemic\n  Uncertainty - E-pi UAI 2023 Workshop"},{"id":"http://arxiv.org/abs/2406.20024v1","updated":"2024-06-28T16:13:55Z","published":"2024-06-28T16:13:55Z","title":"eMoE-Tracker: Environmental MoE-based Transformer for Robust\n  Event-guided Object Tracking","summary":"  The unique complementarity of frame-based and event cameras for high frame\nrate object tracking has recently inspired some research attempts to develop\nmulti-modal fusion approaches. However, these methods directly fuse both\nmodalities and thus ignore the environmental attributes, e.g., motion blur,\nillumination variance, occlusion, scale variation, etc. Meanwhile, no\ninteraction between search and template features makes distinguishing target\nobjects and backgrounds difficult. As a result, performance degradation is\ninduced especially in challenging conditions. This paper proposes a novel and\neffective Transformer-based event-guided tracking framework, called\neMoE-Tracker, which achieves new SOTA performance under various conditions. Our\nkey idea is to disentangle the environment into several learnable attributes to\ndynamically learn the attribute-specific features for better interaction and\ndiscriminability between the target information and background. To achieve the\ngoal, we first propose an environmental Mix-of-Experts (eMoE) module that is\nbuilt upon the environmental Attributes Disentanglement to learn\nattribute-specific features and environmental Attributes Gating to assemble the\nattribute-specific features by the learnable attribute scores dynamically. The\neMoE module is a subtle router that fine-tunes the transformer backbone more\nefficiently. We then introduce a contrastive relation modeling (CRM) module to\nimprove interaction and discriminability between the target information and\nbackground. Extensive experiments on diverse event-based benchmark datasets\nshowcase the superior performance of our eMoE-Tracker compared to the prior\narts.\n","authors":["Yucheng Chen","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.20024v1.pdf","comment":"RGB-event single object tracking"},{"id":"http://arxiv.org/abs/2406.20005v1","updated":"2024-06-28T15:44:55Z","published":"2024-06-28T15:44:55Z","title":"Malaria Cell Detection Using Deep Neural Networks","summary":"  Malaria remains one of the most pressing public health concerns globally,\ncausing significant morbidity and mortality, especially in sub-Saharan Africa.\nRapid and accurate diagnosis is crucial for effective treatment and disease\nmanagement. Traditional diagnostic methods, such as microscopic examination of\nblood smears, are labor-intensive and require significant expertise, which may\nnot be readily available in resource-limited settings. This project aims to\nautomate the detection of malaria-infected cells using a deep learning\napproach. We employed a convolutional neural network (CNN) based on the\nResNet50 architecture, leveraging transfer learning to enhance performance. The\nMalaria Cell Images Dataset from Kaggle, containing 27,558 images categorized\ninto infected and uninfected cells, was used for training and evaluation. Our\nmodel demonstrated high accuracy, precision, and recall, indicating its\npotential as a reliable tool for assisting in malaria diagnosis. Additionally,\na web application was developed using Streamlit to allow users to upload cell\nimages and receive predictions about malaria infection, making the technology\naccessible and user-friendly. This paper provides a comprehensive overview of\nthe methodology, experiments, and results, highlighting the effectiveness of\ndeep learning in medical image analysis.\n","authors":["Saurabh Sawant","Anurag Singh"],"pdf_url":"https://arxiv.org/pdf/2406.20005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00035v3","updated":"2024-06-28T15:42:12Z","published":"2024-01-08T12:19:46Z","title":"Robustness Assessment of a Runway Object Classifier for Safe Aircraft\n  Taxiing","summary":"  As deep neural networks (DNNs) are becoming the prominent solution for many\ncomputational problems, the aviation industry seeks to explore their potential\nin alleviating pilot workload and in improving operational safety. However, the\nuse of DNNs in this type of safety-critical applications requires a thorough\ncertification process. This need can be addressed through formal verification,\nwhich provides rigorous assurances -- e.g.,~by proving the absence of certain\nmispredictions. In this case-study paper, we demonstrate this process using an\nimage-classifier DNN currently under development at Airbus and intended for use\nduring the aircraft taxiing phase. We use formal methods to assess this DNN's\nrobustness to three common image perturbation types: noise, brightness and\ncontrast, and some of their combinations. This process entails multiple\ninvocations of the underlying verifier, which might be computationally\nexpensive; and we therefore propose a method that leverages the monotonicity of\nthese robustness properties, as well as the results of past verification\nqueries, in order to reduce the overall number of verification queries required\nby nearly 60%. Our results provide an indication of the level of robustness\nachieved by the DNN classifier under study, and indicate that it is\nconsiderably more vulnerable to noise than to brightness or contrast\nperturbations.\n","authors":["Yizhak Elboher","Raya Elsaleh","Omri Isac","Mélanie Ducoffe","Audrey Galametz","Guillaume Povéda","Ryma Boumazouza","Noémie Cohen","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2402.00035v3.pdf","comment":"This is a preprint version of the paper in the proceedings of 43rd\n  Digital Avionics Systems Conference (DASC)"},{"id":"http://arxiv.org/abs/2406.19997v1","updated":"2024-06-28T15:32:59Z","published":"2024-06-28T15:32:59Z","title":"Wavelets Are All You Need for Autoregressive Image Generation","summary":"  In this paper, we take a new approach to autoregressive image generation that\nis based on two main ingredients. The first is wavelet image coding, which\nallows to tokenize the visual details of an image from coarse to fine details\nby ordering the information starting with the most significant bits of the most\nsignificant wavelet coefficients. The second is a variant of a language\ntransformer whose architecture is re-designed and optimized for token sequences\nin this 'wavelet language'. The transformer learns the significant statistical\ncorrelations within a token sequence, which are the manifestations of\nwell-known correlations between the wavelet subbands at various resolutions. We\nshow experimental results with conditioning on the generation process.\n","authors":["Wael Mattar","Idan Levy","Nir Sharon","Shai Dekel"],"pdf_url":"https://arxiv.org/pdf/2406.19997v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.19973v1","updated":"2024-06-28T15:01:23Z","published":"2024-06-28T15:01:23Z","title":"STLLaVA-Med: Self-Training Large Language and Vision Assistant for\n  Medical","summary":"  Large Vision-Language Models (LVLMs) have shown significant potential in\nassisting medical diagnosis by leveraging extensive biomedical datasets.\nHowever, the advancement of medical image understanding and reasoning\ncritically depends on building high-quality visual instruction data, which is\ncostly and labor-intensive to obtain, particularly in the medical domain. To\nmitigate this data-starving issue, we introduce Self-Training Large Language\nand Vision Assistant for Medical (STLLaVA-Med). The proposed method is designed\nto train a policy model (an LVLM) capable of auto-generating medical visual\ninstruction data to improve data efficiency, guided through Direct Preference\nOptimization (DPO). Specifically, a more powerful and larger LVLM (e.g.,\nGPT-4o) is involved as a biomedical expert to oversee the DPO fine-tuning\nprocess on the auto-generated data, encouraging the policy model to align\nefficiently with human preferences. We validate the efficacy and data\nefficiency of STLLaVA-Med across three major medical Visual Question Answering\n(VQA) benchmarks, demonstrating competitive zero-shot performance with the\nutilization of only 9% of the medical data.\n","authors":["Guohao Sun","Can Qin","Huazhu Fu","Linwei Wang","Zhiqiang Tao"],"pdf_url":"https://arxiv.org/pdf/2406.19973v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.05779v3","updated":"2024-06-28T14:53:06Z","published":"2024-06-09T13:25:02Z","title":"Learning to utilize image second-order derivative information for crisp\n  edge detection","summary":"  Edge detection is a fundamental task in computer vision. It has made great\nprogress under the development of deep convolutional neural networks (DCNNs),\nsome of which have achieved a beyond human-level performance. However, recent\ntop-performing edge detection methods tend to generate thick and noisy edge\nlines. In this work, we solve this problem from two aspects: (1) the lack of\nprior knowledge regarding image edges, and (2) the issue of imbalanced pixel\ndistribution. We propose a second-order derivative-based multi-scale contextual\nenhancement module (SDMCM) to help the model locate true edge pixels accurately\nby introducing the edge prior knowledge. We also construct a hybrid focal loss\nfunction (HFL) to alleviate the imbalanced distribution issue. In addition, we\nemploy the conditionally parameterized convolution (CondConv) to develop a\nnovel boundary refinement module (BRM), which can further refine the final\noutput edge maps. In the end, we propose a U-shape network named LUS-Net which\nis based on the SDMCM and BRM for crisp edge detection. We perform extensive\nexperiments on three standard benchmarks, and the experiment results illustrate\nthat our method can predict crisp and clean edge maps and achieves\nstate-of-the-art performance on the BSDS500 dataset (ODS=0.829), NYUD-V2\ndataset (ODS=0.768), and BIPED dataset (ODS=0.903).\n","authors":["Changsong Liu","Wei Zhang","Yanyan Liu","Yimeng Fan","Mingyang Li","Wenlin Li"],"pdf_url":"https://arxiv.org/pdf/2406.05779v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17032v2","updated":"2024-06-28T14:34:00Z","published":"2024-06-24T18:00:11Z","title":"DWARF: Disease-weighted network for attention map refinement","summary":"  The interpretability of deep learning is crucial for evaluating the\nreliability of medical imaging models and reducing the risks of inaccurate\npatient recommendations. This study addresses the \"human out of the loop\" and\n\"trustworthiness\" issues in medical image analysis by integrating medical\nprofessionals into the interpretability process. We propose a disease-weighted\nattention map refinement network (DWARF) that leverages expert feedback to\nenhance model relevance and accuracy. Our method employs cyclic training to\niteratively improve diagnostic performance, generating precise and\ninterpretable feature maps. Experimental results demonstrate significant\nimprovements in interpretability and diagnostic accuracy across multiple\nmedical imaging datasets. This approach fosters effective collaboration between\nAI systems and healthcare professionals, ultimately aiming to improve patient\noutcomes\n","authors":["Haozhe Luo","Aurélie Pahud de Mortanges","Oana Inel","Abraham Bernstein","Mauricio Reyes"],"pdf_url":"https://arxiv.org/pdf/2406.17032v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19943v1","updated":"2024-06-28T14:22:30Z","published":"2024-06-28T14:22:30Z","title":"Impact of Initialization on Intra-subject Pediatric Brain MR Image\n  Registration: A Comparative Analysis between SyN ANTs and Deep Learning-Based\n  Approaches","summary":"  This study evaluates the performance of conventional SyN ANTs and\nlearning-based registration methods in the context of pediatric neuroimaging,\nspecifically focusing on intrasubject deformable registration. The comparison\ninvolves three approaches: without (NR), with rigid (RR), and with rigid and\naffine (RAR) initializations. In addition to initialization, performances are\nevaluated in terms of accuracy, speed, and the impact of age intervals and sex\nper pair. Data consists of the publicly available MRI scans from the Calgary\nPreschool dataset, which includes 63 children aged 2-7 years, allowing for 431\nregistration pairs. We implemented the unsupervised DL framework with a U-Net\narchitecture using DeepReg and it was 5-fold cross-validated. Evaluation\nincludes Dice scores for tissue segmentation from 18 smaller regions obtained\nby SynthSeg, analysis of log Jacobian determinants, and registration pro-rated\ntraining and inference times. Learning-based approaches, with or without linear\ninitializations, exhibit slight superiority over SyN ANTs in terms of Dice\nscores. Indeed, DL-based implementations with RR and RAR initializations\nsignificantly outperform SyN ANTs. Both SyN ANTs and DL-based registration\ninvolve parameter optimization, but the choice between these methods depends on\nthe scale of registration: network-based for broader coverage or SyN ANTs for\nspecific structures. Both methods face challenges with larger age intervals due\nto greater growth changes. The main takeaway is that while DL-based methods\nshow promise with faster and more accurate registrations, SyN ANTs remains\nrobust and generalizable without the need for extensive training, highlighting\nthe importance of method selection based on specific registration needs in the\npediatric context. Our code is available at\nhttps://github.com/neuropoly/pediatric-DL-registration\n","authors":["Andjela Dimitrijevic","Vincent Noblet","Benjamin De Leener"],"pdf_url":"https://arxiv.org/pdf/2406.19943v1.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:013"},{"id":"http://arxiv.org/abs/2406.19941v1","updated":"2024-06-28T14:17:16Z","published":"2024-06-28T14:17:16Z","title":"GRACE: Graph-Regularized Attentive Convolutional Entanglement with\n  Laplacian Smoothing for Robust DeepFake Video Detection","summary":"  As DeepFake video manipulation techniques escalate, posing profound threats,\nthe urgent need to develop efficient detection strategies is underscored.\nHowever, one particular issue lies with facial images being mis-detected, often\noriginating from degraded videos or adversarial attacks, leading to unexpected\ntemporal artifacts that can undermine the efficacy of DeepFake video detection\ntechniques. This paper introduces a novel method for robust DeepFake video\ndetection, harnessing the power of the proposed Graph-Regularized Attentive\nConvolutional Entanglement (GRACE) based on the graph convolutional network\nwith graph Laplacian to address the aforementioned challenges. First,\nconventional Convolution Neural Networks are deployed to perform spatiotemporal\nfeatures for the entire video. Then, the spatial and temporal features are\nmutually entangled by constructing a graph with sparse constraint, enforcing\nessential features of valid face images in the noisy face sequences remaining,\nthus augmenting stability and performance for DeepFake video detection.\nFurthermore, the Graph Laplacian prior is proposed in the graph convolutional\nnetwork to remove the noise pattern in the feature space to further improve the\nperformance. Comprehensive experiments are conducted to illustrate that our\nproposed method delivers state-of-the-art performance in DeepFake video\ndetection under noisy face sequences. The source code is available at\nhttps://github.com/ming053l/GRACE.\n","authors":["Chih-Chung Hsu","Shao-Ning Chen","Mei-Hsuan Wu","Yi-Fang Wang","Chia-Ming Lee","Yi-Shiuan Chou"],"pdf_url":"https://arxiv.org/pdf/2406.19941v1.pdf","comment":"Submitted to TPAMI 2024"},{"id":"http://arxiv.org/abs/2404.00548v2","updated":"2024-06-28T14:13:18Z","published":"2024-03-31T03:30:37Z","title":"Modeling State Shifting via Local-Global Distillation for Event-Frame\n  Gaze Tracking","summary":"  This paper tackles the problem of passive gaze estimation using both event\nand frame data. Considering the inherently different physiological structures,\nit is intractable to accurately estimate gaze purely based on a given state.\nThus, we reformulate gaze estimation as the quantification of the state\nshifting from the current state to several prior registered anchor states.\nSpecifically, we propose a two-stage learning-based gaze estimation framework\nthat divides the whole gaze estimation process into a coarse-to-fine approach\ninvolving anchor state selection and final gaze location. Moreover, to improve\nthe generalization ability, instead of learning a large gaze estimation network\ndirectly, we align a group of local experts with a student network, where a\nnovel denoising distillation algorithm is introduced to utilize denoising\ndiffusion techniques to iteratively remove inherent noise in event data.\nExtensive experiments demonstrate the effectiveness of the proposed method,\nwhich surpasses state-of-the-art methods by a large margin of 15$\\%$. The code\nwill be publicly available at\nhttps://github.com/jdjdli/Denoise_distill_EF_gazetracker.\n","authors":["Jiading Li","Zhiyu Zhu","Jinhui Hou","Junhui Hou","Jinjian Wu"],"pdf_url":"https://arxiv.org/pdf/2404.00548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00592v2","updated":"2024-06-28T14:02:06Z","published":"2023-12-01T13:56:28Z","title":"Tracking Object Positions in Reinforcement Learning: A Metric for\n  Keypoint Detection (extended version)","summary":"  Reinforcement learning (RL) for robot control typically requires a detailed\nrepresentation of the environment state, including information about\ntask-relevant objects not directly measurable. Keypoint detectors, such as\nspatial autoencoders (SAEs), are a common approach to extracting a\nlow-dimensional representation from high-dimensional image data. SAEs aim at\nspatial features such as object positions, which are often useful\nrepresentations in robotic RL. However, whether an SAE is actually able to\ntrack objects in the scene and thus yields a spatial state representation well\nsuited for RL tasks has rarely been examined due to a lack of established\nmetrics. In this paper, we propose to assess the performance of an SAE instance\nby measuring how well keypoints track ground truth objects in images. We\npresent a computationally lightweight metric and use it to evaluate common\nbaseline SAE architectures on image data from a simulated robot task. We find\nthat common SAEs differ substantially in their spatial extraction capability.\nFurthermore, we validate that SAEs that perform well in our metric achieve\nsuperior performance when used in downstream RL. Thus, our metric is an\neffective and lightweight indicator of RL performance before executing\nexpensive RL training. Building on these insights, we identify three key\nmodifications of SAE architectures to improve tracking performance. We make our\ncode available at anonymous.4open.science/r/sae-rl.\n","authors":["Emma Cramer","Jonas Reiher","Sebastian Trimpe"],"pdf_url":"https://arxiv.org/pdf/2312.00592v2.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2406.19922v1","updated":"2024-06-28T13:51:59Z","published":"2024-06-28T13:51:59Z","title":"Parallax-tolerant Image Stitching via Segmentation-guided\n  Multi-homography Warping","summary":"  Large parallax between images is an intractable issue in image stitching.\nVarious warping-based methods are proposed to address it, yet the results are\nunsatisfactory. In this paper, we propose a novel image stitching method using\nmulti-homography warping guided by image segmentation. Specifically, we\nleverage the Segment Anything Model to segment the target image into numerous\ncontents and partition the feature points into multiple subsets via the\nenergy-based multi-homography fitting algorithm. The multiple subsets of\nfeature points are used to calculate the corresponding multiple homographies.\nFor each segmented content in the overlapping region, we select its\nbest-fitting homography with the lowest photometric error. For each segmented\ncontent in the non-overlapping region, we calculate a weighted combination of\nthe linearized homographies. Finally, the target image is warped via the\nbest-fitting homographies to align with the reference image, and the final\npanorama is generated via linear blending. Comprehensive experimental results\non the public datasets demonstrate that our method provides the best alignment\naccuracy by a large margin, compared with the state-of-the-art methods. The\nsource code is available at https://github.com/tlliao/multi-homo-warp.\n","authors":["Tianli Liao","Ce Wang","Lei Li","Guangen Liu","Nan Li"],"pdf_url":"https://arxiv.org/pdf/2406.19922v1.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.19905v1","updated":"2024-06-28T13:20:17Z","published":"2024-06-28T13:20:17Z","title":"Solving Token Gradient Conflict in Mixture-of-Experts for Large\n  Vision-Language Model","summary":"  The Mixture-of-Experts (MoE) has gained increasing attention in the study of\nLarge Vision-Language Models (LVLMs). It uses a sparse model to replace the\ndense model, achieving comparable performance while activating fewer parameters\nduring inference, thus significantly reducing the inference cost. Existing MoE\nmethods in LVLMs encourage different experts to handle different tokens, and\nthus they employ a router to predict the routing for each token. However, the\npredictions are based solely on sample features and do not truly reveal the\noptimization direction of tokens. This can lead to severe optimization\nconflicts between different tokens within an expert. To address this problem,\nthis paper proposes a novel method based on token-level gradient analysis.\nSpecifically, we first use token-level gradients to identify conflicting tokens\nin experts. Then, we add a specialized loss tailored to eliminate conflicts\namong tokens within each expert. Our method can serve as a plug-in for diverse\nLarge Vision-Language Models, and extensive experimental results demonstrate\nthe effectiveness of our method. The code will be publicly available at\nhttps://github.com/longrongyang/STGC.\n","authors":["Longrong Yang","Dong Sheng","Chaoxiang Cai","Fan Yang","Size Li","Di Zhang","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2406.19905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14862v3","updated":"2024-06-28T13:19:37Z","published":"2024-06-21T04:39:03Z","title":"LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models","summary":"  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\nLatentExplainer, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\nLatentExplainer tackles three main challenges: inferring the meaning of latent\nvariables, aligning explanations with inductive biases, and handling varying\ndegrees of explainability. By perturbing latent variables and interpreting\nchanges in generated data, the framework provides a systematic approach to\nunderstanding and controlling the data generation process, enhancing the\ntransparency and interpretability of deep generative models. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations of\nlatent variables.\n","authors":["Mengdan Zhu","Raasikh Kanjiani","Jiahui Lu","Andrew Choi","Qirui Ye","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.14862v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19899v1","updated":"2024-06-28T13:07:43Z","published":"2024-06-28T13:07:43Z","title":"On the Value of PHH3 for Mitotic Figure Detection on H&E-stained Images","summary":"  The count of mitotic figures (MFs) observed in hematoxylin and eosin\n(H&E)-stained slides is an important prognostic marker as it is a measure for\ntumor cell proliferation. However, the identification of MFs has a known low\ninter-rater agreement. Deep learning algorithms can standardize this task, but\nthey require large amounts of annotated data for training and validation.\nFurthermore, label noise introduced during the annotation process may impede\nthe algorithm's performance. Unlike H&E, the mitosis-specific antibody\nphospho-histone H3 (PHH3) specifically highlights MFs. Counting MFs on slides\nstained against PHH3 leads to higher agreement among raters and has therefore\nrecently been used as a ground truth for the annotation of MFs in H&E. However,\nas PHH3 facilitates the recognition of cells indistinguishable from H&E stain\nalone, the use of this ground truth could potentially introduce noise into the\nH&E-related dataset, impacting model performance. This study analyzes the\nimpact of PHH3-assisted MF annotation on inter-rater reliability and object\nlevel agreement through an extensive multi-rater experiment. We found that the\nannotators' object-level agreement increased when using PHH3-assisted labeling.\nSubsequently, MF detectors were evaluated on the resulting datasets to\ninvestigate the influence of PHH3-assisted labeling on the models' performance.\nAdditionally, a novel dual-stain MF detector was developed to investigate the\ninterpretation-shift of PHH3-assisted labels used in H&E, which clearly\noutperformed single-stain detectors. However, the PHH3-assisted labels did not\nhave a positive effect on solely H&E-based models. The high performance of our\ndual-input detector reveals an information mismatch between the H&E and\nPHH3-stained images as the cause of this effect.\n","authors":["Jonathan Ganz","Christian Marzahl","Jonas Ammeling","Barbara Richter","Chloé Puget","Daniela Denk","Elena A. Demeter","Flaviu A. Tabaran","Gabriel Wasinger","Karoline Lipnik","Marco Tecilla","Matthew J. Valentine","Michael J. Dark","Niklas Abele","Pompei Bolfa","Ramona Erber","Robert Klopfleisch","Sophie Merz","Taryn A. Donovan","Samir Jabari","Christof A. Bertram","Katharina Breininger","Marc Aubreville"],"pdf_url":"https://arxiv.org/pdf/2406.19899v1.pdf","comment":"10 pages, 5 figures, 1 Table"},{"id":"http://arxiv.org/abs/2406.19875v1","updated":"2024-06-28T12:35:01Z","published":"2024-06-28T12:35:01Z","title":"InfiniBench: A Comprehensive Benchmark for Large Multimodal Models in\n  Very Long Video Understanding","summary":"  Understanding long videos, ranging from tens of minutes to several hours,\npresents unique challenges in video comprehension. Despite the increasing\nimportance of long-form video content, existing benchmarks primarily focus on\nshorter clips. To address this gap, we introduce InfiniBench a comprehensive\nbenchmark for very long video understanding which presents 1)The longest video\nduration, averaging 76.34 minutes; 2) The largest number of question-answer\npairs, 108.2K; 3) Diversity in questions that examine nine different skills and\ninclude both multiple-choice questions and open-ended questions; 4)\nHumancentric, as the video sources come from movies and daily TV shows, with\nspecific human-level question designs such as Movie Spoiler Questions that\nrequire critical thinking and comprehensive understanding. Using InfiniBench,\nwe comprehensively evaluate existing Large MultiModality Models (LMMs) on each\nskill, including the commercial model Gemini 1.5 Flash and the open-source\nmodels. The evaluation shows significant challenges in our benchmark.Our\nresults show that the best AI models such Gemini struggles to perform well with\n42.72% average accuracy and 2.71 out of 5 average score. We hope this benchmark\nwill stimulate the LMMs community towards long video and human-level\nunderstanding. Our benchmark can be accessed at\nhttps://vision-cair.github.io/InfiniBench/\n","authors":["Kirolos Ataallah","Chenhui Gou","Eslam Abdelrahman","Khushbu Pahwa","Jian Ding","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2406.19875v1.pdf","comment":"16 page ,17 figures"},{"id":"http://arxiv.org/abs/2406.11252v2","updated":"2024-06-28T11:59:01Z","published":"2024-06-17T06:28:58Z","title":"Mining Open Semantics from CLIP: A Relation Transition Perspective for\n  Few-Shot Learning","summary":"  Contrastive Vision-Language Pre-training(CLIP) demonstrates impressive\nzero-shot capability. The key to improve the adaptation of CLIP to downstream\ntask with few exemplars lies in how to effectively model and transfer the\nuseful knowledge embedded in CLIP. Previous work mines the knowledge typically\nbased on the limited visual samples and close-set semantics (i.e., within\ntarget category set of downstream task). However, the aligned CLIP image/text\nencoders contain abundant relationships between visual features and almost\ninfinite open semantics, which may benefit the few-shot learning but remains\nunexplored. In this paper, we propose to mine open semantics as anchors to\nperform a relation transition from image-anchor relationship to image-target\nrelationship to make predictions. Specifically, we adopt a transformer module\nwhich takes the visual feature as \"Query\", the text features of the anchors as\n\"Key\" and the similarity matrix between the text features of anchor and target\nclasses as \"Value\". In this way, the output of such a transformer module\nrepresents the relationship between the image and target categories, i.e., the\nclassification predictions. To avoid manually selecting the open semantics, we\nmake the [CLASS] token of input text embedding learnable. We conduct extensive\nexperiments on eleven representative classification datasets. The results show\nthat our method performs favorably against previous state-of-the-arts\nconsidering few-shot classification settings.\n","authors":["Cilin Yan","Haochen Wang","Xiaolong Jiang","Yao Hu","Xu Tang","Guoliang Kang","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2406.11252v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19852v1","updated":"2024-06-28T11:49:59Z","published":"2024-06-28T11:49:59Z","title":"FootBots: A Transformer-based Architecture for Motion Prediction in\n  Soccer","summary":"  Motion prediction in soccer involves capturing complex dynamics from player\nand ball interactions. We present FootBots, an encoder-decoder\ntransformer-based architecture addressing motion prediction and conditioned\nmotion prediction through equivariance properties. FootBots captures temporal\nand social dynamics using set attention blocks and multi-attention block\ndecoder. Our evaluation utilizes two datasets: a real soccer dataset and a\ntailored synthetic one. Insights from the synthetic dataset highlight the\neffectiveness of FootBots' social attention mechanism and the significance of\nconditioned motion prediction. Empirical results on real soccer data\ndemonstrate that FootBots outperforms baselines in motion prediction and excels\nin conditioned tasks, such as predicting the players based on the ball\nposition, predicting the offensive (defensive) team based on the ball and the\ndefensive (offensive) team, and predicting the ball position based on all\nplayers. Our evaluation connects quantitative and qualitative findings.\nhttps://youtu.be/9kaEkfzG3L8\n","authors":["Guillem Capellera","Luis Ferraz","Antonio Rubio","Antonio Agudo","Francesc Moreno-Noguer"],"pdf_url":"https://arxiv.org/pdf/2406.19852v1.pdf","comment":"Published as a conference paper at IEEE ICIP 2024"},{"id":"http://arxiv.org/abs/2406.19844v1","updated":"2024-06-28T11:35:35Z","published":"2024-06-28T11:35:35Z","title":"StreamMOTP: Streaming and Unified Framework for Joint 3D Multi-Object\n  Tracking and Trajectory Prediction","summary":"  3D multi-object tracking and trajectory prediction are two crucial modules in\nautonomous driving systems. Generally, the two tasks are handled separately in\ntraditional paradigms and a few methods have started to explore modeling these\ntwo tasks in a joint manner recently. However, these approaches suffer from the\nlimitations of single-frame training and inconsistent coordinate\nrepresentations between tracking and prediction tasks. In this paper, we\npropose a streaming and unified framework for joint 3D Multi-Object Tracking\nand trajectory Prediction (StreamMOTP) to address the above challenges.\nFirstly, we construct the model in a streaming manner and exploit a memory bank\nto preserve and leverage the long-term latent features for tracked objects more\neffectively. Secondly, a relative spatio-temporal positional encoding strategy\nis introduced to bridge the gap of coordinate representations between the two\ntasks and maintain the pose-invariance for trajectory prediction. Thirdly, we\nfurther improve the quality and consistency of predicted trajectories with a\ndual-stream predictor. We conduct extensive experiments on popular nuSences\ndataset and the experimental results demonstrate the effectiveness and\nsuperiority of StreamMOTP, which outperforms previous methods significantly on\nboth tasks. Furthermore, we also prove that the proposed framework has great\npotential and advantages in actual applications of autonomous driving.\n","authors":["Jiaheng Zhuang","Guoan Wang","Siyu Zhang","Xiyang Wang","Hangning Zhou","Ziyao Xu","Chi Zhang","Zhiheng Li"],"pdf_url":"https://arxiv.org/pdf/2406.19844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03511v3","updated":"2024-06-28T11:23:11Z","published":"2023-12-06T14:13:38Z","title":"Kandinsky 3.0 Technical Report","summary":"  We present Kandinsky 3.0, a large-scale text-to-image generation model based\non latent diffusion, continuing the series of text-to-image Kandinsky models\nand reflecting our progress to achieve higher quality and realism of image\ngeneration. In this report we describe the architecture of the model, the data\ncollection procedure, the training technique, and the production system for\nuser interaction. We focus on the key components that, as we have identified as\na result of a large number of experiments, had the most significant impact on\nimproving the quality of our model compared to the others. We also describe\nextensions and applications of our model, including super resolution,\ninpainting, image editing, image-to-video generation, and a distilled version\nof Kandinsky 3.0 - Kandinsky 3.1, which does inference in 4 steps of the\nreverse process and 20 times faster without visual quality decrease. By\nside-by-side human preferences comparison, Kandinsky becomes better in text\nunderstanding and works better on specific domains. The code is available at\nhttps://github.com/ai-forever/Kandinsky-3\n","authors":["Vladimir Arkhipkin","Andrei Filatov","Viacheslav Vasilev","Anastasia Maltseva","Said Azizov","Igor Pavlov","Julia Agafonova","Andrey Kuznetsov","Denis Dimitrov"],"pdf_url":"https://arxiv.org/pdf/2312.03511v3.pdf","comment":"Project page: https://ai-forever.github.io/Kandinsky-3"},{"id":"http://arxiv.org/abs/2406.19833v1","updated":"2024-06-28T11:11:24Z","published":"2024-06-28T11:11:24Z","title":"LightStereo: Channel Boost Is All Your Need for Efficient 2D Cost\n  Aggregation","summary":"  We present LightStereo, a cutting-edge stereo-matching network crafted to\naccelerate the matching process. Departing from conventional methodologies that\nrely on aggregating computationally intensive 4D costs, LightStereo adopts the\n3D cost volume as a lightweight alternative. While similar approaches have been\nexplored previously, our breakthrough lies in enhancing performance through a\ndedicated focus on the channel dimension of the 3D cost volume, where the\ndistribution of matching costs is encapsulated. Our exhaustive exploration has\nyielded plenty of strategies to amplify the capacity of the pivotal dimension,\nensuring both precision and efficiency. We compare the proposed LightStereo\nwith existing state-of-the-art methods across various benchmarks, which\ndemonstrate its superior performance in speed, accuracy, and resource\nutilization. LightStereo achieves a competitive EPE metric in the SceneFlow\ndatasets while demanding a minimum of only 22 GFLOPs, with an inference time of\njust 17 ms. Our comprehensive analysis reveals the effect of 2D cost\naggregation for stereo matching, paving the way for real-world applications of\nefficient stereo systems. Code will be available at\n\\url{https://github.com/XiandaGuo/OpenStereo}.\n","authors":["Xianda Guo","Chenming Zhang","Dujun Nie","Wenzhao Zheng","Youmin Zhang","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19833v1.pdf","comment":"Code will be available at\n  \\url{https://github.com/XiandaGuo/OpenStereo}"},{"id":"http://arxiv.org/abs/2406.19815v1","updated":"2024-06-28T10:45:37Z","published":"2024-06-28T10:45:37Z","title":"Emotion Loss Attacking: Adversarial Attack Perception for Skeleton based\n  on Multi-dimensional Features","summary":"  Adversarial attack on skeletal motion is a hot topic. However, existing\nresearches only consider part of dynamic features when measuring distance\nbetween skeleton graph sequences, which results in poor imperceptibility. To\nthis end, we propose a novel adversarial attack method to attack action\nrecognizers for skeletal motions. Firstly, our method systematically proposes a\ndynamic distance function to measure the difference between skeletal motions.\nMeanwhile, we innovatively introduce emotional features for complementary\ninformation. In addition, we use Alternating Direction Method of\nMultipliers(ADMM) to solve the constrained optimization problem, which\ngenerates adversarial samples with better imperceptibility to deceive the\nclassifiers. Experiments show that our method is effective on multiple action\nclassifiers and datasets. When the perturbation magnitude measured by l norms\nis the same, the dynamic perturbations generated by our method are much lower\nthan that of other methods. What's more, we are the first to prove the\neffectiveness of emotional features, and provide a new idea for measuring the\ndistance between skeletal motions.\n","authors":["Feng Liu","Qing Xu","Qijian Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.19815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19814v1","updated":"2024-06-28T10:45:25Z","published":"2024-06-28T10:45:25Z","title":"Extract More from Less: Efficient Fine-Grained Visual Recognition in\n  Low-Data Regimes","summary":"  The emerging task of fine-grained image classification in low-data regimes\nassumes the presence of low inter-class variance and large intra-class\nvariation along with a highly limited amount of training samples per class.\nHowever, traditional ways of separately dealing with fine-grained\ncategorisation and extremely scarce data may be inefficient under both these\nharsh conditions presented together. In this paper, we present a novel\nframework, called AD-Net, aiming to enhance deep neural network performance on\nthis challenge by leveraging the power of Augmentation and Distillation\ntechniques. Specifically, our approach is designed to refine learned features\nthrough self-distillation on augmented samples, mitigating harmful overfitting.\nWe conduct comprehensive experiments on popular fine-grained image\nclassification benchmarks where our AD-Net demonstrates consistent improvement\nover traditional fine-tuning and state-of-the-art low-data techniques.\nRemarkably, with the smallest data available, our framework shows an\noutstanding relative accuracy increase of up to 45 % compared to standard\nResNet-50 and up to 27 % compared to the closest SOTA runner-up. We emphasise\nthat our approach is practically architecture-independent and adds zero extra\ncost at inference time. Additionally, we provide an extensive study on the\nimpact of every framework's component, highlighting the importance of each in\nachieving optimal performance. Source code and trained models are publicly\navailable at github.com/demidovd98/fgic_lowd.\n","authors":["Dmitry Demidov","Abduragim Shtanchaev","Mihail Mihaylov","Mohammad Almansoori"],"pdf_url":"https://arxiv.org/pdf/2406.19814v1.pdf","comment":"Main paper and Appendices"},{"id":"http://arxiv.org/abs/2406.19811v1","updated":"2024-06-28T10:39:36Z","published":"2024-06-28T10:39:36Z","title":"EgoGaussian: Dynamic Scene Understanding from Egocentric Video with 3D\n  Gaussian Splatting","summary":"  Human activities are inherently complex, and even simple household tasks\ninvolve numerous object interactions. To better understand these activities and\nbehaviors, it is crucial to model their dynamic interactions with the\nenvironment. The recent availability of affordable head-mounted cameras and\negocentric data offers a more accessible and efficient means to understand\ndynamic human-object interactions in 3D environments. However, most existing\nmethods for human activity modeling either focus on reconstructing 3D models of\nhand-object or human-scene interactions or on mapping 3D scenes, neglecting\ndynamic interactions with objects. The few existing solutions often require\ninputs from multiple sources, including multi-camera setups, depth-sensing\ncameras, or kinesthetic sensors. To this end, we introduce EgoGaussian, the\nfirst method capable of simultaneously reconstructing 3D scenes and dynamically\ntracking 3D object motion from RGB egocentric input alone. We leverage the\nuniquely discrete nature of Gaussian Splatting and segment dynamic interactions\nfrom the background. Our approach employs a clip-level online learning pipeline\nthat leverages the dynamic nature of human activities, allowing us to\nreconstruct the temporal evolution of the scene in chronological order and\ntrack rigid object motion. Additionally, our method automatically segments\nobject and background Gaussians, providing 3D representations for both static\nscenes and dynamic objects. EgoGaussian outperforms previous NeRF and Dynamic\nGaussian methods in challenging in-the-wild videos and we also qualitatively\ndemonstrate the high quality of the reconstructed models.\n","authors":["Daiwei Zhang","Gengyan Li","Jiajie Li","Mickaël Bressieux","Otmar Hilliges","Marc Pollefeys","Luc Van Gool","Xi Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19796v1","updated":"2024-06-28T10:05:58Z","published":"2024-06-28T10:05:58Z","title":"Comprehensive Generative Replay for Task-Incremental Segmentation with\n  Concurrent Appearance and Semantic Forgetting","summary":"  Generalist segmentation models are increasingly favored for diverse tasks\ninvolving various objects from different image sources. Task-Incremental\nLearning (TIL) offers a privacy-preserving training paradigm using tasks\narriving sequentially, instead of gathering them due to strict data sharing\npolicies. However, the task evolution can span a wide scope that involves\nshifts in both image appearance and segmentation semantics with intricate\ncorrelation, causing concurrent appearance and semantic forgetting. To solve\nthis issue, we propose a Comprehensive Generative Replay (CGR) framework that\nrestores appearance and semantic knowledge by synthesizing image-mask pairs to\nmimic past task data, which focuses on two aspects: modeling image-mask\ncorrespondence and promoting scalability for diverse tasks. Specifically, we\nintroduce a novel Bayesian Joint Diffusion (BJD) model for high-quality\nsynthesis of image-mask pairs with their correspondence explicitly preserved by\nconditional denoising. Furthermore, we develop a Task-Oriented Adapter (TOA)\nthat recalibrates prompt embeddings to modulate the diffusion model, making the\ndata synthesis compatible with different tasks. Experiments on incremental\ntasks (cardiac, fundus and prostate segmentation) show its clear advantage for\nalleviating concurrent appearance and semantic forgetting. Code is available at\nhttps://github.com/jingyzhang/CGR.\n","authors":["Wei Li","Jingyang Zhang","Pheng-Ann Heng","Lixu Gu"],"pdf_url":"https://arxiv.org/pdf/2406.19796v1.pdf","comment":"Accepted by MICCAI24"},{"id":"http://arxiv.org/abs/2404.09666v2","updated":"2024-06-28T09:25:25Z","published":"2024-04-15T10:57:16Z","title":"Deformable MRI Sequence Registration for AI-based Prostate Cancer\n  Diagnosis","summary":"  The PI-CAI (Prostate Imaging: Cancer AI) challenge led to expert-level\ndiagnostic algorithms for clinically significant prostate cancer detection. The\nalgorithms receive biparametric MRI scans as input, which consist of\nT2-weighted and diffusion-weighted scans. These scans can be misaligned due to\nmultiple factors in the scanning process. Image registration can alleviate this\nissue by predicting the deformation between the sequences. We investigate the\neffect of image registration on the diagnostic performance of AI-based prostate\ncancer diagnosis. First, the image registration algorithm, developed in\nMeVisLab, is analyzed using a dataset with paired lesion annotations. Second,\nthe effect on diagnosis is evaluated by comparing case-level cancer diagnosis\nperformance between using the original dataset, rigidly aligned\ndiffusion-weighted scans, or deformably aligned diffusion-weighted scans. Rigid\nregistration showed no improvement. Deformable registration demonstrated a\nsubstantial improvement in lesion overlap (+10% median Dice score) and a\npositive yet non-significant improvement in diagnostic performance (+0.3%\nAUROC, p=0.18). Our investigation shows that a substantial improvement in\nlesion alignment does not directly lead to a significant improvement in\ndiagnostic performance. Qualitative analysis indicated that jointly developing\nimage registration methods and diagnostic AI algorithms could enhance\ndiagnostic accuracy and patient outcomes.\n","authors":["Alessa Hering","Sarah de Boer","Anindo Saha","Jasper J. Twilt","Mattias P. Heinrich","Derya Yakar","Maarten de Rooij","Henkjan Huisman","Joeran S. Bosma"],"pdf_url":"https://arxiv.org/pdf/2404.09666v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15613v2","updated":"2024-06-28T09:22:38Z","published":"2024-05-24T14:58:51Z","title":"Automatic Data Curation for Self-Supervised Learning: A Clustering-Based\n  Approach","summary":"  Self-supervised features are the cornerstone of modern machine learning\nsystems. They are typically pre-trained on data collections whose construction\nand curation typically require extensive human effort. This manual process has\nsome limitations similar to those encountered in supervised learning, e.g., the\ncrowd-sourced selection of data is costly and time-consuming, preventing\nscaling the dataset size. In this work, we consider the problem of automatic\ncuration of high-quality datasets for self-supervised pre-training. We posit\nthat such datasets should be large, diverse and balanced, and propose a\nclustering-based approach for building ones satisfying all these criteria. Our\nmethod involves successive and hierarchical applications of $k$-means on a\nlarge and diverse data repository to obtain clusters that distribute uniformly\namong data concepts, followed by a hierarchical, balanced sampling step from\nthese clusters. Extensive experiments on three different data domains including\nweb-based images, satellite images and text show that features trained on our\nautomatically curated datasets outperform those trained on uncurated data while\nbeing on par or better than ones trained on manually curated data. Code is\navailable at https://github.com/facebookresearch/ssl-data-curation.\n","authors":["Huy V. Vo","Vasil Khalidov","Timothée Darcet","Théo Moutakanni","Nikita Smetanin","Marc Szafraniec","Hugo Touvron","Camille Couprie","Maxime Oquab","Armand Joulin","Hervé Jégou","Patrick Labatut","Piotr Bojanowski"],"pdf_url":"https://arxiv.org/pdf/2405.15613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19336v2","updated":"2024-06-28T09:20:01Z","published":"2024-06-27T17:10:10Z","title":"LiverUSRecon: Automatic 3D Reconstruction and Volumetry of the Liver\n  with a Few Partial Ultrasound Scans","summary":"  3D reconstruction of the liver for volumetry is important for qualitative\nanalysis and disease diagnosis. Liver volumetry using ultrasound (US) scans,\nalthough advantageous due to less acquisition time and safety, is challenging\ndue to the inherent noisiness in US scans, blurry boundaries, and partial liver\nvisibility. We address these challenges by using the segmentation masks of a\nfew incomplete sagittal-plane US scans of the liver in conjunction with a\nstatistical shape model (SSM) built using a set of CT scans of the liver. We\ncompute the shape parameters needed to warp this canonical SSM to fit the US\nscans through a parametric regression network. The resulting 3D liver\nreconstruction is accurate and leads to automatic liver volume calculation. We\nevaluate the accuracy of the estimated liver volumes with respect to CT\nsegmentation volumes using RMSE. Our volume computation is statistically much\ncloser to the volume estimated using CT scans than the volume computed using\nChilds' method by radiologists: p-value of 0.094 (>0.05) says that there is no\nsignificant difference between CT segmentation volumes and ours in contrast to\nChilds' method. We validate our method using investigations (ablation studies)\non the US image resolution, the number of CT scans used for SSM, the number of\nprincipal components, and the number of input US scans. To the best of our\nknowledge, this is the first automatic liver volumetry system using a few\nincomplete US scans given a set of CT scans of livers for SSM.\n","authors":["Kaushalya Sivayogaraj","Sahan T. Guruge","Udari Liyanage","Jeevani Udupihille","Saroj Jayasinghe","Gerard Fernando","Ranga Rodrigo","M. Rukshani Liyanaarachchi"],"pdf_url":"https://arxiv.org/pdf/2406.19336v2.pdf","comment":"10 pages, Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2304.10839v4","updated":"2024-06-28T09:01:03Z","published":"2023-04-21T09:30:22Z","title":"Cross-domain Denoising for Low-dose Multi-frame Spiral Computed\n  Tomography","summary":"  Computed tomography (CT) has been used worldwide as a non-invasive test to\nassist in diagnosis. However, the ionizing nature of X-ray exposure raises\nconcerns about potential health risks such as cancer. The desire for lower\nradiation doses has driven researchers to improve reconstruction quality.\nAlthough previous studies on low-dose computed tomography (LDCT) denoising have\ndemonstrated the effectiveness of learning-based methods, most were developed\non the simulated data. However, the real-world scenario differs significantly\nfrom the simulation domain, especially when using the multi-slice spiral\nscanner geometry. This paper proposes a two-stage method for the commercially\navailable multi-slice spiral CT scanners that better exploits the complete\nreconstruction pipeline for LDCT denoising across different domains. Our\napproach makes good use of the high redundancy of multi-slice projections and\nthe volumetric reconstructions while leveraging the over-smoothing problem in\nconventional cascaded frameworks caused by aggressive denoising. The dedicated\ndesign also provides a more explicit interpretation of the data flow. Extensive\nexperiments on various datasets showed that the proposed method could remove up\nto 70\\% of noise without compromised spatial resolution, and subjective\nevaluations by two experienced radiologists further supported its superior\nperformance against state-of-the-art methods in clinical practice.\n","authors":["Yucheng Lu","Zhixin Xu","Moon Hyung Choi","Jimin Kim","Seung-Won Jung"],"pdf_url":"https://arxiv.org/pdf/2304.10839v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19756v1","updated":"2024-06-28T08:54:44Z","published":"2024-06-28T08:54:44Z","title":"Structure-aware World Model for Probe Guidance via Large-scale\n  Self-supervised Pre-train","summary":"  The complex structure of the heart leads to significant challenges in\nechocardiography, especially in acquisition cardiac ultrasound images.\nSuccessful echocardiography requires a thorough understanding of the structures\non the two-dimensional plane and the spatial relationships between planes in\nthree-dimensional space. In this paper, we innovatively propose a large-scale\nself-supervised pre-training method to acquire a cardiac structure-aware world\nmodel. The core innovation lies in constructing a self-supervised task that\nrequires structural inference by predicting masked structures on a 2D plane and\nimagining another plane based on pose transformation in 3D space. To support\nlarge-scale pre-training, we collected over 1.36 million echocardiograms from\nten standard views, along with their 3D spatial poses. In the downstream probe\nguidance task, we demonstrate that our pre-trained model consistently reduces\nguidance errors across the ten most common standard views on the test set with\n0.29 million samples from 74 routine clinical scans, indicating that\nstructure-aware pre-training benefits the scanning.\n","authors":["Haojun Jiang","Meng Li","Zhenguo Sun","Ning Jia","Yu Sun","Shaqi Luo","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2406.19756v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2406.19749v1","updated":"2024-06-28T08:48:14Z","published":"2024-06-28T08:48:14Z","title":"SPIRONet: Spatial-Frequency Learning and Topological Channel Interaction\n  Network for Vessel Segmentation","summary":"  Automatic vessel segmentation is paramount for developing next-generation\ninterventional navigation systems. However, current approaches suffer from\nsuboptimal segmentation performances due to significant challenges in\nintraoperative images (i.e., low signal-to-noise ratio, small or slender\nvessels, and strong interference). In this paper, a novel spatial-frequency\nlearning and topological channel interaction network (SPIRONet) is proposed to\naddress the above issues. Specifically, dual encoders are utilized to\ncomprehensively capture local spatial and global frequency vessel features.\nThen, a cross-attention fusion module is introduced to effectively fuse spatial\nand frequency features, thereby enhancing feature discriminability.\nFurthermore, a topological channel interaction module is designed to filter out\ntask-irrelevant responses based on graph neural networks. Extensive\nexperimental results on several challenging datasets (CADSA, CAXF, DCA1, and\nXCAD) demonstrate state-of-the-art performances of our method. Moreover, the\ninference speed of SPIRONet is 21 FPS with a 512x512 input size, surpassing\nclinical real-time requirements (6~12FPS). These promising outcomes indicate\nSPIRONet's potential for integration into vascular interventional navigation\nsystems. Code is available at https://github.com/Dxhuang-CASIA/SPIRONet.\n","authors":["De-Xing Huang","Xiao-Hu Zhou","Xiao-Liang Xie","Shi-Qi Liu","Shuang-Yi Wang","Zhen-Qiu Feng","Mei-Jiang Gui","Hao Li","Tian-Yu Xiang","Bo-Xian Yao","Zeng-Guang Hou"],"pdf_url":"https://arxiv.org/pdf/2406.19749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19736v1","updated":"2024-06-28T08:25:27Z","published":"2024-06-28T08:25:27Z","title":"MM-Instruct: Generated Visual Instructions for Large Multimodal Model\n  Alignment","summary":"  This paper introduces MM-Instruct, a large-scale dataset of diverse and\nhigh-quality visual instruction data designed to enhance the\ninstruction-following capabilities of large multimodal models (LMMs). While\nexisting visual instruction datasets often focus on question-answering, they\nstruggle to generalize to broader application scenarios such as creative\nwriting, summarization, or image analysis. To address these limitations, we\npropose a novel approach to constructing MM-Instruct that leverages the strong\ninstruction-following capabilities of existing LLMs to generate novel visual\ninstruction data from large-scale but conventional image captioning datasets.\nMM-Instruct first leverages ChatGPT to automatically generate diverse\ninstructions from a small set of seed instructions through augmenting and\nsummarization. It then matches these instructions with images and uses an\nopen-sourced large language model (LLM) to generate coherent answers to the\ninstruction-image pairs. The LLM is grounded by the detailed text descriptions\nof images in the whole answer generation process to guarantee the alignment of\nthe instruction data. Moreover, we introduce a benchmark based on the generated\ninstruction data to evaluate the instruction-following capabilities of existing\nLMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5\nmodel on the generated data, denoted as LLaVA-Instruct, which exhibits\nsignificant improvements in instruction-following capabilities compared to\nLLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models\nare available at https://github.com/jihaonew/MM-Instruct.\n","authors":["Jihao Liu","Xin Huang","Jinliang Zheng","Boxiao Liu","Jia Wang","Osamu Yoshie","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2406.19736v1.pdf","comment":"Dataset and models are available at\n  https://github.com/jihaonew/MM-Instruct"},{"id":"http://arxiv.org/abs/2406.19726v1","updated":"2024-06-28T08:16:54Z","published":"2024-06-28T08:16:54Z","title":"EPOCH: Jointly Estimating the 3D Pose of Cameras and Humans","summary":"  Monocular Human Pose Estimation (HPE) aims at determining the 3D positions of\nhuman joints from a single 2D image captured by a camera. However, a single 2D\npoint in the image may correspond to multiple points in 3D space. Typically,\nthe uniqueness of the 2D-3D relationship is approximated using an orthographic\nor weak-perspective camera model. In this study, instead of relying on\napproximations, we advocate for utilizing the full perspective camera model.\nThis involves estimating camera parameters and establishing a precise,\nunambiguous 2D-3D relationship. To do so, we introduce the EPOCH framework,\ncomprising two main components: the pose lifter network (LiftNet) and the pose\nregressor network (RegNet). LiftNet utilizes the full perspective camera model\nto precisely estimate the 3D pose in an unsupervised manner. It takes a 2D pose\nand camera parameters as inputs and produces the corresponding 3D pose\nestimation. These inputs are obtained from RegNet, which starts from a single\nimage and provides estimates for the 2D pose and camera parameters. RegNet\nutilizes only 2D pose data as weak supervision. Internally, RegNet predicts a\n3D pose, which is then projected to 2D using the estimated camera parameters.\nThis process enables RegNet to establish the unambiguous 2D-3D relationship.\nOur experiments show that modeling the lifting as an unsupervised task with a\ncamera in-the-loop results in better generalization to unseen data. We obtain\nstate-of-the-art results for the 3D HPE on the Human3.6M and MPI-INF-3DHP\ndatasets. Our code is available at: [Github link upon acceptance, see\nsupplementary materials].\n","authors":["Nicola Garau","Giulia Martinelli","Niccolò Bisagno","Denis Tomè","Carsten Stoll"],"pdf_url":"https://arxiv.org/pdf/2406.19726v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2309.01469v2","updated":"2024-06-28T08:13:48Z","published":"2023-09-04T09:26:04Z","title":"Defect Detection in Synthetic Fibre Ropes using Detectron2 Framework","summary":"  Fibre ropes with the latest technology have emerged as an appealing\nalternative to steel ropes for offshore industries due to their lightweight and\nhigh tensile strength. At the same time, frequent inspection of these ropes is\nessential to ensure the proper functioning and safety of the entire system. The\ndevelopment of deep learning (DL) models in condition monitoring (CM)\napplications offers a simpler and more effective approach for defect detection\nin synthetic fibre ropes (SFRs). The present paper investigates the performance\nof Detectron2, a state-of-the-art library for defect detection and instance\nsegmentation. Detectron2 with Mask R-CNN architecture is used for segmenting\ndefects in SFRs. Mask R-CNN with various backbone configurations has been\ntrained and tested on an experimentally obtained dataset comprising 1,803\nhigh-dimensional images containing seven damage classes (placking high,\nplacking medium, placking low, compression, core out, chafing, and normal\nrespectively) for SFRs. By leveraging the capabilities of Detectron2, this\nstudy aims to develop an automated and efficient method for detecting defects\nin SFRs, enhancing the inspection process, and ensuring the safety of the fibre\nropes.\n","authors":["Anju Rani","Daniel O. Arroyo","Petar Durdevic"],"pdf_url":"https://arxiv.org/pdf/2309.01469v2.pdf","comment":"12 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.18584v2","updated":"2024-06-28T07:34:25Z","published":"2024-06-06T06:22:06Z","title":"Assessment of Sentinel-2 spatial and temporal coverage based on the\n  scene classification layer","summary":"  Since the launch of the Sentinel-2 (S2) satellites, many ML models have used\nthe data for diverse applications. The scene classification layer (SCL) inside\nthe S2 product provides rich information for training, such as filtering images\nwith high cloud coverage. However, there is more potential in this. We propose\na technique to assess the clean optical coverage of a region, expressed by a\nSITS and calculated with the S2-based SCL data. With a manual threshold and\nspecific labels in the SCL, the proposed technique assigns a percentage of\nspatial and temporal coverage across the time series and a high/low assessment.\nBy evaluating the AI4EO challenge for Enhanced Agriculture, we show that the\nassessment is correlated to the predictive results of ML models. The\nclassification results in a region with low spatial and temporal coverage is\nworse than in a region with high coverage. Finally, we applied the technique\nacross all continents of the global dataset LandCoverNet.\n","authors":["Cristhian Sanchez","Francisco Mena","Marcela Charfuelan","Marlon Nuske","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2406.18584v2.pdf","comment":"Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2024"},{"id":"http://arxiv.org/abs/2406.19703v1","updated":"2024-06-28T07:28:50Z","published":"2024-06-28T07:28:50Z","title":"Vision Transformer with Key-select Routing Attention for Single Image\n  Dehazing","summary":"  We present Ksformer, utilizing Multi-scale Key-select Routing Attention\n(MKRA) for intelligent selection of key areas through multi-channel,\nmulti-scale windows with a top-k operator, and Lightweight Frequency Processing\nModule (LFPM) to enhance high-frequency features, outperforming other dehazing\nmethods in tests.\n","authors":["Lihan Tong","Weijia Li","Qingxia Yang","Liyuan Chen","Peng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19703v1.pdf","comment":"5 pages,4 figures,IEICE Trans. Information and Systems"},{"id":"http://arxiv.org/abs/2402.11622v2","updated":"2024-06-28T07:20:22Z","published":"2024-02-18T15:28:39Z","title":"Logical Closed Loop: Uncovering Object Hallucinations in Large\n  Vision-Language Models","summary":"  Object hallucination has been an Achilles' heel which hinders the broader\napplications of large vision-language models (LVLMs). Object hallucination\nrefers to the phenomenon that the LVLMs claim non-existent objects in the\nimage. To mitigate the object hallucinations, instruction tuning and external\nmodel-based detection methods have been proposed, which either require\nlarge-scare computational resources or depend on the detection result of\nexternal models. However, there remains an under-explored field to utilize the\nLVLM itself to alleviate object hallucinations. In this work, we adopt the\nintuition that the LVLM tends to respond logically consistently for existent\nobjects but inconsistently for hallucinated objects. Therefore, we propose a\nLogical Closed Loop-based framework for Object Hallucination Detection and\nMitigation, namely LogicCheckGPT. In specific, we devise logical consistency\nprobing to raise questions with logical correlations, inquiring about\nattributes from objects and vice versa. Whether their responses can form a\nlogical closed loop serves as an indicator of object hallucination. As a\nplug-and-play method, it can be seamlessly applied to all existing LVLMs.\nComprehensive experiments conducted on three benchmarks across four LVLMs have\ndemonstrated significant improvements brought by our method, indicating its\neffectiveness and generality.\n","authors":["Junfei Wu","Qiang Liu","Ding Wang","Jinghao Zhang","Shu Wu","Liang Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2402.11622v2.pdf","comment":"Accept to ACL 2024; 19 Pages, 15 Figures, 6 Tables"},{"id":"http://arxiv.org/abs/2406.19693v1","updated":"2024-06-28T07:09:06Z","published":"2024-06-28T07:09:06Z","title":"MMRo: Are Multimodal LLMs Eligible as the Brain for In-Home Robotics?","summary":"  It is fundamentally challenging for robots to serve as useful assistants in\nhuman environments because this requires addressing a spectrum of sub-problems\nacross robotics, including perception, language understanding, reasoning, and\nplanning. The recent advancements in Multimodal Large Language Models (MLLMs)\nhave demonstrated their exceptional abilities in solving complex mathematical\nproblems, mastering commonsense and abstract reasoning. This has led to the\nrecent utilization of MLLMs as the brain in robotic systems, enabling these\nmodels to conduct high-level planning prior to triggering low-level control\nactions for task execution. However, it remains uncertain whether existing\nMLLMs are reliable in serving the brain role of robots. In this study, we\nintroduce the first benchmark for evaluating Multimodal LLM for Robotic (MMRo)\nbenchmark, which tests the capability of MLLMs for robot applications.\nSpecifically, we identify four essential capabilities perception, task\nplanning, visual reasoning, and safety measurement that MLLMs must possess to\nqualify as the robot's central processing unit. We have developed several\nscenarios for each capability, resulting in a total of 14 metrics for\nevaluation. We present experimental results for various MLLMs, including both\ncommercial and open-source models, to assess the performance of existing\nsystems. Our findings indicate that no single model excels in all areas,\nsuggesting that current MLLMs are not yet trustworthy enough to serve as the\ncognitive core for robots. Our data can be found in\nhttps://mm-robobench.github.io/.\n","authors":["Jinming Li","Yichen Zhu","Zhiyuan Xu","Jindong Gu","Minjie Zhu","Xin Liu","Ning Liu","Yaxin Peng","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2406.19693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19690v1","updated":"2024-06-28T07:06:02Z","published":"2024-06-28T07:06:02Z","title":"Deep Fusion Model for Brain Tumor Classification Using Fine-Grained\n  Gradient Preservation","summary":"  Brain tumors are one of the most common diseases that lead to early death if\nnot diagnosed at an early stage. Traditional diagnostic approaches are\nextremely time-consuming and prone to errors. In this context, computer\nvision-based approaches have emerged as an effective tool for accurate brain\ntumor classification. While some of the existing solutions demonstrate\nnoteworthy accuracy, the models become infeasible to deploy in areas where\ncomputational resources are limited. This research addresses the need for\naccurate and fast classification of brain tumors with a priority of deploying\nthe model in technologically underdeveloped regions. The research presents a\nnovel architecture for precise brain tumor classification fusing pretrained\nResNet152V2 and modified VGG16 models. The proposed architecture undergoes a\ndiligent fine-tuning process that ensures fine gradients are preserved in deep\nneural networks, which are essential for effective brain tumor classification.\nThe proposed solution incorporates various image processing techniques to\nimprove image quality and achieves an astounding accuracy of 98.36% and 98.04%\nin Figshare and Kaggle datasets respectively. This architecture stands out for\nhaving a streamlined profile, with only 2.8 million trainable parameters. We\nhave leveraged 8-bit quantization to produce a model of size 73.881 MB,\nsignificantly reducing it from the previous size of 289.45 MB, ensuring smooth\ndeployment in edge devices even in resource-constrained areas. Additionally,\nthe use of Grad-CAM improves the interpretability of the model, offering\ninsightful information regarding its decision-making process. Owing to its high\ndiscriminative ability, this model can be a reliable option for accurate brain\ntumor classification.\n","authors":["Niful Islam","Mohaiminul Islam Bhuiyan","Jarin Tasnim Raya","Nur Shazwani Kamarudin","Khan Md Hasib","M. F. Mridha","Dewan Md. Farid"],"pdf_url":"https://arxiv.org/pdf/2406.19690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16462v2","updated":"2024-06-28T07:04:21Z","published":"2023-11-28T03:45:29Z","title":"Viewport Prediction for Volumetric Video Streaming by Exploring Video\n  Saliency and Trajectory Information","summary":"  Volumetric video, also known as hologram video, is a novel medium that\nportrays natural content in Virtual Reality (VR), Augmented Reality (AR), and\nMixed Reality (MR). It is expected to be the next-gen video technology and a\nprevalent use case for 5G and beyond wireless communication. Considering that\neach user typically only watches a section of the volumetric video, known as\nthe viewport, it is essential to have precise viewport prediction for optimal\nperformance. However, research on this topic is still in its infancy. In the\nend, this paper presents and proposes a novel approach, named Saliency and\nTrajectory Viewport Prediction (STVP), which aims to improve the precision of\nviewport prediction in volumetric video streaming. The STVP extensively\nutilizes video saliency information and viewport trajectory. To our knowledge,\nthis is the first comprehensive study of viewport prediction in volumetric\nvideo streaming. In particular, we introduce a novel sampling method, Uniform\nRandom Sampling (URS), to reduce computational complexity while still\npreserving video features in an efficient manner. Then we present a saliency\ndetection technique that incorporates both spatial and temporal information for\ndetecting static, dynamic geometric, and color salient regions. Finally, we\nintelligently fuse saliency and trajectory information to achieve more accurate\nviewport prediction. We conduct extensive simulations to evaluate the\neffectiveness of our proposed viewport prediction methods using\nstate-of-the-art volumetric video sequences. The experimental results show the\nsuperiority of the proposed method over existing schemes. The dataset and\nsource code will be publicly accessible after acceptance.\n","authors":["Jie Li","Zhixin Li","Zhi Liu","Pengyuan Zhou","Richang Hong","Qiyue Li","Han Hu"],"pdf_url":"https://arxiv.org/pdf/2311.16462v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19686v1","updated":"2024-06-28T06:51:38Z","published":"2024-06-28T06:51:38Z","title":"Enhancing Radiological Diagnosis: A Collaborative Approach Integrating\n  AI and Human Expertise for Visual Miss Correction","summary":"  Human-AI collaboration to identify and correct perceptual errors in chest\nradiographs has not been previously explored. This study aimed to develop a\ncollaborative AI system, CoRaX, which integrates eye gaze data and radiology\nreports to enhance diagnostic accuracy in chest radiology by pinpointing\nperceptual errors and refining the decision-making process. Using public\ndatasets REFLACX and EGD-CXR, the study retrospectively developed CoRaX,\nemploying a large multimodal model to analyze image embeddings, eye gaze data,\nand radiology reports. The system's effectiveness was evaluated based on its\nreferral-making process, the quality of referrals, and performance in\ncollaborative diagnostic settings. CoRaX was tested on a simulated error\ndataset of 271 samples with 28% (93 of 332) missed abnormalities. The system\ncorrected 21% (71 of 332) of these errors, leaving 7% (22 of 312) unresolved.\nThe Referral-Usefulness score, indicating the accuracy of predicted regions for\nall true referrals, was 0.63 (95% CI 0.59, 0.68). The Total-Usefulness score,\nreflecting the diagnostic accuracy of CoRaX's interactions with radiologists,\nshowed that 84% (237 of 280) of these interactions had a score above 0.40. In\nconclusion, CoRaX efficiently collaborates with radiologists to address\nperceptual errors across various abnormalities, with potential applications in\nthe education and training of novice radiologists.\n","authors":["Akash Awasthi","Ngan Le","Zhigang Deng","Carol C. Wu","Hien Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2406.19686v1.pdf","comment":"Under Review in Journal"},{"id":"http://arxiv.org/abs/2406.19070v2","updated":"2024-06-28T06:47:10Z","published":"2024-06-27T10:40:35Z","title":"FAGhead: Fully Animate Gaussian Head from Monocular Videos","summary":"  High-fidelity reconstruction of 3D human avatars has a wild application in\nvisual reality. In this paper, we introduce FAGhead, a method that enables\nfully controllable human portraits from monocular videos. We explicit the\ntraditional 3D morphable meshes (3DMM) and optimize the neutral 3D Gaussians to\nreconstruct with complex expressions. Furthermore, we employ a novel\nPoint-based Learnable Representation Field (PLRF) with learnable Gaussian point\npositions to enhance reconstruction performance. Meanwhile, to effectively\nmanage the edges of avatars, we introduced the alpha rendering to supervise the\nalpha value of each pixel. Extensive experimental results on the open-source\ndatasets and our capturing datasets demonstrate that our approach is able to\ngenerate high-fidelity 3D head avatars and fully control the expression and\npose of the virtual avatars, which is outperforming than existing works.\n","authors":["Yixin Xuan","Xinyang Li","Gongxin Yao","Shiwei Zhou","Donghui Sun","Xiaoxin Chen","Yu Pan"],"pdf_url":"https://arxiv.org/pdf/2406.19070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18146v2","updated":"2024-06-28T06:43:39Z","published":"2024-06-26T07:56:17Z","title":"A Refer-and-Ground Multimodal Large Language Model for Biomedicine","summary":"  With the rapid development of multimodal large language models (MLLMs),\nespecially their capabilities in visual chat through refer and ground\nfunctionalities, their significance is increasingly recognized. However, the\nbiomedical field currently exhibits a substantial gap in this area, primarily\ndue to the absence of a dedicated refer and ground dataset for biomedical\nimages. To address this challenge, we devised the Med-GRIT-270k dataset. It\ncomprises 270k question-and-answer pairs and spans eight distinct medical\nimaging modalities. Most importantly, it is the first dedicated to the\nbiomedical domain and integrating refer and ground conversations. The key idea\nis to sample large-scale biomedical image-mask pairs from medical segmentation\ndatasets and generate instruction datasets from text using chatGPT.\nAdditionally, we introduce a Refer-and-Ground Multimodal Large Language Model\nfor Biomedicine (BiRD) by using this dataset and multi-task instruction\nlearning. Extensive experiments have corroborated the efficacy of the\nMed-GRIT-270k dataset and the multi-modal, fine-grained interactive\ncapabilities of the BiRD model. This holds significant reference value for the\nexploration and development of intelligent biomedical assistants.\n","authors":["Xiaoshuang Huang","Haifeng Huang","Lingdong Shen","Yehui Yang","Fangxin Shang","Junwei Liu","Jia Liu"],"pdf_url":"https://arxiv.org/pdf/2406.18146v2.pdf","comment":"Accepted by MICCAI2024"},{"id":"http://arxiv.org/abs/2406.19680v1","updated":"2024-06-28T06:40:53Z","published":"2024-06-28T06:40:53Z","title":"MimicMotion: High-Quality Human Motion Video Generation with\n  Confidence-aware Pose Guidance","summary":"  In recent years, generative artificial intelligence has achieved significant\nadvancements in the field of image generation, spawning a variety of\napplications. However, video generation still faces considerable challenges in\nvarious aspects, such as controllability, video length, and richness of\ndetails, which hinder the application and popularization of this technology. In\nthis work, we propose a controllable video generation framework, dubbed\nMimicMotion, which can generate high-quality videos of arbitrary length\nmimicking specific motion guidance. Compared with previous methods, our\napproach has several highlights. Firstly, we introduce confidence-aware pose\nguidance that ensures high frame quality and temporal smoothness. Secondly, we\nintroduce regional loss amplification based on pose confidence, which\nsignificantly reduces image distortion. Lastly, for generating long and smooth\nvideos, we propose a progressive latent fusion strategy. By this means, we can\nproduce videos of arbitrary length with acceptable resource consumption. With\nextensive experiments and user studies, MimicMotion demonstrates significant\nimprovements over previous approaches in various aspects. Detailed results and\ncomparisons are available on our project page:\nhttps://tencent.github.io/MimicMotion .\n","authors":["Yuang Zhang","Jiaxi Gu","Li-Wen Wang","Han Wang","Junqi Cheng","Yuefeng Zhu","Fangyuan Zou"],"pdf_url":"https://arxiv.org/pdf/2406.19680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19675v1","updated":"2024-06-28T06:25:21Z","published":"2024-06-28T06:25:21Z","title":"Deep Learning-based Depth Estimation Methods from Monocular Image and\n  Videos: A Comprehensive Survey","summary":"  Estimating depth from single RGB images and videos is of widespread interest\ndue to its applications in many areas, including autonomous driving, 3D\nreconstruction, digital entertainment, and robotics. More than 500 deep\nlearning-based papers have been published in the past 10 years, which indicates\nthe growing interest in the task. This paper presents a comprehensive survey of\nthe existing deep learning-based methods, the challenges they address, and how\nthey have evolved in their architecture and supervision methods. It provides a\ntaxonomy for classifying the current work based on their input and output\nmodalities, network architectures, and learning methods. It also discusses the\nmajor milestones in the history of monocular depth estimation, and different\npipelines, datasets, and evaluation metrics used in existing methods.\n","authors":["Uchitha Rajapaksha","Ferdous Sohel","Hamid Laga","Dean Diepeveen","Mohammed Bennamoun"],"pdf_url":"https://arxiv.org/pdf/2406.19675v1.pdf","comment":"46 pages, 10 figures, The paper has been accepted for publication in\n  ACM Computing Surveys 2024"},{"id":"http://arxiv.org/abs/2405.05164v2","updated":"2024-06-28T06:11:11Z","published":"2024-05-08T15:54:57Z","title":"ProbRadarM3F: mmWave Radar based Human Skeletal Pose Estimation with\n  Probability Map Guided Multi-Format Feature Fusion","summary":"  Millimeter wave (mmWave) radar is a non-intrusive privacy and relatively\nconvenient and inexpensive device, which has been demonstrated to be applicable\nin place of RGB cameras in human indoor pose estimation tasks. However, mmWave\nradar relies on the collection of reflected signals from the target, and the\nradar signals containing information is difficult to be fully applied. This has\nbeen a long-standing hindrance to the improvement of pose estimation accuracy.\nTo address this major challenge, this paper introduces a probability map guided\nmulti-format feature fusion model, ProbRadarM3F. This is a novel radar feature\nextraction framework using a traditional FFT method in parallel with a\nprobability map based positional encoding method. ProbRadarM3F fuses the\ntraditional heatmap features and the positional features, then effectively\nachieves the estimation of 14 keypoints of the human body. Experimental\nevaluation on the HuPR dataset proves the effectiveness of the model proposed\nin this paper, outperforming other methods experimented on this dataset with an\nAP of 69.9 %. The emphasis of our study is focusing on the position information\nthat is not exploited before in radar singal. This provides direction to\ninvestigate other potential non-redundant information from mmWave rader.\n","authors":["Bing Zhu","Zixin He","Weiyi Xiong","Guanhua Ding","Jianan Liu","Tao Huang","Wei Chen","Wei Xiang"],"pdf_url":"https://arxiv.org/pdf/2405.05164v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19672v1","updated":"2024-06-28T06:06:01Z","published":"2024-06-28T06:06:01Z","title":"Beyond First-Order: A Multi-Scale Approach to Finger Knuckle Print\n  Biometrics","summary":"  Recently, finger knuckle prints (FKPs) have gained attention due to their\nrich textural patterns, positioning them as a promising biometric for identity\nrecognition. Prior FKP recognition methods predominantly leverage first-order\nfeature descriptors, which capture intricate texture details but fail to\naccount for structural information. Emerging research, however, indicates that\nsecond-order textures, which describe the curves and arcs of the textures,\nencompass this overlooked structural information. This paper introduces a novel\nFKP recognition approach, the Dual-Order Texture Competition Network (DOTCNet),\ndesigned to capture texture information in FKP images comprehensively. DOTCNet\nincorporates three dual-order texture competitive modules (DTCMs), each\ntargeting textures at different scales. Each DTCM employs a learnable texture\ndescriptor, specifically a learnable Gabor filter (LGF), to extract texture\nfeatures. By leveraging LGFs, the network extracts first and second order\ntextures to describe fine textures and structural features thoroughly.\nFurthermore, an attention mechanism enhances relevant features in the\nfirst-order features, thereby highlighting significant texture details. For\nsecond-order features, a competitive mechanism emphasizes structural\ninformation while reducing noise from higher-order features. Extensive\nexperimental results reveal that DOTCNet significantly outperforms several\nstandard algorithms on the publicly available PolyU-FKP dataset.\n","authors":["Chengrui Gao","Ziyuan Yang","Andrew Beng Jin Teoh","Min Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.19672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19364v2","updated":"2024-06-28T05:56:08Z","published":"2024-06-27T17:46:13Z","title":"SimTxtSeg: Weakly-Supervised Medical Image Segmentation with Simple Text\n  Cues","summary":"  Weakly-supervised medical image segmentation is a challenging task that aims\nto reduce the annotation cost while keep the segmentation performance. In this\npaper, we present a novel framework, SimTxtSeg, that leverages simple text cues\nto generate high-quality pseudo-labels and study the cross-modal fusion in\ntraining segmentation models, simultaneously. Our contribution consists of two\nkey components: an effective Textual-to-Visual Cue Converter that produces\nvisual prompts from text prompts on medical images, and a text-guided\nsegmentation model with Text-Vision Hybrid Attention that fuses text and image\nfeatures. We evaluate our framework on two medical image segmentation tasks:\ncolonic polyp segmentation and MRI brain tumor segmentation, and achieve\nconsistent state-of-the-art performance.\n","authors":["Yuxin Xie","Tao Zhou","Yi Zhou","Geng Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19364v2.pdf","comment":"accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.17051v2","updated":"2024-06-28T05:50:11Z","published":"2024-06-24T18:13:09Z","title":"Leveraging Knowledge Distillation for Lightweight Skin Cancer\n  Classification: Balancing Accuracy and Computational Efficiency","summary":"  Skin cancer is a major concern to public health, accounting for one-third of\nthe reported cancers. If not detected early, the cancer has the potential for\nsevere consequences. Recognizing the critical need for effective skin cancer\nclassification, we address the limitations of existing models, which are often\ntoo large to deploy in areas with limited computational resources. In response,\nwe present a knowledge distillation based approach for creating a lightweight\nyet high-performing classifier. The proposed solution involves fusing three\nmodels, namely ResNet152V2, ConvNeXtBase, and ViT Base, to create an effective\nteacher model. The teacher model is then employed to guide a lightweight\nstudent model of size 2.03 MB. This student model is further compressed to\n469.77 KB using 16-bit quantization, enabling smooth incorporation into edge\ndevices. With six-stage image preprocessing, data augmentation, and a rigorous\nablation study, the model achieves an impressive accuracy of 98.75% on the\nHAM10000 dataset and 98.94% on the Kaggle dataset in classifying benign and\nmalignant skin cancers. With its high accuracy and compact size, our model\nappears to be a potential choice for accurate skin cancer classification,\nparticularly in resource-constrained settings.\n","authors":["Niful Islam","Khan Md Hasib","Fahmida Akter Joti","Asif Karim","Sami Azam"],"pdf_url":"https://arxiv.org/pdf/2406.17051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19237v2","updated":"2024-06-28T05:43:46Z","published":"2024-06-27T15:01:48Z","title":"FlowVQA: Mapping Multimodal Logic in Visual Question Answering with\n  Flowcharts","summary":"  Existing benchmarks for visual question answering lack in visual grounding\nand complexity, particularly in evaluating spatial reasoning skills. We\nintroduce FlowVQA, a novel benchmark aimed at assessing the capabilities of\nvisual question-answering multimodal language models in reasoning with\nflowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and\nhuman-verified flowchart images from three distinct content sources, along with\n22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks,\nincluding information localization, decision-making, and logical progression.\nWe conduct a thorough baseline evaluation on a suite of both open-source and\nproprietary multimodal language models using various strategies, followed by an\nanalysis of directional bias. The results underscore the benchmark's potential\nas a vital tool for advancing the field of multimodal modeling, providing a\nfocused and challenging environment for enhancing model performance in visual\nand logical reasoning tasks.\n","authors":["Shubhankar Singh","Purvi Chaurasia","Yerram Varun","Pranshu Pandya","Vatsal Gupta","Vivek Gupta","Dan Roth"],"pdf_url":"https://arxiv.org/pdf/2406.19237v2.pdf","comment":"Accepted in ACL 2024 (Findings), 21 pages, 7 figures, 9 Tables"},{"id":"http://arxiv.org/abs/2406.18684v2","updated":"2024-06-28T05:43:41Z","published":"2024-06-26T18:42:22Z","title":"CSI4Free: GAN-Augmented mmWave CSI for Improved Pose Classification","summary":"  In recent years, Joint Communication and Sensing (JC&S), has demonstrated\nsignificant success, particularly in utilizing sub-6 GHz frequencies with\ncommercial-off-the-shelf (COTS) Wi-Fi devices for applications such as\nlocalization, gesture recognition, and pose classification. Deep learning and\nthe existence of large public datasets has been pivotal in achieving such\nresults. However, at mmWave frequencies (30-300 GHz), which has shown potential\nfor more accurate sensing performance, there is a noticeable lack of research\nin the domain of COTS Wi-Fi sensing. Challenges such as limited research\nhardware, the absence of large datasets, limited functionality in COTS\nhardware, and the complexities of data collection present obstacles to a\ncomprehensive exploration of this field. In this work, we aim to address these\nchallenges by developing a method that can generate synthetic mmWave channel\nstate information (CSI) samples. In particular, we use a generative adversarial\nnetwork (GAN) on an existing dataset, to generate 30,000 additional CSI\nsamples. The augmented samples exhibit a remarkable degree of consistency with\nthe original data, as indicated by the notably high GAN-train and GAN-test\nscores. Furthermore, we integrate the augmented samples in training a pose\nclassification model. We observe that the augmented samples complement the real\ndata and improve the generalization of the classification model.\n","authors":["Nabeel Nisar Bhat","Rafael Berkvens","Jeroen Famaey"],"pdf_url":"https://arxiv.org/pdf/2406.18684v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19668v1","updated":"2024-06-28T05:38:32Z","published":"2024-06-28T05:38:32Z","title":"PopAlign: Population-Level Alignment for Fair Text-to-Image Generation","summary":"  Text-to-image (T2I) models achieve high-fidelity generation through extensive\ntraining on large datasets. However, these models may unintentionally pick up\nundesirable biases of their training data, such as over-representation of\nparticular identities in gender or ethnicity neutral prompts. Existing\nalignment methods such as Reinforcement Learning from Human Feedback (RLHF) and\nDirect Preference Optimization (DPO) fail to address this problem effectively\nbecause they operate on pairwise preferences consisting of individual samples,\nwhile the aforementioned biases can only be measured at a population level. For\nexample, a single sample for the prompt \"doctor\" could be male or female, but a\nmodel generating predominantly male doctors even with repeated sampling\nreflects a gender bias. To address this limitation, we introduce PopAlign, a\nnovel approach for population-level preference optimization, while standard\noptimization would prefer entire sets of samples over others. We further derive\na stochastic lower bound that directly optimizes for individual samples from\npreferred populations over others for scalable training. Using human evaluation\nand standard image quality and bias metrics, we show that PopAlign\nsignificantly mitigates the bias of pretrained T2I models while largely\npreserving the generation quality. Code is available at\nhttps://github.com/jacklishufan/PopAlignSDXL.\n","authors":["Shufan Li","Harkanwar Singh","Aditya Grover"],"pdf_url":"https://arxiv.org/pdf/2406.19668v1.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.19666v1","updated":"2024-06-28T05:25:57Z","published":"2024-06-28T05:25:57Z","title":"CSAKD: Knowledge Distillation with Cross Self-Attention for\n  Hyperspectral and Multispectral Image Fusion","summary":"  Hyperspectral imaging, capturing detailed spectral information for each\npixel, is pivotal in diverse scientific and industrial applications. Yet, the\nacquisition of high-resolution (HR) hyperspectral images (HSIs) often needs to\nbe addressed due to the hardware limitations of existing imaging systems. A\nprevalent workaround involves capturing both a high-resolution multispectral\nimage (HR-MSI) and a low-resolution (LR) HSI, subsequently fusing them to yield\nthe desired HR-HSI. Although deep learning-based methods have shown promising\nin HR-MSI/LR-HSI fusion and LR-HSI super-resolution (SR), their substantial\nmodel complexities hinder deployment on resource-constrained imaging devices.\nThis paper introduces a novel knowledge distillation (KD) framework for\nHR-MSI/LR-HSI fusion to achieve SR of LR-HSI. Our KD framework integrates the\nproposed Cross-Layer Residual Aggregation (CLRA) block to enhance efficiency\nfor constructing Dual Two-Streamed (DTS) network structure, designed to extract\njoint and distinct features from LR-HSI and HR-MSI simultaneously. To fully\nexploit the spatial and spectral feature representations of LR-HSI and HR-MSI,\nwe propose a novel Cross Self-Attention (CSA) fusion module to adaptively fuse\nthose features to improve the spatial and spectral quality of the reconstructed\nHR-HSI. Finally, the proposed KD-based joint loss function is employed to\nco-train the teacher and student networks. Our experimental results demonstrate\nthat the student model not only achieves comparable or superior LR-HSI SR\nperformance but also significantly reduces the model-size and computational\nrequirements. This marks a substantial advancement over existing\nstate-of-the-art methods. The source code is available at\nhttps://github.com/ming053l/CSAKD.\n","authors":["Chih-Chung Hsu","Chih-Chien Ni","Chia-Ming Lee","Li-Wei Kang"],"pdf_url":"https://arxiv.org/pdf/2406.19666v1.pdf","comment":"Submitted to TIP 2024"},{"id":"http://arxiv.org/abs/2405.19769v2","updated":"2024-06-28T05:25:19Z","published":"2024-05-30T07:34:05Z","title":"All-In-One Medical Image Restoration via Task-Adaptive Routing","summary":"  Although single-task medical image restoration (MedIR) has witnessed\nremarkable success, the limited generalizability of these methods poses a\nsubstantial obstacle to wider application. In this paper, we focus on the task\nof all-in-one medical image restoration, aiming to address multiple distinct\nMedIR tasks with a single universal model. Nonetheless, due to significant\ndifferences between different MedIR tasks, training a universal model often\nencounters task interference issues, where different tasks with shared\nparameters may conflict with each other in the gradient update direction. This\ntask interference leads to deviation of the model update direction from the\noptimal path, thereby affecting the model's performance. To tackle this issue,\nwe propose a task-adaptive routing strategy, allowing conflicting tasks to\nselect different network paths in spatial and channel dimensions, thereby\nmitigating task interference. Experimental results demonstrate that our\nproposed \\textbf{A}ll-in-one \\textbf{M}edical \\textbf{I}mage\n\\textbf{R}estoration (\\textbf{AMIR}) network achieves state-of-the-art\nperformance in three MedIR tasks: MRI super-resolution, CT denoising, and PET\nsynthesis, both in single-task and all-in-one settings. The code and data will\nbe available at\n\\href{https://github.com/Yaziwel/All-In-One-Medical-Image-Restoration-via-Task-Adaptive-Routing.git}{https://github.com/Yaziwel/AMIR}.\n","authors":["Zhiwen Yang","Haowei Chen","Ziniu Qian","Yang Yi","Hui Zhang","Dan Zhao","Bingzheng Wei","Yan Xu"],"pdf_url":"https://arxiv.org/pdf/2405.19769v2.pdf","comment":"This article has been early accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.19665v1","updated":"2024-06-28T05:22:39Z","published":"2024-06-28T05:22:39Z","title":"PM-VIS+: High-Performance Video Instance Segmentation without Video\n  Annotation","summary":"  Video instance segmentation requires detecting, segmenting, and tracking\nobjects in videos, typically relying on costly video annotations. This paper\nintroduces a method that eliminates video annotations by utilizing image\ndatasets. The PM-VIS algorithm is adapted to handle both bounding box and\ninstance-level pixel annotations dynamically. We introduce ImageNet-bbox to\nsupplement missing categories in video datasets and propose the PM-VIS+\nalgorithm to adjust supervision based on annotation types. To enhance accuracy,\nwe use pseudo masks and semi-supervised optimization techniques on unannotated\nvideo data. This method achieves high video instance segmentation performance\nwithout manual video annotations, offering a cost-effective solution and new\nperspectives for video instance segmentation applications. The code will be\navailable in https://github.com/ldknight/PM-VIS-plus\n","authors":["Zhangjing Yang","Dun Liu","Xin Wang","Zhe Li","Barathwaj Anandan","Yi Wu"],"pdf_url":"https://arxiv.org/pdf/2406.19665v1.pdf","comment":"MIPR 2024"},{"id":"http://arxiv.org/abs/2406.18844v2","updated":"2024-06-28T05:21:13Z","published":"2024-06-27T02:31:03Z","title":"Revisiting Backdoor Attacks against Large Vision-Language Models","summary":"  Instruction tuning enhances large vision-language models (LVLMs) but raises\nsecurity risks through potential backdoor attacks due to their openness.\nPrevious backdoor studies focus on enclosed scenarios with consistent training\nand testing instructions, neglecting the practical domain gaps that could\naffect attack effectiveness. This paper empirically examines the\ngeneralizability of backdoor attacks during the instruction tuning of LVLMs for\nthe first time, revealing certain limitations of most backdoor strategies in\npractical scenarios. We quantitatively evaluate the generalizability of six\ntypical backdoor attacks on image caption benchmarks across multiple LVLMs,\nconsidering both visual and textual domain offsets. Our findings indicate that\nattack generalizability is positively correlated with the backdoor trigger's\nirrelevance to specific images/models and the preferential correlation of the\ntrigger pattern. Additionally, we modify existing backdoor attacks based on the\nabove key observations, demonstrating significant improvements in cross-domain\nscenario generalizability (+86% attack success rate). Notably, even without\naccess to the instruction datasets, a multimodal instruction set can be\nsuccessfully poisoned with a very low poisoning rate (0.2%), achieving an\nattack success rate of over 97%. This paper underscores that even simple\ntraditional backdoor strategies pose a serious threat to LVLMs, necessitating\nmore attention and in-depth research.\n","authors":["Siyuan Liang","Jiawei Liang","Tianyu Pang","Chao Du","Aishan Liu","Ee-Chien Chang","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2406.18844v2.pdf","comment":"23 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.19655v1","updated":"2024-06-28T04:49:57Z","published":"2024-06-28T04:49:57Z","title":"Basketball-SORT: An Association Method for Complex Multi-object\n  Occlusion Problems in Basketball Multi-object Tracking","summary":"  Recent deep learning-based object detection approaches have led to\nsignificant progress in multi-object tracking (MOT) algorithms. The current MOT\nmethods mainly focus on pedestrian or vehicle scenes, but basketball sports\nscenes are usually accompanied by three or more object occlusion problems with\nsimilar appearances and high-intensity complex motions, which we call complex\nmulti-object occlusion (CMOO). Here, we propose an online and robust MOT\napproach, named Basketball-SORT, which focuses on the CMOO problems in\nbasketball videos. To overcome the CMOO problem, instead of using the\nintersection-over-union-based (IoU-based) approach, we use the trajectories of\nneighboring frames based on the projected positions of the players. Our method\ndesigns the basketball game restriction (BGR) and reacquiring Long-Lost IDs\n(RLLI) based on the characteristics of basketball scenes, and we also solve the\nocclusion problem based on the player trajectories and appearance features.\nExperimental results show that our method achieves a Higher Order Tracking\nAccuracy (HOTA) score of 63.48$\\%$ on the basketball fixed video dataset and\noutperforms other recent popular approaches. Overall, our approach solved the\nCMOO problem more effectively than recent MOT algorithms.\n","authors":["Qingrui Hu","Atom Scott","Calvin Yeung","Keisuke Fujii"],"pdf_url":"https://arxiv.org/pdf/2406.19655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19649v1","updated":"2024-06-28T04:38:12Z","published":"2024-06-28T04:38:12Z","title":"AstMatch: Adversarial Self-training Consistency Framework for\n  Semi-Supervised Medical Image Segmentation","summary":"  Semi-supervised learning (SSL) has shown considerable potential in medical\nimage segmentation, primarily leveraging consistency regularization and\npseudo-labeling. However, many SSL approaches only pay attention to low-level\nconsistency and overlook the significance of pseudo-label reliability.\nTherefore, in this work, we propose an adversarial self-training consistency\nframework (AstMatch). Firstly, we design an adversarial consistency\nregularization (ACR) approach to enhance knowledge transfer and strengthen\nprediction consistency under varying perturbation intensities. Second, we apply\na feature matching loss for adversarial training to incorporate high-level\nconsistency regularization. Additionally, we present the pyramid channel\nattention (PCA) and efficient channel and spatial attention (ECSA) modules to\nimprove the discriminator's performance. Finally, we propose an adaptive\nself-training (AST) approach to ensure the pseudo-labels' quality. The proposed\nAstMatch has been extensively evaluated with cutting-edge SSL methods on three\npublic-available datasets. The experimental results under different labeled\nratios indicate that AstMatch outperforms other existing methods, achieving new\nstate-of-the-art performance. Our code will be available at\nhttps://github.com/GuanghaoZhu663/AstMatch.\n","authors":["Guanghao Zhu","Jing Zhang","Juanxiu Liu","Xiaohui Du","Ruqian Hao","Yong Liu","Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2406.19649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19640v1","updated":"2024-06-28T04:10:21Z","published":"2024-06-28T04:10:21Z","title":"Efficient Event Stream Super-Resolution with Recursive Multi-Branch\n  Fusion","summary":"  Current Event Stream Super-Resolution (ESR) methods overlook the redundant\nand complementary information present in positive and negative events within\nthe event stream, employing a direct mixing approach for super-resolution,\nwhich may lead to detail loss and inefficiency. To address these issues, we\npropose an efficient Recursive Multi-Branch Information Fusion Network (RMFNet)\nthat separates positive and negative events for complementary information\nextraction, followed by mutual supplementation and refinement. Particularly, we\nintroduce Feature Fusion Modules (FFM) and Feature Exchange Modules (FEM). FFM\nis designed for the fusion of contextual information within neighboring event\nstreams, leveraging the coupling relationship between positive and negative\nevents to alleviate the misleading of noises in the respective branches. FEM\nefficiently promotes the fusion and exchange of information between positive\nand negative branches, enabling superior local information enhancement and\nglobal information complementation. Experimental results demonstrate that our\napproach achieves over 17% and 31% improvement on synthetic and real datasets,\naccompanied by a 2.3X acceleration. Furthermore, we evaluate our method on two\ndownstream event-driven applications, \\emph{i.e.}, object recognition and video\nreconstruction, achieving remarkable results that outperform existing methods.\nOur code and Supplementary Material are available at\nhttps://github.com/Lqm26/RMFNet.\n","authors":["Quanmin Liang","Zhilin Huang","Xiawu Zheng","Feidiao Yang","Jun Peng","Kai Huang","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2406.19640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19638v1","updated":"2024-06-28T03:58:02Z","published":"2024-06-28T03:58:02Z","title":"Precision matters: Precision-aware ensemble for weakly supervised\n  semantic segmentation","summary":"  Weakly Supervised Semantic Segmentation (WSSS) employs weak supervision, such\nas image-level labels, to train the segmentation model. Despite the impressive\nachievement in recent WSSS methods, we identify that introducing weak labels\nwith high mean Intersection of Union (mIoU) does not guarantee high\nsegmentation performance. Existing studies have emphasized the importance of\nprioritizing precision and reducing noise to improve overall performance. In\nthe same vein, we propose ORANDNet, an advanced ensemble approach tailored for\nWSSS. ORANDNet combines Class Activation Maps (CAMs) from two different\nclassifiers to increase the precision of pseudo-masks (PMs). To further\nmitigate small noise in the PMs, we incorporate curriculum learning. This\ninvolves training the segmentation model initially with pairs of smaller-sized\nimages and corresponding PMs, gradually transitioning to the original-sized\npairs. By combining the original CAMs of ResNet-50 and ViT, we significantly\nimprove the segmentation performance over the single-best model and the naive\nensemble model, respectively. We further extend our ensemble method to CAMs\nfrom AMN (ResNet-like) and MCTformer (ViT-like) models, achieving performance\nbenefits in advanced WSSS models. It highlights the potential of our ORANDNet\nas a final add-on module for WSSS models.\n","authors":["Junsung Park","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2406.19638v1.pdf","comment":"5 pages, 5 figures, accepted in AAAI 2024 Edge Intelligence Workshop"},{"id":"http://arxiv.org/abs/2310.01712v2","updated":"2024-06-28T03:53:56Z","published":"2023-10-03T00:54:13Z","title":"Generative Autoencoding of Dropout Patterns","summary":"  We propose a generative model termed Deciphering Autoencoders. In this model,\nwe assign a unique random dropout pattern to each data point in the training\ndataset and then train an autoencoder to reconstruct the corresponding data\npoint using this pattern as information to be encoded. Even if a completely\nrandom dropout pattern is assigned to each data point regardless of their\nsimilarities, a sufficiently large encoder can smoothly map them to a\nlow-dimensional latent space to reconstruct individual training data points.\nDuring inference, using a dropout pattern different from those used during\ntraining allows the model to function as a generator. Since the training of\nDeciphering Autoencoders relies solely on reconstruction error, it offers more\nstable training compared to other generative models. Despite their simplicity,\nDeciphering Autoencoders show sampling quality comparable to DCGAN on the\nCIFAR-10 dataset.\n","authors":["Shunta Maeda"],"pdf_url":"https://arxiv.org/pdf/2310.01712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18070v3","updated":"2024-06-28T03:50:19Z","published":"2024-06-26T05:01:37Z","title":"EgoVideo: Exploring Egocentric Foundation Model and Downstream\n  Adaptation","summary":"  In this report, we present our solutions to the EgoVis Challenges in CVPR\n2024, including five tracks in the Ego4D challenge and three tracks in the\nEPIC-Kitchens challenge. Building upon the video-language two-tower model and\nleveraging our meticulously organized egocentric video data, we introduce a\nnovel foundation model called EgoVideo. This model is specifically designed to\ncater to the unique characteristics of egocentric videos and provides strong\nsupport for our competition submissions. In the Ego4D challenges, we tackle\nvarious tasks including Natural Language Queries, Step Grounding, Moment\nQueries, Short-term Object Interaction Anticipation, and Long-term Action\nAnticipation. In addition, we also participate in the EPIC-Kitchens challenge,\nwhere we engage in the Action Recognition, Multiple Instance Retrieval, and\nDomain Adaptation for Action Recognition tracks. By adapting EgoVideo to these\ndiverse tasks, we showcase its versatility and effectiveness in different\negocentric video analysis scenarios, demonstrating the powerful representation\nability of EgoVideo as an egocentric foundation model. Our codebase and\npretrained models are publicly available at\nhttps://github.com/OpenGVLab/EgoVideo.\n","authors":["Baoqi Pei","Guo Chen","Jilan Xu","Yuping He","Yicheng Liu","Kanghua Pan","Yifei Huang","Yali Wang","Tong Lu","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2406.18070v3.pdf","comment":"Champion solutions in the EgoVis CVPR 2024 workshop"},{"id":"http://arxiv.org/abs/2406.19635v1","updated":"2024-06-28T03:46:53Z","published":"2024-06-28T03:46:53Z","title":"Model Predictive Simulation Using Structured Graphical Models and\n  Transformers","summary":"  We propose an approach to simulating trajectories of multiple interacting\nagents (road users) based on transformers and probabilistic graphical models\n(PGMs), and apply it to the Waymo SimAgents challenge. The transformer baseline\nis based on the MTR model, which predicts multiple future trajectories\nconditioned on the past trajectories and static road layout features. We then\nimprove upon these generated trajectories using a PGM, which contains factors\nwhich encode prior knowledge, such as a preference for smooth trajectories, and\navoidance of collisions with static obstacles and other moving agents. We\nperform (approximate) MAP inference in this PGM using the Gauss-Newton method.\nFinally we sample $K=32$ trajectories for each of the $N \\sim 100$ agents for\nthe next $T=8 \\Delta$ time steps, where $\\Delta=10$ is the sampling rate per\nsecond. Following the Model Predictive Control (MPC) paradigm, we only return\nthe first element of our forecasted trajectories at each step, and then we\nreplan, so that the simulation can constantly adapt to its changing\nenvironment. We therefore call our approach \"Model Predictive Simulation\" or\nMPS. We show that MPS improves upon the MTR baseline, especially in safety\ncritical metrics such as collision rate. Furthermore, our approach is\ncompatible with any underlying forecasting model, and does not require extra\ntraining, so we believe it is a valuable contribution to the community.\n","authors":["Xinghua Lou","Meet Dave","Shrinu Kushagra","Miguel Lazaro-Gredilla","Kevin Murphy"],"pdf_url":"https://arxiv.org/pdf/2406.19635v1.pdf","comment":"Special Mention at the Waymo Sim Agents Challenge 2024"},{"id":"http://arxiv.org/abs/2406.19632v1","updated":"2024-06-28T03:43:49Z","published":"2024-06-28T03:43:49Z","title":"PPTFormer: Pseudo Multi-Perspective Transformer for UAV Segmentation","summary":"  The ascension of Unmanned Aerial Vehicles (UAVs) in various fields\nnecessitates effective UAV image segmentation, which faces challenges due to\nthe dynamic perspectives of UAV-captured images. Traditional segmentation\nalgorithms falter as they cannot accurately mimic the complexity of UAV\nperspectives, and the cost of obtaining multi-perspective labeled datasets is\nprohibitive. To address these issues, we introduce the PPTFormer, a novel\n\\textbf{P}seudo Multi-\\textbf{P}erspective \\textbf{T}rans\\textbf{former}\nnetwork that revolutionizes UAV image segmentation. Our approach circumvents\nthe need for actual multi-perspective data by creating pseudo perspectives for\nenhanced multi-perspective learning. The PPTFormer network boasts Perspective\nDecomposition, novel Perspective Prototypes, and a specialized encoder and\ndecoder that together achieve superior segmentation results through Pseudo\nMulti-Perspective Attention (PMP Attention) and fusion. Our experiments\ndemonstrate that PPTFormer achieves state-of-the-art performance across five\nUAV segmentation datasets, confirming its capability to effectively simulate\nUAV flight perspectives and significantly advance segmentation precision. This\nwork presents a pioneering leap in UAV scene understanding and sets a new\nbenchmark for future developments in semantic segmentation.\n","authors":["Deyi Ji","Wenwei Jin","Hongtao Lu","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.19632v1.pdf","comment":"IJCAI 2024"},{"id":"http://arxiv.org/abs/2406.19630v1","updated":"2024-06-28T03:36:38Z","published":"2024-06-28T03:36:38Z","title":"Optimal Video Compression using Pixel Shift Tracking","summary":"  The Video comprises approximately ~85\\% of all internet traffic, but video\nencoding/compression is being historically done with hard coded rules, which\nhas worked well but only to a certain limit. We have seen a surge in video\ncompression algorithms using ML-based models in the last few years and many of\nthem have outperformed several legacy codecs. The models range from encoding\nvideo end to end using an ML approach or replacing some intermediate steps in\nlegacy codecs using ML models to increase the efficiency of those steps.\n  Optimizing video storage is an essential aspect of video processing, so we\nare proposing one of the possible approaches to achieve it is by avoiding\nredundant data at each frame. In this paper, we want to introduce the approach\nof redundancies removal in subsequent frames for a given video as a main\napproach for video compression. We call this method Redundancy Removal using\nShift (R\\textsuperscript2S). This method can be utilized across various Machine\nLearning model algorithms, and make the compression more accessible and\nadaptable. In this study, we have utilized a computer vision-based pixel point\ntracking method to identify redundant pixels to encode video for optimal\nstorage.\n","authors":["Hitesh Saai Mananchery Panneerselvam","Smit Anand"],"pdf_url":"https://arxiv.org/pdf/2406.19630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18893v2","updated":"2024-06-28T03:22:33Z","published":"2024-06-27T05:08:46Z","title":"AlignIT: Enhancing Prompt Alignment in Customization of Text-to-Image\n  Models","summary":"  We consider the problem of customizing text-to-image diffusion models with\nuser-supplied reference images. Given new prompts, the existing methods can\ncapture the key concept from the reference images but fail to align the\ngenerated image with the prompt. In this work, we seek to address this key\nissue by proposing new methods that can easily be used in conjunction with\nexisting customization methods that optimize the embeddings/weights at various\nintermediate stages of the text encoding process.\n  The first contribution of this paper is a dissection of the various stages of\nthe text encoding process leading up to the conditioning vector for\ntext-to-image models. We take a holistic view of existing customization methods\nand notice that key and value outputs from this process differs substantially\nfrom their corresponding baseline (non-customized) models (e.g., baseline\nstable diffusion). While this difference does not impact the concept being\ncustomized, it leads to other parts of the generated image not being aligned\nwith the prompt. Further, we also observe that these keys and values allow\nindependent control various aspects of the final generation, enabling semantic\nmanipulation of the output. Taken together, the features spanning these keys\nand values, serve as the basis for our next contribution where we fix the\naforementioned issues with existing methods. We propose a new post-processing\nalgorithm, AlignIT, that infuses the keys and values for the concept of\ninterest while ensuring the keys and values for all other tokens in the input\nprompt are unchanged.\n  Our proposed method can be plugged in directly to existing customization\nmethods, leading to a substantial performance improvement in the alignment of\nthe final result with the input prompt while retaining the customization\nquality.\n","authors":["Aishwarya Agarwal","Srikrishna Karanam","Balaji Vasan Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2406.18893v2.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.16537v2","updated":"2024-06-28T03:21:15Z","published":"2024-06-24T11:16:37Z","title":"Character-Adapter: Prompt-Guided Region Control for High-Fidelity\n  Character Customization","summary":"  Customized image generation, which seeks to synthesize images with consistent\ncharacters, holds significant relevance for applications such as storytelling,\nportrait generation, and character design. However, previous approaches have\nencountered challenges in preserving characters with high-fidelity consistency\ndue to inadequate feature extraction and concept confusion of reference\ncharacters. Therefore, we propose Character-Adapter, a plug-and-play framework\ndesigned to generate images that preserve the details of reference characters,\nensuring high-fidelity consistency. Character-Adapter employs prompt-guided\nsegmentation to ensure fine-grained regional features of reference characters\nand dynamic region-level adapters to mitigate concept confusion. Extensive\nexperiments are conducted to validate the effectiveness of Character-Adapter.\nBoth quantitative and qualitative results demonstrate that Character-Adapter\nachieves the state-of-the-art performance of consistent character generation,\nwith an improvement of 24.8% compared with other methods. Our code will be\nreleased at https://github.com/Character-Adapter/Character-Adapte\n","authors":["Yuhang Ma","Wenting Xu","Jiji Tang","Qinfeng Jin","Rongsheng Zhang","Zeng Zhao","Changjie Fan","Zhipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2406.16537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06777v3","updated":"2024-06-28T03:07:29Z","published":"2024-06-10T20:25:18Z","title":"MolX: Enhancing Large Language Models for Molecular Learning with A\n  Multi-Modal Extension","summary":"  Recently, Large Language Models (LLMs) with their strong task-handling\ncapabilities have shown remarkable advancements across a spectrum of fields,\nmoving beyond natural language understanding. However, their proficiency within\nthe chemistry domain remains restricted, especially in solving professional\nmolecule-related tasks. This challenge is attributed to their inherent\nlimitations in comprehending molecules using only common textual\nrepresentations, i.e., SMILES strings. In this study, we seek to enhance the\nability of LLMs to comprehend molecules by designing and equipping them with a\nmulti-modal external module, namely MolX. In particular, instead of directly\nusing a SMILES string to represent a molecule, we utilize specific encoders to\nextract fine-grained features from both SMILES string and 2D molecular graph\nrepresentations for feeding into an LLM. Moreover, a human-defined molecular\nfingerprint is incorporated to leverage its embedded domain knowledge. Then, to\nestablish an alignment between MolX and the LLM's textual input space, the\nwhole model in which the LLM is frozen, is pre-trained with a versatile\nstrategy including a diverse set of tasks. Extensive experimental evaluations\ndemonstrate that our proposed method only introduces a small number of\ntrainable parameters while outperforming baselines on various downstream\nmolecule-related tasks ranging from molecule-to-text translation to\nretrosynthesis, with and without fine-tuning the LLM.\n","authors":["Khiem Le","Zhichun Guo","Kaiwen Dong","Xiaobao Huang","Bozhao Nan","Roshni Iyer","Xiangliang Zhang","Olaf Wiest","Wei Wang","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2406.06777v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18958v2","updated":"2024-06-28T02:47:07Z","published":"2024-06-27T07:40:59Z","title":"AnyControl: Create Your Artwork with Versatile Control on Text-to-Image\n  Generation","summary":"  The field of text-to-image (T2I) generation has made significant progress in\nrecent years, largely driven by advancements in diffusion models. Linguistic\ncontrol enables effective content creation, but struggles with fine-grained\ncontrol over image generation. This challenge has been explored, to a great\nextent, by incorporating additional user-supplied spatial conditions, such as\ndepth maps and edge maps, into pre-trained T2I models through extra encoding.\nHowever, multi-control image synthesis still faces several challenges.\nSpecifically, current approaches are limited in handling free combinations of\ndiverse input control signals, overlook the complex relationships among\nmultiple spatial conditions, and often fail to maintain semantic alignment with\nprovided textual prompts. This can lead to suboptimal user experiences. To\naddress these challenges, we propose AnyControl, a multi-control image\nsynthesis framework that supports arbitrary combinations of diverse control\nsignals. AnyControl develops a novel Multi-Control Encoder that extracts a\nunified multi-modal embedding to guide the generation process. This approach\nenables a holistic understanding of user inputs, and produces high-quality,\nfaithful results under versatile control signals, as demonstrated by extensive\nquantitative and qualitative evaluations. Our project page is available in\nhttps://any-control.github.io.\n","authors":["Yanan Sun","Yanchen Liu","Yinhao Tang","Wenjie Pei","Kai Chen"],"pdf_url":"https://arxiv.org/pdf/2406.18958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19602v1","updated":"2024-06-28T02:18:16Z","published":"2024-06-28T02:18:16Z","title":"A Survey on Deep Clustering: From the Prior Perspective","summary":"  Facilitated by the powerful feature extraction ability of neural networks,\ndeep clustering has achieved great success in analyzing high-dimensional and\ncomplex real-world data. The performance of deep clustering methods is affected\nby various factors such as network structures and learning objectives. However,\nas pointed out in this survey, the essence of deep clustering lies in the\nincorporation and utilization of prior knowledge, which is largely ignored by\nexisting works. From pioneering deep clustering methods based on data structure\nassumptions to recent contrastive clustering methods based on data augmentation\ninvariances, the development of deep clustering intrinsically corresponds to\nthe evolution of prior knowledge. In this survey, we provide a comprehensive\nreview of deep clustering methods by categorizing them into six types of prior\nknowledge. We find that in general the prior innovation follows two trends,\nnamely, i) from mining to constructing, and ii) from internal to external.\nBesides, we provide a benchmark on five widely-used datasets and analyze the\nperformance of methods with diverse priors. By providing a novel prior\nknowledge perspective, we hope this survey could provide some novel insights\nand inspire future research in the deep clustering community.\n","authors":["Yiding Lu","Haobin Li","Yunfan Li","Yijie Lin","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2406.19602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18915v2","updated":"2024-06-28T02:13:22Z","published":"2024-06-27T06:12:01Z","title":"Manipulate-Anything: Automating Real-World Robots using Vision-Language\n  Models","summary":"  Large-scale endeavors like RT-1 and widespread community efforts such as\nOpen-X-Embodiment have contributed to growing the scale of robot demonstration\ndata. However, there is still an opportunity to improve the quality, quantity,\nand diversity of robot demonstration data. Although vision-language models have\nbeen shown to automatically generate demonstration data, their utility has been\nlimited to environments with privileged state information, they require\nhand-designed skills, and are limited to interactions with few object\ninstances. We propose Manipulate-Anything, a scalable automated generation\nmethod for real-world robotic manipulation. Unlike prior work, our method can\noperate in real-world environments without any privileged state information,\nhand-designed skills, and can manipulate any static object. We evaluate our\nmethod using two setups. First, Manipulate-Anything successfully generates\ntrajectories for all 5 real-world and 12 simulation tasks, significantly\noutperforming existing methods like VoxPoser. Second, Manipulate-Anything's\ndemonstrations can train more robust behavior cloning policies than training\nwith human demonstrations, or from data generated by VoxPoser and\nCode-As-Policies. We believe Manipulate-Anything can be the scalable method for\nboth generating data for robotics and solving novel tasks in a zero-shot\nsetting.\n","authors":["Jiafei Duan","Wentao Yuan","Wilbert Pumacay","Yi Ru Wang","Kiana Ehsani","Dieter Fox","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2406.18915v2.pdf","comment":"Project page: https://robot-ma.github.io/"},{"id":"http://arxiv.org/abs/2406.14534v2","updated":"2024-06-28T02:12:20Z","published":"2024-06-20T17:47:30Z","title":"Epicardium Prompt-guided Real-time Cardiac Ultrasound Frame-to-volume\n  Registration","summary":"  A comprehensive guidance view for cardiac interventional surgery can be\nprovided by the real-time fusion of the intraoperative 2D images and\npreoperative 3D volume based on the ultrasound frame-to-volume registration.\nHowever, cardiac ultrasound images are characterized by a low signal-to-noise\nratio and small differences between adjacent frames, coupled with significant\ndimension variations between 2D frames and 3D volumes to be registered,\nresulting in real-time and accurate cardiac ultrasound frame-to-volume\nregistration being a very challenging task. This paper introduces a lightweight\nend-to-end Cardiac Ultrasound frame-to-volume Registration network, termed\nCU-Reg. Specifically, the proposed model leverages epicardium prompt-guided\nanatomical clues to reinforce the interaction of 2D sparse and 3D dense\nfeatures, followed by a voxel-wise local-global aggregation of enhanced\nfeatures, thereby boosting the cross-dimensional matching effectiveness of\nlow-quality ultrasound modalities. We further embed an inter-frame\ndiscriminative regularization term within the hybrid supervised learning to\nincrease the distinction between adjacent slices in the same ultrasound volume\nto ensure registration stability. Experimental results on the reprocessed CAMUS\ndataset demonstrate that our CU-Reg surpasses existing methods in terms of\nregistration accuracy and efficiency, meeting the guidance requirements of\nclinical cardiac interventional surgery.\n","authors":["Long Lei","Jun Zhou","Jialun Pei","Baoliang Zhao","Yueming Jin","Yuen-Chun Jeremy Teoh","Jing Qin","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2406.14534v2.pdf","comment":"This paper has been accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.11445v2","updated":"2024-06-28T01:24:45Z","published":"2024-06-17T11:57:14Z","title":"Solving the Inverse Problem of Electrocardiography for Cardiac Digital\n  Twins: A Survey","summary":"  Cardiac digital twins are personalized virtual representations used to\nunderstand complex heart mechanisms. Solving the ECG inverse problem is crucial\nfor accurate virtual heart modelling, enabling the derivation of internal\nelectrical activity information from recorded surface potentials. Despite\nchallenges from cardiac complexity, noisy ECG data, and computational\nefficiency, recent advancements hold significant promise for enhancing virtual\nheart modelling, ultimately advancing precision medicine in cardiology. This\npaper aims to provide a comprehensive review of the methods of solving ECG\ninverse problem, the validation strategies, the clinical applications, and\nfuture perspectives. For the computing methodologies, we broadly classify\nstate-of-the-art approaches into two categories: deterministic and\nprobabilistic methods, including conventional and deep learning-based\ntechniques. Integrating physics laws with deep learning models holds promise,\nbut challenges such as capturing dynamic electrophysiology accurately,\naccessing accurate domain knowledge, and quantifying prediction uncertainty\npersist. Integrating models into clinical workflows while ensuring\ninterpretability and usability for healthcare professionals is essential.\nOvercoming these challenges will drive further research in cardiac digital\ntwins.\n","authors":["Lei Li","Julia Camps","Blanca Rodriguez","Vicente Grau"],"pdf_url":"https://arxiv.org/pdf/2406.11445v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04940v2","updated":"2024-06-28T01:23:10Z","published":"2024-05-08T10:15:04Z","title":"Harnessing the Power of MLLMs for Transferable Text-to-Image Person ReID","summary":"  Text-to-image person re-identification (ReID) retrieves pedestrian images\naccording to textual descriptions. Manually annotating textual descriptions is\ntime-consuming, restricting the scale of existing datasets and therefore the\ngeneralization ability of ReID models. As a result, we study the transferable\ntext-to-image ReID problem, where we train a model on our proposed large-scale\ndatabase and directly deploy it to various datasets for evaluation. We obtain\nsubstantial training data via Multi-modal Large Language Models (MLLMs).\nMoreover, we identify and address two key challenges in utilizing the obtained\ntextual descriptions. First, an MLLM tends to generate descriptions with\nsimilar structures, causing the model to overfit specific sentence patterns.\nThus, we propose a novel method that uses MLLMs to caption images according to\nvarious templates. These templates are obtained using a multi-turn dialogue\nwith a Large Language Model (LLM). Therefore, we can build a large-scale\ndataset with diverse textual descriptions. Second, an MLLM may produce\nincorrect descriptions. Hence, we introduce a novel method that automatically\nidentifies words in a description that do not correspond with the image. This\nmethod is based on the similarity between one text and all patch token\nembeddings in the image. Then, we mask these words with a larger probability in\nthe subsequent training epoch, alleviating the impact of noisy textual\ndescriptions. The experimental results demonstrate that our methods\nsignificantly boost the direct transfer text-to-image ReID performance.\nBenefiting from the pre-trained model weights, we also achieve state-of-the-art\nperformance in the traditional evaluation settings.\n","authors":["Wentao Tan"],"pdf_url":"https://arxiv.org/pdf/2405.04940v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2406.19593v1","updated":"2024-06-28T01:14:43Z","published":"2024-06-28T01:14:43Z","title":"SK-VQA: Synthetic Knowledge Generation at Scale for Training\n  Context-Augmented Multimodal LLMs","summary":"  Synthetic data generation has gained significant attention recently for its\nutility in training large vision and language models. However, the application\nof synthetic data to the training of multimodal context-augmented generation\nsystems has been relatively unexplored. This gap in existing work is important\nbecause existing vision and language models (VLMs) are not trained specifically\nfor context-augmented generation. Resources for adapting such models are\ntherefore crucial for enabling their use in retrieval-augmented generation\n(RAG) settings, where a retriever is used to gather relevant information that\nis then subsequently provided to a generative model via context augmentation.\nTo address this challenging problem, we generate SK-VQA: a large synthetic\nmultimodal dataset containing over 2 million question-answer pairs which\nrequire external knowledge to determine the final answer. Our dataset is both\nlarger and significantly more diverse than existing resources of its kind,\npossessing over 11x more unique questions and containing images from a greater\nvariety of sources than previously-proposed datasets. Through extensive\nexperiments, we demonstrate that our synthetic dataset can not only serve as a\nchallenging benchmark, but is also highly effective for adapting existing\ngenerative multimodal models for context-augmented generation.\n","authors":["Xin Su","Man Luo","Kris W Pan","Tien Pei Chou","Vasudev Lal","Phillip Howard"],"pdf_url":"https://arxiv.org/pdf/2406.19593v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2406.20098v1","updated":"2024-06-28T17:59:46Z","published":"2024-06-28T17:59:46Z","title":"Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework\n  for Multimodal LLMs","summary":"  Multimodal large language models (MLLMs) have shown impressive success across\nmodalities such as image, video, and audio in a variety of understanding and\ngeneration tasks. However, current MLLMs are surprisingly poor at understanding\nwebpage screenshots and generating their corresponding HTML code. To address\nthis problem, we propose Web2Code, a benchmark consisting of a new large-scale\nwebpage-to-code dataset for instruction tuning and an evaluation framework for\nthe webpage understanding and HTML code translation abilities of MLLMs. For\ndataset construction, we leverage pretrained LLMs to enhance existing\nwebpage-to-code datasets as well as generate a diverse pool of new webpages\nrendered into images. Specifically, the inputs are webpage images and\ninstructions, while the responses are the webpage's HTML code. We further\ninclude diverse natural language QA pairs about the webpage content in the\nresponses to enable a more comprehensive understanding of the web content. To\nevaluate model performance in these tasks, we develop an evaluation framework\nfor testing MLLMs' abilities in webpage understanding and web-to-code\ngeneration. Extensive experiments show that our proposed dataset is beneficial\nnot only to our proposed tasks but also in the general visual domain, while\nprevious datasets result in worse performance. We hope our work will contribute\nto the development of general MLLMs suitable for web-based content generation\nand task automation. Our data and code will be available at\nhttps://github.com/MBZUAI-LLM/web2code.\n","authors":["Sukmin Yun","Haokun Lin","Rusiru Thushara","Mohammad Qazim Bhat","Yongxin Wang","Zutao Jiang","Mingkai Deng","Jinhong Wang","Tianhua Tao","Junbo Li","Haonan Li","Preslav Nakov","Timothy Baldwin","Zhengzhong Liu","Eric P. Xing","Xiaodan Liang","Zhiqiang Shen"],"pdf_url":"https://arxiv.org/pdf/2406.20098v1.pdf","comment":"Website at https://mbzuai-llm.github.io/webpage2code/"},{"id":"http://arxiv.org/abs/2406.20095v1","updated":"2024-06-28T17:59:12Z","published":"2024-06-28T17:59:12Z","title":"LLaRA: Supercharging Robot Learning Data for Vision-Language Policy","summary":"  Large Language Models (LLMs) equipped with extensive world knowledge and\nstrong reasoning skills can tackle diverse tasks across domains, often by\nposing them as conversation-style instruction-response pairs. In this paper, we\npropose LLaRA: Large Language and Robotics Assistant, a framework which\nformulates robot action policy as conversations, and provides improved\nresponses when trained with auxiliary data that complements policy learning.\nLLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity\nto process state information as visual-textual prompts and generate optimal\npolicy decisions in text. To train such action policy VLMs, we first introduce\nan automated pipeline to generate diverse high-quality robotics instruction\ndata from existing behavior cloning data. A VLM finetuned with the resulting\ncollection of datasets based on a conversation-style formulation tailored for\nrobotics tasks, can generate meaningful robot action policy decisions. Our\nexperiments across multiple simulated and real-world environments demonstrate\nthe state-of-the-art performance of the proposed LLaRA framework. The code,\ndatasets, and pretrained models are available at\nhttps://github.com/LostXine/LLaRA.\n","authors":["Xiang Li","Cristina Mata","Jongwoo Park","Kumara Kahatapitiya","Yoo Sung Jang","Jinghuan Shang","Kanchana Ranasinghe","Ryan Burgert","Mu Cai","Yong Jae Lee","Michael S. Ryoo"],"pdf_url":"https://arxiv.org/pdf/2406.20095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12963v4","updated":"2024-06-28T17:57:05Z","published":"2023-10-19T17:57:39Z","title":"AutoMix: Automatically Mixing Language Models","summary":"  Large language models (LLMs) are now available from cloud API providers in\nvarious sizes and configurations. While this diversity offers a broad spectrum\nof choices, effectively leveraging the options to optimize computational cost\nand performance remains challenging. In this work, we present Automix, an\napproach that strategically routes queries to larger LMs, based on the\napproximate correctness of outputs from a smaller LM. Central to Automix are\ntwo key technical contributions. First, it has a few-shot self-verification\nmechanism, which estimates the reliability of its own outputs without requiring\nextensive training. Second, given that self-verification can be noisy, it\nemploys a POMDP based router that can effectively select an appropriately sized\nmodel, based on answer confidence. Experiments across five language models and\nfive challenging datasets show that Automix consistently surpasses strong\nbaselines, reducing computational cost by over 50% for comparable performance.\n","authors":["Pranjal Aggarwal","Aman Madaan","Ankit Anand","Srividya Pranavi Potharaju","Swaroop Mishra","Pei Zhou","Aditya Gupta","Dheeraj Rajagopal","Karthik Kappaganthu","Yiming Yang","Shyam Upadhyay","Manaal Faruqui"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2310.12963v4.pdf","comment":"The first two authors contributed equally. Work started and partly\n  done during Aman's internship at Google. This version adds results on\n  additional models and datasets"},{"id":"http://arxiv.org/abs/2406.20087v1","updated":"2024-06-28T17:55:24Z","published":"2024-06-28T17:55:24Z","title":"ProgressGym: Alignment with a Millennium of Moral Progress","summary":"  Frontier AI systems, including large language models (LLMs), hold increasing\ninfluence over the epistemology of human users. Such influence can reinforce\nprevailing societal values, potentially contributing to the lock-in of\nmisguided moral beliefs and, consequently, the perpetuation of problematic\nmoral practices on a broad scale. We introduce progress alignment as a\ntechnical solution to mitigate this imminent risk. Progress alignment\nalgorithms learn to emulate the mechanics of human moral progress, thereby\naddressing the susceptibility of existing alignment methods to contemporary\nmoral blindspots. To empower research in progress alignment, we introduce\nProgressGym, an experimental framework allowing the learning of moral progress\nmechanics from history, in order to facilitate future progress in real-world\nmoral decisions. Leveraging 9 centuries of historical text and 18 historical\nLLMs, ProgressGym enables codification of real-world progress alignment\nchallenges into concrete benchmarks. Specifically, we introduce three core\nchallenges: tracking evolving values (PG-Follow), preemptively anticipating\nmoral progress (PG-Predict), and regulating the feedback loop between human and\nAI value shifts (PG-Coevolve). Alignment methods without a temporal dimension\nare inapplicable to these tasks. In response, we present lifelong and\nextrapolative algorithms as baseline methods of progress alignment, and build\nan open leaderboard soliciting novel algorithms and challenges. The framework\nand the leaderboard are available at\nhttps://github.com/PKU-Alignment/ProgressGym and\nhttps://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard\nrespectively.\n","authors":["Tianyi Qiu","Yang Zhang","Xuchuan Huang","Jasmine Xinze Li","Jiaming Ji","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2406.20087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20080v1","updated":"2024-06-28T17:45:25Z","published":"2024-06-28T17:45:25Z","title":"AI for Extreme Event Modeling and Understanding: Methodologies and\n  Challenges","summary":"  In recent years, artificial intelligence (AI) has deeply impacted various\nfields, including Earth system sciences. Here, AI improved weather forecasting,\nmodel emulation, parameter estimation, and the prediction of extreme events.\nHowever, the latter comes with specific challenges, such as developing accurate\npredictors from noisy, heterogeneous and limited annotated data. This paper\nreviews how AI is being used to analyze extreme events (like floods, droughts,\nwildfires and heatwaves), highlighting the importance of creating accurate,\ntransparent, and reliable AI models. We discuss the hurdles of dealing with\nlimited data, integrating information in real-time, deploying models, and\nmaking them understandable, all crucial for gaining the trust of stakeholders\nand meeting regulatory needs. We provide an overview of how AI can help\nidentify and explain extreme events more effectively, improving disaster\nresponse and communication. We emphasize the need for collaboration across\ndifferent fields to create AI solutions that are practical, understandable, and\ntrustworthy for analyzing and predicting extreme events. Such collaborative\nefforts aim to enhance disaster readiness and disaster risk reduction.\n","authors":["Gustau Camps-Valls","Miguel-Ángel Fernández-Torres","Kai-Hendrik Cohrs","Adrian Höhl","Andrea Castelletti","Aytac Pacal","Claire Robin","Francesco Martinuzzi","Ioannis Papoutsis","Ioannis Prapas","Jorge Pérez-Aracil","Katja Weigel","Maria Gonzalez-Calabuig","Markus Reichstein","Martin Rabel","Matteo Giuliani","Miguel Mahecha","Oana-Iuliana Popescu","Oscar J. Pellicer-Valero","Said Ouala","Sancho Salcedo-Sanz","Sebastian Sippel","Spyros Kondylatos","Tamara Happé","Tristan Williams"],"pdf_url":"https://arxiv.org/pdf/2406.20080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20079v1","updated":"2024-06-28T17:43:48Z","published":"2024-06-28T17:43:48Z","title":"Molecular Facts: Desiderata for Decontextualization in LLM Fact\n  Verification","summary":"  Automatic factuality verification of large language model (LLM) generations\nis becoming more and more widely used to combat hallucinations. A major point\nof tension in the literature is the granularity of this fact-checking: larger\nchunks of text are hard to fact-check, but more atomic facts like propositions\nmay lack context to interpret correctly. In this work, we assess the role of\ncontext in these atomic facts. We argue that fully atomic facts are not the\nright representation, and define two criteria for molecular facts:\ndecontextuality, or how well they can stand alone, and minimality, or how\nlittle extra information is added to achieve decontexuality. We quantify the\nimpact of decontextualization on minimality, then present a baseline\nmethodology for generating molecular facts automatically, aiming to add the\nright amount of information. We compare against various methods of\ndecontextualization and find that molecular facts balance minimality with fact\nverification accuracy in ambiguous settings.\n","authors":["Anisha Gunjal","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2406.20079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18757v2","updated":"2024-06-28T17:12:37Z","published":"2024-06-26T20:55:26Z","title":"The Impact of Feature Representation on the Accuracy of Photonic Neural\n  Networks","summary":"  Photonic Neural Networks (PNNs) are gaining significant interest in the\nresearch community due to their potential for high parallelization, low\nlatency, and energy efficiency. PNNs compute using light, which leads to\nseveral differences in implementation when compared to electronics, such as the\nneed to represent input features in the photonic domain before feeding them\ninto the network. In this encoding process, it is common to combine multiple\nfeatures into a single input to reduce the number of inputs and associated\ndevices, leading to smaller and more energy-efficient PNNs. Although this\nalters the network's handling of input data, its impact on PNNs remains\nunderstudied. This paper addresses this open question, investigating the effect\nof commonly used encoding strategies that combine features on the performance\nand learning capabilities of PNNs. Here, using the concept of feature\nimportance, we develop a mathematical methodology for analyzing feature\ncombination. Through this methodology, we demonstrate that encoding multiple\nfeatures together in a single input determines their relative importance, thus\nlimiting the network's ability to learn from the data. Given some prior\nknowledge of the data, however, this can also be leveraged for higher accuracy.\nBy selecting an optimal encoding method, we achieve up to a 12.3% improvement\nin accuracy of PNNs trained on the Iris dataset compared to other encoding\ntechniques, surpassing the performance of networks where features are not\ncombined. These findings highlight the importance of carefully choosing the\nencoding to the accuracy and decision-making strategies of PNNs, particularly\nin size or power constrained applications.\n","authors":["Mauricio Gomes de Queiroz","Paul Jimenez","Raphael Cardoso","Mateus Vidaletti Costa","Mohab Abdalla","Ian O'Connor","Alberto Bosio","Fabio Pavanello"],"pdf_url":"https://arxiv.org/pdf/2406.18757v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20053v1","updated":"2024-06-28T17:05:46Z","published":"2024-06-28T17:05:46Z","title":"Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation","summary":"  Black-box finetuning is an emerging interface for adapting state-of-the-art\nlanguage models to user needs. However, such access may also let malicious\nactors undermine model safety. To demonstrate the challenge of defending\nfinetuning interfaces, we introduce covert malicious finetuning, a method to\ncompromise model safety via finetuning while evading detection. Our method\nconstructs a malicious dataset where every individual datapoint appears\ninnocuous, but finetuning on the dataset teaches the model to respond to\nencoded harmful requests with encoded harmful responses. Applied to GPT-4, our\nmethod produces a finetuned model that acts on harmful instructions 99% of the\ntime and avoids detection by defense mechanisms such as dataset inspection,\nsafety evaluations, and input/output classifiers. Our findings question whether\nblack-box finetuning access can be secured against sophisticated adversaries.\n","authors":["Danny Halawi","Alexander Wei","Eric Wallace","Tony T. Wang","Nika Haghtalab","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2406.20053v1.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2406.12315v2","updated":"2024-06-28T17:03:03Z","published":"2024-06-18T06:37:26Z","title":"PruningBench: A Comprehensive Benchmark of Structural Pruning","summary":"  Structural pruning has emerged as a promising approach for producing more\nefficient models. Nevertheless, the community suffers from a lack of\nstandardized benchmarks and metrics, leaving the progress in this area not\nfully comprehended. To fill this gap, we present the first comprehensive\nbenchmark, termed \\textit{PruningBench}, for structural pruning. PruningBench\nshowcases the following three characteristics: 1) PruningBench employs a\nunified and consistent framework for evaluating the effectiveness of diverse\nstructural pruning techniques; 2) PruningBench systematically evaluates 16\nexisting pruning methods, encompassing a wide array of models (e.g., CNNs and\nViTs) and tasks (e.g., classification and detection); 3) PruningBench provides\neasily implementable interfaces to facilitate the implementation of future\npruning methods, and enables the subsequent researchers to incorporate their\nwork into our leaderboards. We provide an online pruning platform\nhttp://pruning.vipazoo.cn for customizing pruning tasks and reproducing all\nresults in this paper. Codes will be made publicly on\nhttps://github.com/HollyLee2000/PruningBench.\n","authors":["Haoling Li","Changhao Li","Mengqi Xue","Gongfan Fang","Sheng Zhou","Zunlei Feng","Huiqiong Wang","Yong Wang","Lechao Cheng","Mingli Song","Jie Song"],"pdf_url":"https://arxiv.org/pdf/2406.12315v2.pdf","comment":"Submitted to NeurIPS 2024 Datasets and Benchmarks Track"},{"id":"http://arxiv.org/abs/2406.20044v1","updated":"2024-06-28T16:53:06Z","published":"2024-06-28T16:53:06Z","title":"Electrostatics-based particle sampling and approximate inference","summary":"  A new particle-based sampling and approximate inference method, based on\nelectrostatics and Newton mechanics principles, is introduced with theoretical\nground, algorithm design and experimental validation. This method simulates an\ninteracting particle system (IPS) where particles, i.e. the freely-moving\nnegative charges and spatially-fixed positive charges with magnitudes\nproportional to the target distribution, interact with each other via\nattraction and repulsion induced by the resulting electric fields described by\nPoisson's equation. The IPS evolves towards a steady-state where the\ndistribution of negative charges conforms to the target distribution. This\nphysics-inspired method offers deterministic, gradient-free sampling and\ninference, achieving comparable performance as other particle-based and MCMC\nmethods in benchmark tasks of inferring complex densities, Bayesian logistic\nregression and dynamical system identification. A discrete-time, discrete-space\nalgorithmic design, readily extendable to continuous time and space, is\nprovided for usage in more general inference problems occurring in\nprobabilistic machine learning scenarios such as Bayesian inference, generative\nmodelling, and beyond.\n","authors":["Yongchao Huang"],"pdf_url":"https://arxiv.org/pdf/2406.20044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.20041v1","updated":"2024-06-28T16:39:20Z","published":"2024-06-28T16:39:20Z","title":"BMW Agents -- A Framework For Task Automation Through Multi-agent\n  Collaboration","summary":"  Autonomous agents driven by Large Language Models (LLMs) offer enormous\npotential for automation. Early proof of this technology can be found in\nvarious demonstrations of agents solving complex tasks, interacting with\nexternal systems to augment their knowledge, and triggering actions. In\nparticular, workflows involving multiple agents solving complex tasks in a\ncollaborative fashion exemplify their capacity to operate in less strict and\nless well-defined environments. Thus, a multi-agent approach has great\npotential for serving as a backbone in many industrial applications, ranging\nfrom complex knowledge retrieval systems to next generation robotic process\nautomation. Given the reasoning abilities within the current generation of\nLLMs, complex processes require a multi-step approach that includes a plan of\nwell-defined and modular tasks. Depending on the level of complexity, these\ntasks can be executed either by a single agent or a group of agents. In this\nwork, we focus on designing a flexible agent engineering framework with careful\nattention to planning and execution, capable of handling complex use case\napplications across various domains. The proposed framework provides\nreliability in industrial applications and presents techniques to ensure a\nscalable, flexible, and collaborative workflow for multiple autonomous agents\nworking together towards solving tasks.\n","authors":["Noel Crawford","Edward B. Duffy","Iman Evazzade","Torsten Foehr","Gregory Robbins","Debbrata Kumar Saha","Jiya Varma","Marcin Ziolkowski"],"pdf_url":"https://arxiv.org/pdf/2406.20041v1.pdf","comment":"24 pages. 21 PDF images"},{"id":"http://arxiv.org/abs/2309.16035v2","updated":"2024-06-28T16:21:45Z","published":"2023-09-27T21:26:03Z","title":"MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical\n  Question Answering","summary":"  Large Language Models (LLMs), although powerful in general domains, often\nperform poorly on domain-specific tasks like medical question answering (QA).\nMoreover, they tend to function as \"black-boxes,\" making it challenging to\nmodify their behavior. To address the problem, our study delves into retrieval\naugmented generation (RAG), aiming to improve LLM responses without the need\nfor fine-tuning or retraining. Specifically, we propose a comprehensive\nretrieval strategy to extract medical facts from an external knowledge base,\nand then inject them into the query prompt for LLMs. Focusing on medical QA\nusing the MedQA-SMILE dataset, we evaluate the impact of different retrieval\nmodels and the number of facts provided to the LLM. Notably, our\nretrieval-augmented Vicuna-7B model exhibited an accuracy improvement from\n44.46% to 48.54%. This work underscores the potential of RAG to enhance LLM\nperformance, offering a practical approach to mitigate the challenges of\nblack-box LLMs.\n","authors":["Yucheng Shi","Shaochen Xu","Tianze Yang","Zhengliang Liu","Tianming Liu","Xiang Li","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2309.16035v2.pdf","comment":"Accepted by AMIA 2024 Annual Symposium"},{"id":"http://arxiv.org/abs/2406.20031v1","updated":"2024-06-28T16:20:22Z","published":"2024-06-28T16:20:22Z","title":"Pairwise Difference Learning for Classification","summary":"  Pairwise difference learning (PDL) has recently been introduced as a new\nmeta-learning technique for regression. Instead of learning a mapping from\ninstances to outcomes in the standard way, the key idea is to learn a function\nthat takes two instances as input and predicts the difference between the\nrespective outcomes. Given a function of this kind, predictions for a query\ninstance are derived from every training example and then averaged. This paper\nextends PDL toward the task of classification and proposes a meta-learning\ntechnique for inducing a PDL classifier by solving a suitably defined (binary)\nclassification problem on a paired version of the original training data. We\nanalyze the performance of the PDL classifier in a large-scale empirical study\nand find that it outperforms state-of-the-art methods in terms of prediction\nperformance. Last but not least, we provide an easy-to-use and publicly\navailable implementation of PDL in a Python package.\n","authors":["Mohamed Karim Belaid","Maximilian Rabus","Eyke Hüllermeier"],"pdf_url":"https://arxiv.org/pdf/2406.20031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18492v2","updated":"2024-06-28T16:12:39Z","published":"2024-05-28T18:01:52Z","title":"LLMs and Memorization: On Quality and Specificity of Copyright\n  Compliance","summary":"  Memorization in large language models (LLMs) is a growing concern. LLMs have\nbeen shown to easily reproduce parts of their training data, including\ncopyrighted work. This is an important problem to solve, as it may violate\nexisting copyright laws as well as the European AI Act. In this work, we\npropose a systematic analysis to quantify the extent of potential copyright\ninfringements in LLMs using European law as an example. Unlike previous work,\nwe evaluate instruction-finetuned models in a realistic end-user scenario. Our\nanalysis builds on a proposed threshold of 160 characters, which we borrow from\nthe German Copyright Service Provider Act and a fuzzy text matching algorithm\nto identify potentially copyright-infringing textual reproductions. The\nspecificity of countermeasures against copyright infringement is analyzed by\ncomparing model behavior on copyrighted and public domain data. We investigate\nwhat behaviors models show instead of producing protected text (such as refusal\nor hallucination) and provide a first legal assessment of these behaviors. We\nfind that there are huge differences in copyright compliance, specificity, and\nappropriate refusal among popular LLMs. Alpaca, GPT 4, GPT 3.5, and Luminous\nperform best in our comparison, with OpenGPT-X, Alpaca, and Luminous producing\na particularly low absolute number of potential copyright violations. Code will\nbe published soon.\n","authors":["Felix B Mueller","Rebekka Görge","Anna K Bernzen","Janna C Pirk","Maximilian Poretschkin"],"pdf_url":"https://arxiv.org/pdf/2405.18492v2.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2406.20015v1","updated":"2024-06-28T16:03:30Z","published":"2024-06-28T16:03:30Z","title":"ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for\n  Tool-Augmented Large Language Models","summary":"  Tool-augmented large language models (LLMs) are rapidly being integrated into\nreal-world applications. Due to the lack of benchmarks, the community still\nneeds to fully understand the hallucination issues within these models. To\naddress this challenge, we introduce a comprehensive diagnostic benchmark,\nToolBH. Specifically, we assess the LLM's hallucinations through two\nperspectives: depth and breadth. In terms of depth, we propose a multi-level\ndiagnostic process, including (1) solvability detection, (2) solution planning,\nand (3) missing-tool analysis. For breadth, we consider three scenarios based\non the characteristics of the toolset: missing necessary tools, potential\ntools, and limited functionality tools. Furthermore, we developed seven tasks\nand collected 700 evaluation samples through multiple rounds of manual\nannotation. The results show the significant challenges presented by the ToolBH\nbenchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve a\ntotal score of 45.3 and 37.0, respectively, on a scale of 100. In this\nbenchmark, larger model parameters do not guarantee better performance; the\ntraining data and response strategies also play a crucial role in tool-enhanced\nLLM scenarios. Our diagnostic analysis indicates that the primary reason for\nmodel errors lies in assessing task solvability. Additionally, open-weight\nmodels suffer from performance drops with verbose replies, whereas proprietary\nmodels excel with longer reasoning.\n","authors":["Yuxiang Zhang","Jing Chen","Junjie Wang","Yaxin Liu","Cheng Yang","Chufan Shi","Xinyu Zhu","Zihao Lin","Hanwen Wan","Yujiu Yang","Tetsuya Sakai","Tian Feng","Hayato Yamana"],"pdf_url":"https://arxiv.org/pdf/2406.20015v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04376v2","updated":"2024-06-28T15:36:50Z","published":"2024-02-06T20:30:19Z","title":"Scaling laws for learning with real and surrogate data","summary":"  Collecting large quantities of high-quality data can be prohibitively\nexpensive or impractical, and a bottleneck in machine learning. One may instead\naugment a small set of $n$ data points from the target distribution with data\nfrom more accessible sources, e.g. data collected under different circumstances\nor synthesized by generative models. We refer to such data as `surrogate data.'\nWe introduce a weighted empirical risk minimization (ERM) approach for\nintegrating surrogate data into training. We analyze mathematically this method\nunder several classical statistical models, and validate our findings\nempirically on datasets from different domains. Our main findings are: $(i)$\nIntegrating surrogate data can significantly reduce the test error on the\noriginal distribution. Surprisingly, this can happen even when the surrogate\ndata is unrelated to the original ones. We trace back this behavior to the\nclassical Stein's paradox. $(ii)$ In order to reap the benefit of surrogate\ndata, it is crucial to use optimally weighted ERM. $(iii)$ The test error of\nmodels trained on mixtures of real and surrogate data is approximately\ndescribed by a scaling law. This scaling law can be used to predict the optimal\nweighting scheme, and to choose the amount of surrogate data to add.\n","authors":["Ayush Jain","Andrea Montanari","Eren Sasoglu"],"pdf_url":"https://arxiv.org/pdf/2402.04376v2.pdf","comment":"Added new experiments"},{"id":"http://arxiv.org/abs/2405.14105v2","updated":"2024-06-28T15:34:26Z","published":"2024-05-23T02:14:17Z","title":"Distributed Speculative Inference of Large Language Models","summary":"  Accelerating the inference of large language models (LLMs) is an important\nchallenge in artificial intelligence. This paper introduces distributed\nspeculative inference (DSI), a novel distributed inference algorithm that is\nprovably faster than speculative inference (SI) [leviathan2023fast,\nchen2023accelerating, miao2023specinfer] and traditional autoregressive\ninference (non-SI). Like other SI algorithms, DSI works on frozen LLMs,\nrequiring no training or architectural modifications, and it preserves the\ntarget distribution.\n  Prior studies on SI have demonstrated empirical speedups (compared to non-SI)\nbut require a fast and accurate drafter LLM. In practice, off-the-shelf LLMs\noften do not have matching drafters that are sufficiently fast and accurate. We\nshow a gap: SI gets slower than non-SI when using slower or less accurate\ndrafters. We close this gap by proving that DSI is faster than both SI and\nnon-SI given any drafters. By orchestrating multiple instances of the target\nand drafters, DSI is not only faster than SI but also supports LLMs that cannot\nbe accelerated with SI.\n  Our simulations show speedups of off-the-shelf LLMs in realistic settings:\nDSI is 1.29-1.92x faster than SI.\n","authors":["Nadav Timor","Jonathan Mamou","Daniel Korat","Moshe Berchansky","Oren Pereg","Moshe Wasserblat","Tomer Galanti","Michal Gordon","David Harel"],"pdf_url":"https://arxiv.org/pdf/2405.14105v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19997v1","updated":"2024-06-28T15:32:59Z","published":"2024-06-28T15:32:59Z","title":"Wavelets Are All You Need for Autoregressive Image Generation","summary":"  In this paper, we take a new approach to autoregressive image generation that\nis based on two main ingredients. The first is wavelet image coding, which\nallows to tokenize the visual details of an image from coarse to fine details\nby ordering the information starting with the most significant bits of the most\nsignificant wavelet coefficients. The second is a variant of a language\ntransformer whose architecture is re-designed and optimized for token sequences\nin this 'wavelet language'. The transformer learns the significant statistical\ncorrelations within a token sequence, which are the manifestations of\nwell-known correlations between the wavelet subbands at various resolutions. We\nshow experimental results with conditioning on the generation process.\n","authors":["Wael Mattar","Idan Levy","Nir Sharon","Shai Dekel"],"pdf_url":"https://arxiv.org/pdf/2406.19997v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.19995v1","updated":"2024-06-28T15:27:57Z","published":"2024-06-28T15:27:57Z","title":"Single Parent Family: A Spectrum of Family Members from a Single\n  Pre-Trained Foundation Model","summary":"  This paper introduces a novel method of Progressive Low Rank Decomposition\n(PLRD) tailored for the compression of large language models. Our approach\nleverages a pre-trained model, which is then incrementally decompressed to\nsmaller sizes using progressively lower ranks. This method allows for\nsignificant reductions in computational overhead and energy consumption, as\nsubsequent models are derived from the original without the need for retraining\nfrom scratch. We detail the implementation of PLRD, which strategically\ndecreases the tensor ranks, thus optimizing the trade-off between model\nperformance and resource usage. The efficacy of PLRD is demonstrated through\nextensive experiments showing that models trained with PLRD method on only 1B\ntokens maintain comparable performance with traditionally trained models while\nusing 0.1% of the tokens. The versatility of PLRD is highlighted by its ability\nto generate multiple model sizes from a single foundational model, adapting\nfluidly to varying computational and memory budgets. Our findings suggest that\nPLRD could set a new standard for the efficient scaling of LLMs, making\nadvanced AI more feasible on diverse platforms.\n","authors":["Habib Hajimolahoseini","Mohammad Hassanpour","Foozhan Ataiefard","Boxing Chen","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2406.19995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11658v2","updated":"2024-06-28T15:16:53Z","published":"2024-02-18T17:32:53Z","title":"Dynamic planning in hierarchical active inference","summary":"  By dynamic planning, we refer to the ability of the human brain to infer and\nimpose motor trajectories related to cognitive decisions. A recent paradigm,\nactive inference, brings fundamental insights into the adaptation of biological\norganisms, constantly striving to minimize prediction errors to restrict\nthemselves to life-compatible states. Over the past years, many studies have\nshown how human and animal behavior could be explained in terms of an active\ninferential process - either as discrete decision-making or continuous motor\ncontrol - inspiring innovative solutions in robotics and artificial\nintelligence. Still, the literature lacks a comprehensive outlook on how to\neffectively plan actions in changing environments. Setting ourselves the goal\nof modeling tool use, we delve into the topic of dynamic planning in active\ninference, keeping in mind two crucial aspects of biological goal-directed\nbehavior: the capacity to understand and exploit affordances for object\nmanipulation, and to learn the hierarchical interactions between the self and\nthe environment, including other agents. We start from a simple unit and\ngradually describe more advanced structures, comparing recently proposed design\nchoices and providing basic examples for each section. This study distances\nitself from traditional views centered on neural networks and reinforcement\nlearning, and points toward a yet unexplored direction in active inference:\nhybrid representations in hierarchical models.\n","authors":["Matteo Priorelli","Ivilin Peev Stoianov"],"pdf_url":"https://arxiv.org/pdf/2402.11658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06045v2","updated":"2024-06-28T15:10:03Z","published":"2023-09-12T08:29:53Z","title":"Improved Monte Carlo tree search (MCTS) formulation with multiple root\n  nodes for discrete sizing optimization of truss structures","summary":"  This paper proposes a new method for discrete optimum design of truss\nstructures utilizing Monte Carlo tree search (MCTS) with update process, the\nbest reward, accelerating technique, and terminal condition. An improved MCTS\nformulation with multiple root nodes is developed in this study. Update process\nmeans that once a final solution is found, it is used as the initial solution\nfor next search tree. The best reward is used in the backpropagation step.\nAccelerating technique is introduced by decreasing the width of search tree and\nreducing maximum number of iterations. The agent is trained to minimize the\ntotal structural weight under various constraints until the terminal condition\nis satisfied. Then, optimal solution is the minimum value of all solutions\nfound by search trees. These numerical examples show that the agent can find\noptimal solution with low computational cost, stably produces an optimal\ndesign, and is suitable for practical engineering problems.\n","authors":["Fu-Yao Ko","Katsuyuki Suzuki","Kazuo Yonekura"],"pdf_url":"https://arxiv.org/pdf/2309.06045v2.pdf","comment":"28 pages, 20 figures, 14 tables"},{"id":"http://arxiv.org/abs/2406.19967v1","updated":"2024-06-28T14:56:21Z","published":"2024-06-28T14:56:21Z","title":"Into the Unknown: Generating Geospatial Descriptions for New\n  Environments","summary":"  Similar to vision-and-language navigation (VLN) tasks that focus on bridging\nthe gap between vision and language for embodied navigation, the new Rendezvous\n(RVS) task requires reasoning over allocentric spatial relationships\n(independent of the observer's viewpoint) using non-sequential navigation\ninstructions and maps. However, performance substantially drops in new\nenvironments with no training data. Using opensource descriptions paired with\ncoordinates (e.g., Wikipedia) provides training data but suffers from limited\nspatially-oriented text resulting in low geolocation resolution. We propose a\nlarge-scale augmentation method for generating high-quality synthetic data for\nnew environments using readily available geospatial data. Our method constructs\na grounded knowledge-graph, capturing entity relationships. Sampled entities\nand relations (`shop north of school') generate navigation instructions via (i)\ngenerating numerous templates using context-free grammar (CFG) to embed\nspecific entities and relations; (ii) feeding the entities and relation into a\nlarge language model (LLM) for instruction generation. A comprehensive\nevaluation on RVS, showed that our approach improves the 100-meter accuracy by\n45.83% on unseen environments. Furthermore, we demonstrate that models trained\nwith CFG-based augmentation achieve superior performance compared with those\ntrained with LLM-based augmentation, both in unseen and seen environments.\nThese findings suggest that the potential advantages of explicitly structuring\nspatial information for text-based geospatial reasoning in previously unknown,\ncan unlock data-scarce scenarios.\n","authors":["Tzuf Paz-Argaman","John Palowitch","Sayali Kulkarni","Reut Tsarfaty","Jason Baldridge"],"pdf_url":"https://arxiv.org/pdf/2406.19967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19963v1","updated":"2024-06-28T14:51:01Z","published":"2024-06-28T14:51:01Z","title":"Text2Robot: Evolutionary Robot Design from Text Descriptions","summary":"  Robot design has traditionally been costly and labor-intensive. Despite\nadvancements in automated processes, it remains challenging to navigate a vast\ndesign space while producing physically manufacturable robots. We introduce\nText2Robot, a framework that converts user text specifications and performance\npreferences into physical quadrupedal robots. Within minutes, Text2Robot can\nuse text-to-3D models to provide strong initializations of diverse\nmorphologies. Within a day, our geometric processing algorithms and\nbody-control co-optimization produce a walking robot by explicitly considering\nreal-world electronics and manufacturability. Text2Robot enables rapid\nprototyping and opens new opportunities for robot design with generative\nmodels.\n","authors":["Ryan P. Ringel","Zachary S. Charlick","Jiaxun Liu","Boxi Xia","Boyuan Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19963v1.pdf","comment":"Our project website is at: https://generalroboticslab.com/Text2Robot"},{"id":"http://arxiv.org/abs/2406.19953v1","updated":"2024-06-28T14:39:21Z","published":"2024-06-28T14:39:21Z","title":"Uncovering the hidden core-periphery structure in hyperbolic networks","summary":"  The hyperbolic network models exhibit very fundamental and essential\nfeatures, like small-worldness, scale-freeness, high-clustering coefficient,\nand community structure. In this paper, we comprehensively explore the presence\nof an important feature, the core-periphery structure, in the hyperbolic\nnetwork models, which is often exhibited by real-world networks. We focused on\nwell-known hyperbolic models such as popularity-similarity optimization model\n(PSO) and S1/H2 models and studied core-periphery structures using a\nwell-established method that is based on standard random walk Markov chain\nmodel. The observed core-periphery centralization values indicate that the\ncore-periphery structure can be very pronounced under certain conditions. We\nalso validate our findings by statistically testing for the significance of the\nobserved core-periphery structure in the network geometry. This study extends\nnetwork science and reveals core-periphery insights applicable to various\ndomains, enhancing network performance and resiliency in transportation and\ninformation systems.\n","authors":["Imran Ansari","Pawanesh Yadav","Niteesh Sahni"],"pdf_url":"https://arxiv.org/pdf/2406.19953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19934v1","updated":"2024-06-28T14:04:10Z","published":"2024-06-28T14:04:10Z","title":"From the Least to the Most: Building a Plug-and-Play Visual Reasoner via\n  Data Synthesis","summary":"  We explore multi-step reasoning in vision-language models (VLMs). The problem\nis challenging, as reasoning data consisting of multiple steps of visual and\nlanguage processing are barely available. To overcome the challenge, we first\nintroduce a least-to-most visual reasoning paradigm, which interleaves steps of\ndecomposing a question into sub-questions and invoking external tools for\nresolving sub-questions. Based on the paradigm, we further propose a novel data\nsynthesis approach that can automatically create questions and multi-step\nreasoning paths for an image in a bottom-up manner. Our approach divides the\ncomplex synthesis task into a few simple sub-tasks, and (almost entirely)\nrelies on open-sourced models to accomplish the sub-tasks. Therefore, the\nentire synthesis process is reproducible and cost-efficient, and the\nsynthesized data is quality guaranteed. With the approach, we construct $50$k\nvisual reasoning examples. Then, we develop a visual reasoner through\nsupervised fine-tuning, which is capable of generally enhancing the reasoning\nabilities of a wide range of existing VLMs in a plug-and-play fashion.\nExtensive experiments indicate that the visual reasoner can consistently and\nsignificantly improve four VLMs on four VQA benchmarks. Our code and dataset\nare available at https://github.com/steven-ccq/VisualReasoner.\n","authors":["Chuanqi Cheng","Jian Guan","Wei Wu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2406.19934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19931v1","updated":"2024-06-28T14:01:22Z","published":"2024-06-28T14:01:22Z","title":"Decoupling General and Personalized Knowledge in Federated Learning via\n  Additive and Low-Rank Decomposition","summary":"  To address data heterogeneity, the key strategy of Personalized Federated\nLearning (PFL) is to decouple general knowledge (shared among clients) and\nclient-specific knowledge, as the latter can have a negative impact on\ncollaboration if not removed. Existing PFL methods primarily adopt a parameter\npartitioning approach, where the parameters of a model are designated as one of\ntwo types: parameters shared with other clients to extract general knowledge\nand parameters retained locally to learn client-specific knowledge. However, as\nthese two types of parameters are put together like a jigsaw puzzle into a\nsingle model during the training process, each parameter may simultaneously\nabsorb both general and client-specific knowledge, thus struggling to separate\nthe two types of knowledge effectively. In this paper, we introduce FedDecomp,\na simple but effective PFL paradigm that employs parameter additive\ndecomposition to address this issue. Instead of assigning each parameter of a\nmodel as either a shared or personalized one, FedDecomp decomposes each\nparameter into the sum of two parameters: a shared one and a personalized one,\nthus achieving a more thorough decoupling of shared and personalized knowledge\ncompared to the parameter partitioning method. In addition, as we find that\nretaining local knowledge of specific clients requires much lower model\ncapacity compared with general knowledge across all clients, we let the matrix\ncontaining personalized parameters be low rank during the training process.\nMoreover, a new alternating training strategy is proposed to further improve\nthe performance. Experimental results across multiple datasets and varying\ndegrees of data heterogeneity demonstrate that FedDecomp outperforms\nstate-of-the-art methods up to 4.9\\%.\n","authors":["Xinghao Wu","Xuefeng Liu","Jianwei Niu","Haolin Wang","Shaojie Tang","Guogang Zhu","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2406.19931v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.01317v2","updated":"2024-06-28T13:27:36Z","published":"2024-06-03T13:29:36Z","title":"The Intelligible and Effective Graph Neural Additive Networks","summary":"  Graph Neural Networks (GNNs) have emerged as the predominant approach for\nlearning over graph-structured data. However, most GNNs operate as black-box\nmodels and require post-hoc explanations, which may not suffice in high-stakes\nscenarios where transparency is crucial. In this paper, we present a GNN that\nis interpretable by design. Our model, Graph Neural Additive Network (GNAN), is\na novel extension of the interpretable class of Generalized Additive Models,\nand can be visualized and fully understood by humans. GNAN is designed to be\nfully interpretable, allowing both global and local explanations at the feature\nand graph levels through direct visualization of the model. These\nvisualizations describe the exact way the model uses the relationships between\nthe target variable, the features, and the graph. We demonstrate the\nintelligibility of GNANs in a series of examples on different tasks and\ndatasets. In addition, we show that the accuracy of GNAN is on par with\nblack-box GNNs, making it suitable for critical applications where transparency\nis essential, alongside high accuracy.\n","authors":["Maya Bechler-Speicher","Amir Globerson","Ran Gilad-Bachrach"],"pdf_url":"https://arxiv.org/pdf/2406.01317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19896v1","updated":"2024-06-28T13:04:16Z","published":"2024-06-28T13:04:16Z","title":"AuthAttLyzer-V2: Unveiling Code Authorship Attribution using Enhanced\n  Ensemble Learning Models & Generating Benchmark Dataset","summary":"  Source Code Authorship Attribution (SCAA) is crucial for software\nclassification because it provides insights into the origin and behavior of\nsoftware. By accurately identifying the author or group behind a piece of code,\nexperts can better understand the motivations and techniques of developers. In\nthe cybersecurity era, this attribution helps trace the source of malicious\nsoftware, identify patterns in the code that may indicate specific threat\nactors or groups, and ultimately enhance threat intelligence and mitigation\nstrategies. This paper presents AuthAttLyzer-V2, a new source code feature\nextractor for SCAA, focusing on lexical, semantic, syntactic, and N-gram\nfeatures. Our research explores author identification in C++ by examining\n24,000 source code samples from 3,000 authors. Our methodology integrates\nRandom Forest, Gradient Boosting, and XGBoost models, enhanced with SHAP for\ninterpretability. The study demonstrates how ensemble models can effectively\ndiscern individual coding styles, offering insights into the unique attributes\nof code authorship. This approach is pivotal in understanding and interpreting\ncomplex patterns in authorship attribution, especially for malware\nclassification.\n","authors":["Bhaskar Joshi","Sepideh HajiHossein Khani","Arash HabibiLashkari"],"pdf_url":"https://arxiv.org/pdf/2406.19896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19888v1","updated":"2024-06-28T12:54:10Z","published":"2024-06-28T12:54:10Z","title":"Fine-tuning of Geospatial Foundation Models for Aboveground Biomass\n  Estimation","summary":"  Global vegetation structure mapping is critical for understanding the global\ncarbon cycle and maximizing the efficacy of nature-based carbon sequestration\ninitiatives. Moreover, vegetation structure mapping can help reduce the impacts\nof climate change by, for example, guiding actions to improve water security,\nincrease biodiversity and reduce flood risk. Global satellite measurements\nprovide an important set of observations for monitoring and managing\ndeforestation and degradation of existing forests, natural forest regeneration,\nreforestation, biodiversity restoration, and the implementation of sustainable\nagricultural practices. In this paper, we explore the effectiveness of\nfine-tuning of a geospatial foundation model to estimate above-ground biomass\n(AGB) using space-borne data collected across different eco-regions in Brazil.\nThe fine-tuned model architecture consisted of a Swin-B transformer as the\nencoder (i.e., backbone) and a single convolutional layer for the decoder head.\nAll results were compared to a U-Net which was trained as the baseline model\nExperimental results of this sparse-label prediction task demonstrate that the\nfine-tuned geospatial foundation model with a frozen encoder has comparable\nperformance to a U-Net trained from scratch. This is despite the fine-tuned\nmodel having 13 times less parameters requiring optimization, which saves both\ntime and compute resources. Further, we explore the transfer-learning\ncapabilities of the geospatial foundation models by fine-tuning on satellite\nimagery with sparse labels from different eco-regions in Brazil.\n","authors":["Michal Muszynski","Levente Klein","Ademir Ferreira da Silva","Anjani Prasad Atluri","Carlos Gomes","Daniela Szwarcman","Gurkanwar Singh","Kewen Gu","Maciel Zortea","Naomi Simumba","Paolo Fraccaro","Shraddha Singh","Steve Meliksetian","Campbell Watson","Daiki Kimura","Harini Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2406.19888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19874v1","updated":"2024-06-28T12:28:52Z","published":"2024-06-28T12:28:52Z","title":"Detecting Subtle Differences between Human and Model Languages Using\n  Spectrum of Relative Likelihood","summary":"  Human and model-generated texts can be distinguished by examining the\nmagnitude of likelihood in language. However, it is becoming increasingly\ndifficult as language model's capabilities of generating human-like texts keep\nevolving. This study provides a new perspective by using the relative\nlikelihood values instead of absolute ones, and extracting useful features from\nthe spectrum-view of likelihood for the human-model text detection task. We\npropose a detection procedure with two classification methods, supervised and\nheuristic-based, respectively, which results in competitive performances with\nprevious zero-shot detection methods and a new state-of-the-art on short-text\ndetection. Our method can also reveal subtle differences between human and\nmodel languages, which find theoretical roots in psycholinguistics studies. Our\ncode is available at https://github.com/CLCS-SUSTech/FourierGPT\n","authors":["Yang Xu","Yu Wang","Hao An","Zhichen Liu","Yongyuan Li"],"pdf_url":"https://arxiv.org/pdf/2406.19874v1.pdf","comment":"13 pages, 12 figures"},{"id":"http://arxiv.org/abs/2406.19859v1","updated":"2024-06-28T11:58:26Z","published":"2024-06-28T11:58:26Z","title":"MetaDesigner: Advancing Artistic Typography through AI-Driven,\n  User-Centric, and Multilingual WordArt Synthesis","summary":"  MetaDesigner revolutionizes artistic typography synthesis by leveraging the\nstrengths of Large Language Models (LLMs) to drive a design paradigm centered\naround user engagement. At the core of this framework lies a multi-agent system\ncomprising the Pipeline, Glyph, and Texture agents, which collectively enable\nthe creation of customized WordArt, ranging from semantic enhancements to the\nimposition of complex textures. MetaDesigner incorporates a comprehensive\nfeedback mechanism that harnesses insights from multimodal models and user\nevaluations to refine and enhance the design process iteratively. Through this\nfeedback loop, the system adeptly tunes hyperparameters to align with\nuser-defined stylistic and thematic preferences, generating WordArt that not\nonly meets but exceeds user expectations of visual appeal and contextual\nrelevance. Empirical validations highlight MetaDesigner's capability to\neffectively serve diverse WordArt applications, consistently producing\naesthetically appealing and context-sensitive results.\n","authors":["Jun-Yan He","Zhi-Qi Cheng","Chenyang Li","Jingdong Sun","Qi He","Wangmeng Xiang","Hanyuan Chen","Jin-Peng Lan","Xianhui Lin","Kang Zhu","Bin Luo","Yifeng Geng","Xuansong Xie","Alexander G. Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2406.19859v1.pdf","comment":"18 pages, 16 figures, Project:\n  https://modelscope.cn/studios/WordArt/WordArt"},{"id":"http://arxiv.org/abs/2406.19853v1","updated":"2024-06-28T11:52:53Z","published":"2024-06-28T11:52:53Z","title":"YuLan: An Open-source Large Language Model","summary":"  Large language models (LLMs) have become the foundation of many applications,\nleveraging their extensive capabilities in processing and understanding natural\nlanguage. While many open-source LLMs have been released with technical\nreports, the lack of training details hinders further research and development.\nThis paper presents the development of YuLan, a series of open-source LLMs with\n$12$ billion parameters. The base model of YuLan is pre-trained on\napproximately $1.7$T tokens derived from a diverse corpus, including massive\nEnglish, Chinese, and multilingual texts. We design a three-stage pre-training\nmethod to enhance YuLan's overall capabilities. Subsequent phases of training\nincorporate instruction-tuning and human alignment, employing a substantial\nvolume of high-quality synthesized data. To facilitate the learning of complex\nand long-tail knowledge, we devise a curriculum-learning framework throughout\nacross these stages, which helps LLMs learn knowledge in an easy-to-hard\nmanner. YuLan's training is finished on Jan, 2024 and has achieved performance\non par with state-of-the-art LLMs across various English and Chinese\nbenchmarks. This paper outlines a comprehensive technical roadmap for\ndeveloping LLMs from scratch. Our model and codes are available at\nhttps://github.com/RUC-GSAI/YuLan-Chat.\n","authors":["Yutao Zhu","Kun Zhou","Kelong Mao","Wentong Chen","Yiding Sun","Zhipeng Chen","Qian Cao","Yihan Wu","Yushuo Chen","Feng Wang","Lei Zhang","Junyi Li","Xiaolei Wang","Lei Wang","Beichen Zhang","Zican Dong","Xiaoxue Cheng","Yuhan Chen","Xinyu Tang","Yupeng Hou","Qiangqiang Ren","Xincheng Pang","Shufang Xie","Wayne Xin Zhao","Zhicheng Dou","Jiaxin Mao","Yankai Lin","Ruihua Song","Jun Xu","Xu Chen","Rui Yan","Zhewei Wei","Di Hu","Wenbing Huang","Ze-Feng Gao","Yueguo Chen","Weizheng Lu","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2406.19853v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13699v2","updated":"2024-06-28T11:49:52Z","published":"2024-01-22T03:17:41Z","title":"Generative AI-Driven Human Digital Twin in IoT-Healthcare: A\n  Comprehensive Survey","summary":"  The Internet of things (IoT) can significantly enhance the quality of human\nlife, specifically in healthcare, attracting extensive attentions to\nIoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as\nan innovative paradigm that can comprehensively characterize the replication of\nthe individual human body in the digital world and reflect its physical status\nin real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the\napplication of healthcare monitoring by acting as a versatile and vivid human\ndigital testbed, simulating the outcomes and guiding the practical treatments.\nHowever, successfully establishing HDT requires high-fidelity virtual modeling\nand strong information interactions but possibly with scarce, biased and noisy\ndata. Fortunately, a recent popular technology called generative artificial\nintelligence (GAI) may be a promising solution because it can leverage advanced\nAI algorithms to automatically create, manipulate, and modify valuable while\ndiverse data. This survey particularly focuses on the implementation of\nGAI-driven HDT in IoT-healthcare. We start by introducing the background of\nIoT-healthcare and the potential of GAI-driven HDT. Then, we delve into the\nfundamental techniques and present the overall framework of GAI-driven HDT.\nAfter that, we explore the realization of GAI-driven HDT in detail, including\nGAI-enabled data acquisition, communication, data management, digital modeling,\nand data analysis. Besides, we discuss typical IoT-healthcare applications that\ncan be revolutionized by GAI-driven HDT, namely personalized health monitoring\nand diagnosis, personalized prescription, and personalized rehabilitation.\nFinally, we conclude this survey by highlighting some future research\ndirections.\n","authors":["Jiayuan Chen","You Shi","Changyan Yi","Hongyang Du","Jiawen Kang","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2401.13699v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15114v3","updated":"2024-06-28T11:49:51Z","published":"2024-03-22T11:16:11Z","title":"Solving a Real-World Package Delivery Routing Problem Using Quantum\n  Annealers","summary":"  Research focused on the conjunction between quantum computing and routing\nproblems has been very prolific in recent years. Most of the works revolve\naround classical problems such as the Traveling Salesman Problem or the Vehicle\nRouting Problem. The real-world applicability of these problems is dependent on\nthe objectives and constraints considered. Anyway, it is undeniable that it is\noften difficult to translate complex requirements into these classical\nformulations.The main objective of this research is to present a solving scheme\nfor dealing with realistic instances while maintaining all the characteristics\nand restrictions of the original real-world problem. Thus, a quantum-classical\nstrategy has been developed, coined Q4RPD, that considers a set of real\nconstraints such as a heterogeneous fleet of vehicles, priority deliveries, and\ncapacities characterized by two values: weight and dimensions of the packages.\nQ4RPD resorts to the Leap Constrained Quadratic Model Hybrid Solver of D-Wave.\nTo demonstrate the application of Q4RPD, an experimentation composed of six\ndifferent instances has been conducted, aiming to serve as illustrative\nexamples.\n","authors":["Eneko Osaba","Esther Villar-Rodriguez","Antón Asla"],"pdf_url":"https://arxiv.org/pdf/2403.15114v3.pdf","comment":"16 pages, 11 figures and 4 tables. Paper submitted for review in\n  Scientific Reports"},{"id":"http://arxiv.org/abs/2306.02766v3","updated":"2024-06-28T11:39:10Z","published":"2023-06-05T10:45:39Z","title":"Networked Communication for Decentralised Agents in Mean-Field Games","summary":"  We introduce networked communication to the mean-field game framework, in\nparticular to oracle-free settings where $N$ decentralised agents learn along a\nsingle, non-episodic run of the empirical system. We prove that our\narchitecture, with only a few reasonable assumptions about network structure,\nhas sample guarantees bounded between those of the centralised- and\nindependent-learning cases. We discuss how the sample guarantees of the three\ntheoretical algorithms do not actually result in practical convergence. We\ntherefore show that in practical settings where the theoretical parameters are\nnot observed (leading to poor estimation of the Q-function), our communication\nscheme significantly accelerates convergence over the independent case (and\noften even the centralised case), without relying on the assumption of a\ncentralised learner. We contribute further practical enhancements to all three\ntheoretical algorithms, allowing us to present their first empirical\ndemonstrations. Our experiments confirm that we can remove several of the\ntheoretical assumptions of the algorithms, and display the empirical\nconvergence benefits brought by our new networked communication. We\nadditionally show that the networked approach has significant advantages, over\nboth the centralised and independent alternatives, in terms of robustness to\nunexpected learning failures and to changes in population size.\n","authors":["Patrick Benjamin","Alessandro Abate"],"pdf_url":"https://arxiv.org/pdf/2306.02766v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02772v6","updated":"2024-06-28T11:33:07Z","published":"2023-10-04T12:42:21Z","title":"Spike Accumulation Forwarding for Effective Training of Spiking Neural\n  Networks","summary":"  In this article, we propose a new paradigm for training spiking neural\nnetworks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are\nenergy-efficient but difficult to train. Consequently, many researchers have\nproposed various methods to solve this problem, among which online training\nthrough time (OTTT) is a method that allows inferring at each time step while\nsuppressing the memory cost. However, to compute efficiently on GPUs, OTTT\nrequires operations with spike trains and weighted summation of spike trains\nduring forwarding. In addition, OTTT has shown a relationship with the Spike\nRepresentation, an alternative training method, though theoretical agreement\nwith Spike Representation has yet to be proven. Our proposed method can solve\nthese problems; namely, SAF can halve the number of operations during the\nforward process, and it can be theoretically proven that SAF is consistent with\nthe Spike Representation and OTTT, respectively. Furthermore, we confirmed the\nabove contents through experiments and showed that it is possible to reduce\nmemory and training time while maintaining accuracy.\n","authors":["Ryuji Saiin","Tomoya Shirakawa","Sota Yoshihara","Yoshihide Sawada","Hiroyuki Kusumoto"],"pdf_url":"https://arxiv.org/pdf/2310.02772v6.pdf","comment":"14 pages, 5 figures, Appendix:10 pages, 2 figures, v6:Published in\n  Transactions on Machine Learning Research"},{"id":"http://arxiv.org/abs/2406.19840v1","updated":"2024-06-28T11:28:44Z","published":"2024-06-28T11:28:44Z","title":"AnomaLLMy -- Detecting anomalous tokens in black-box LLMs through\n  low-confidence single-token predictions","summary":"  This paper introduces AnomaLLMy, a novel technique for the automatic\ndetection of anomalous tokens in black-box Large Language Models (LLMs) with\nAPI-only access. Utilizing low-confidence single-token predictions as a\ncost-effective indicator, AnomaLLMy identifies irregularities in model\nbehavior, addressing the issue of anomalous tokens degrading the quality and\nreliability of models. Validated on the cl100k_base dataset, the token set of\nGPT-4, AnomaLLMy detected 413 major and 65 minor anomalies, demonstrating the\nmethod's efficiency with just \\$24.39 spent in API credits. The insights from\nthis research are expected to be beneficial for enhancing the robustness of and\naccuracy of LLMs, particularly in the development and assessment of tokenizers.\n","authors":["Waligóra Witold"],"pdf_url":"https://arxiv.org/pdf/2406.19840v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2312.10170v4","updated":"2024-06-28T11:25:41Z","published":"2023-12-15T19:37:39Z","title":"UINav: A Practical Approach to Train On-Device Automation Agents","summary":"  Automation systems that can autonomously drive application user interfaces to\ncomplete user tasks are of great benefit, especially when users are\nsituationally or permanently impaired. Prior automation systems do not produce\ngeneralizable models while AI-based automation agents work reliably only in\nsimple, hand-crafted applications or incur high computation costs. We propose\nUINav, a demonstration-based approach to train automation agents that fit\nmobile devices, yet achieving high success rates with modest numbers of\ndemonstrations. To reduce the demonstration overhead, UINav uses a referee\nmodel that provides users with immediate feedback on tasks where the agent\nfails, and automatically augments human demonstrations to increase diversity in\ntraining data. Our evaluation shows that with only 10 demonstrations UINav can\nachieve 70% accuracy, and that with enough demonstrations it can surpass 90%\naccuracy.\n","authors":["Wei Li","Fu-Lin Hsu","Will Bishop","Folawiyo Campbell-Ajala","Max Lin","Oriana Riva"],"pdf_url":"https://arxiv.org/pdf/2312.10170v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19820v1","updated":"2024-06-28T10:53:48Z","published":"2024-06-28T10:53:48Z","title":"BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for\n  Multi-hop Question Answering","summary":"  Large language models (LLMs) have demonstrated strong reasoning capabilities.\nNevertheless, they still suffer from factual errors when tackling\nknowledge-intensive tasks. Retrieval-augmented reasoning represents a promising\napproach. However, significant challenges still persist, including inaccurate\nand insufficient retrieval for complex questions, as well as difficulty in\nintegrating multi-source knowledge. To address this, we propose Beam\nAggregation Reasoning, BeamAggR, a reasoning framework for knowledge-intensive\nmulti-hop QA. BeamAggR explores and prioritizes promising answers at each hop\nof question. Concretely, we parse the complex questions into trees, which\ninclude atom and composite questions, followed by bottom-up reasoning. For\natomic questions, the LLM conducts reasoning on multi-source knowledge to get\nanswer candidates. For composite questions, the LLM combines beam candidates,\nexplores multiple reasoning paths through probabilistic aggregation, and\nprioritizes the most promising trajectory. Extensive experiments on four\nopen-domain multi-hop reasoning datasets show that our method significantly\noutperforms SOTA methods by 8.5%. Furthermore, our analysis reveals that\nBeamAggR elicits better knowledge collaboration and answer aggregation.\n","authors":["Zheng Chu","Jingchang Chen","Qianglong Chen","Haotian Wang","Kun Zhu","Xiyuan Du","Weijiang Yu","Ming Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2406.19820v1.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2406.19815v1","updated":"2024-06-28T10:45:37Z","published":"2024-06-28T10:45:37Z","title":"Emotion Loss Attacking: Adversarial Attack Perception for Skeleton based\n  on Multi-dimensional Features","summary":"  Adversarial attack on skeletal motion is a hot topic. However, existing\nresearches only consider part of dynamic features when measuring distance\nbetween skeleton graph sequences, which results in poor imperceptibility. To\nthis end, we propose a novel adversarial attack method to attack action\nrecognizers for skeletal motions. Firstly, our method systematically proposes a\ndynamic distance function to measure the difference between skeletal motions.\nMeanwhile, we innovatively introduce emotional features for complementary\ninformation. In addition, we use Alternating Direction Method of\nMultipliers(ADMM) to solve the constrained optimization problem, which\ngenerates adversarial samples with better imperceptibility to deceive the\nclassifiers. Experiments show that our method is effective on multiple action\nclassifiers and datasets. When the perturbation magnitude measured by l norms\nis the same, the dynamic perturbations generated by our method are much lower\nthan that of other methods. What's more, we are the first to prove the\neffectiveness of emotional features, and provide a new idea for measuring the\ndistance between skeletal motions.\n","authors":["Feng Liu","Qing Xu","Qijian Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.19815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19812v1","updated":"2024-06-28T10:41:17Z","published":"2024-06-28T10:41:17Z","title":"Fuzzy Logic Guided Reward Function Variation: An Oracle for Testing\n  Reinforcement Learning Programs","summary":"  Reinforcement Learning (RL) has gained significant attention across various\ndomains. However, the increasing complexity of RL programs presents testing\nchallenges, particularly the oracle problem: defining the correctness of the RL\nprogram. Conventional human oracles struggle to cope with the complexity,\nleading to inefficiencies and potential unreliability in RL testing. To\nalleviate this problem, we propose an automated oracle approach that leverages\nRL properties using fuzzy logic. Our oracle quantifies an agent's behavioral\ncompliance with reward policies and analyzes its trend over training episodes.\nIt labels an RL program as \"Buggy\" if the compliance trend violates\nexpectations derived from RL characteristics. We evaluate our oracle on RL\nprograms with varying complexities and compare it with human oracles. Results\nshow that while human oracles perform well in simpler testing scenarios, our\nfuzzy oracle demonstrates superior performance in complex environments. The\nproposed approach shows promise in addressing the oracle problem for RL\ntesting, particularly in complex cases where manual testing falls short. It\noffers a potential solution to improve the efficiency, reliability, and\nscalability of RL program testing. This research takes a step towards automated\ntesting of RL programs and highlights the potential of fuzzy logic-based\noracles in tackling the oracle problem.\n","authors":["Shiyu Zhang","Haoyang Song","Qixin Wang","Yu Pei"],"pdf_url":"https://arxiv.org/pdf/2406.19812v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.10133v2","updated":"2024-06-28T10:41:02Z","published":"2024-02-15T17:37:25Z","title":"Zero-Shot Reasoning: Personalized Content Generation Without the Cold\n  Start Problem","summary":"  Procedural content generation uses algorithmic techniques to create large\namounts of new content for games at much lower production costs. In newer\napproaches, procedural content generation utilizes machine learning. However,\nthese methods usually require expensive collection of large amounts of data, as\nwell as the development and training of fairly complex learning models, which\ncan be both extremely time-consuming and expensive. The core of our research is\nto explore whether we can lower the barrier to the use of personalized\nprocedural content generation through a more practical and generalizable\napproach with large language models. Matching game content with player\npreferences benefits both players, who enjoy the game more, and developers, who\nincreasingly depend on players enjoying the game before being able to monetize\nit. Therefore, this paper presents a novel approach to achieving\npersonalization by using large language models to propose levels based on the\ngameplay data continuously collected from individual players. We compared the\nlevels generated using our approach with levels generated with more traditional\nprocedural generation techniques. Our easily reproducible method has proven\nviable in a production setting and outperformed levels generated by traditional\nmethods in the probability that a player will not quit the game mid-level.\n","authors":["Davor Hafnar","Jure Demšar"],"pdf_url":"https://arxiv.org/pdf/2402.10133v2.pdf","comment":"9 pages, 6 figures. Paper accepted to IEEE Transactions on Games"},{"id":"http://arxiv.org/abs/2311.17667v2","updated":"2024-06-28T10:40:26Z","published":"2023-11-29T14:30:16Z","title":"TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in\n  Large Language Models","summary":"  Grasping the concept of time is a fundamental facet of human cognition,\nindispensable for truly comprehending the intricacies of the world. Previous\nstudies typically focus on specific aspects of time, lacking a comprehensive\ntemporal reasoning benchmark. To address this, we propose TimeBench, a\ncomprehensive hierarchical temporal reasoning benchmark that covers a broad\nspectrum of temporal reasoning phenomena. TimeBench provides a thorough\nevaluation for investigating the temporal reasoning capabilities of large\nlanguage models. We conduct extensive experiments on GPT-4, LLaMA2, and other\npopular LLMs under various settings. Our experimental results indicate a\nsignificant performance gap between the state-of-the-art LLMs and humans,\nhighlighting that there is still a considerable distance to cover in temporal\nreasoning. Besides, LLMs exhibit capability discrepancies across different\nreasoning categories. Furthermore, we thoroughly analyze the impact of multiple\naspects on temporal reasoning and emphasize the associated challenges. We\naspire for TimeBench to serve as a comprehensive benchmark, fostering research\nin temporal reasoning. Resources are available at:\nhttps://github.com/zchuz/TimeBench\n","authors":["Zheng Chu","Jingchang Chen","Qianglong Chen","Weijiang Yu","Haotian Wang","Ming Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2311.17667v2.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2402.18485v3","updated":"2024-06-28T10:35:56Z","published":"2024-02-28T17:06:54Z","title":"A Multimodal Foundation Agent for Financial Trading: Tool-Augmented,\n  Diversified, and Generalist","summary":"  Financial trading is a crucial component of the markets, informed by a\nmultimodal information landscape encompassing news, prices, and Kline charts,\nand encompasses diverse tasks such as quantitative trading and high-frequency\ntrading with various assets. While advanced AI techniques like deep learning\nand reinforcement learning are extensively utilized in finance, their\napplication in financial trading tasks often faces challenges due to inadequate\nhandling of multimodal data and limited generalizability across various tasks.\nTo address these challenges, we present FinAgent, a multimodal foundational\nagent with tool augmentation for financial trading. FinAgent's market\nintelligence module processes a diverse range of data-numerical, textual, and\nvisual-to accurately analyze the financial market. Its unique dual-level\nreflection module not only enables rapid adaptation to market dynamics but also\nincorporates a diversified memory retrieval system, enhancing the agent's\nability to learn from historical data and improve decision-making processes.\nThe agent's emphasis on reasoning for actions fosters trust in its financial\ndecisions. Moreover, FinAgent integrates established trading strategies and\nexpert insights, ensuring that its trading approaches are both data-driven and\nrooted in sound financial principles. With comprehensive experiments on 6\nfinancial datasets, including stocks and Crypto, FinAgent significantly\noutperforms 9 state-of-the-art baselines in terms of 6 financial metrics with\nover 36% average improvement on profit. Specifically, a 92.27% return (a 84.39%\nrelative improvement) is achieved on one dataset. Notably, FinAgent is the\nfirst advanced multimodal foundation agent designed for financial trading\ntasks.\n","authors":["Wentao Zhang","Lingxuan Zhao","Haochong Xia","Shuo Sun","Jiaze Sun","Molei Qin","Xinyi Li","Yuqing Zhao","Yilei Zhao","Xinyu Cai","Longtao Zheng","Xinrun Wang","Bo An"],"pdf_url":"https://arxiv.org/pdf/2402.18485v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19807v1","updated":"2024-06-28T10:30:46Z","published":"2024-06-28T10:30:46Z","title":"Deceptive Diffusion: Generating Synthetic Adversarial Examples","summary":"  We introduce the concept of deceptive diffusion -- training a generative AI\nmodel to produce adversarial images. Whereas a traditional adversarial attack\nalgorithm aims to perturb an existing image to induce a misclassificaton, the\ndeceptive diffusion model can create an arbitrary number of new, misclassified\nimages that are not directly associated with training or test images. Deceptive\ndiffusion offers the possibility of strengthening defence algorithms by\nproviding adversarial training data at scale, including types of\nmisclassification that are otherwise difficult to find. In our experiments, we\nalso investigate the effect of training on a partially attacked data set. This\nhighlights a new type of vulnerability for generative diffusion models: if an\nattacker is able to stealthily poison a portion of the training data, then the\nresulting diffusion model will generate a similar proportion of misleading\noutputs.\n","authors":["Lucas Beerens","Catherine F. Higham","Desmond J. Higham"],"pdf_url":"https://arxiv.org/pdf/2406.19807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16783v2","updated":"2024-06-28T10:14:53Z","published":"2024-06-24T16:45:13Z","title":"M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in\n  Large Language Models","summary":"  Instruction finetuning (IFT) is critical for aligning Large Language Models\n(LLMs) to follow instructions. While many effective IFT datasets have been\nintroduced recently, they predominantly focus on high-resource languages like\nEnglish. To better align LLMs across a broad spectrum of languages and tasks,\nwe propose a fully synthetic, novel taxonomy (Evol) guided Multilingual,\nMulti-turn instruction finetuning dataset, called M2Lingual. It is constructed\nby first selecting a diverse set of seed examples and then utilizing the\nproposed Evol taxonomy to convert these seeds into complex and challenging\nmulti-turn instructions. We demonstrate the effectiveness of M2Lingual by\ntraining LLMs of varying sizes and showcasing the enhanced performance across a\ndiverse set of languages. We contribute the 2 step Evol taxonomy with the\nguided generation code: https://github.com/ServiceNow/M2Lingual, as well as the\nfirst fully synthetic, general and task-oriented, multi-turn, multilingual\ndataset built with Evol - M2Lingual:\nhttps://huggingface.co/datasets/ServiceNow-AI/ M2Lingual - containing 182K\ntotal IFT pairs, covering 70 languages and 17+ NLP tasks.\n","authors":["Rishabh Maheshwary","Vikas Yadav","Hoang Nguyen","Khyati Mahajan","Sathwik Tejaswi Madhusudhan"],"pdf_url":"https://arxiv.org/pdf/2406.16783v2.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2402.03216v4","updated":"2024-06-28T09:55:49Z","published":"2024-02-05T17:26:49Z","title":"BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity\n  Text Embeddings Through Self-Knowledge Distillation","summary":"  In this paper, we present a new embedding model, called M3-Embedding, which\nis distinguished for its versatility in Multi-Linguality, Multi-Functionality,\nand Multi-Granularity. It can support more than 100 working languages, leading\nto new state-of-the-art performances on multi-lingual and cross-lingual\nretrieval tasks. It can simultaneously perform the three common retrieval\nfunctionalities of embedding model: dense retrieval, multi-vector retrieval,\nand sparse retrieval, which provides a unified model foundation for real-world\nIR applications. It is able to process inputs of different granularities,\nspanning from short sentences to long documents of up to 8192 tokens. The\neffective training of M3-Embedding involves the following technical\ncontributions. We propose a novel self-knowledge distillation approach, where\nthe relevance scores from different retrieval functionalities can be integrated\nas the teacher signal to enhance the training quality. We also optimize the\nbatching strategy, enabling a large batch size and high training throughput to\nensure the discriminativeness of embeddings. To the best of our knowledge,\nM3-Embedding is the first embedding model which realizes such a strong\nversatility. The model and code will be publicly available at\nhttps://github.com/FlagOpen/FlagEmbedding.\n","authors":["Jianlv Chen","Shitao Xiao","Peitian Zhang","Kun Luo","Defu Lian","Zheng Liu"],"pdf_url":"https://arxiv.org/pdf/2402.03216v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15613v2","updated":"2024-06-28T09:22:38Z","published":"2024-05-24T14:58:51Z","title":"Automatic Data Curation for Self-Supervised Learning: A Clustering-Based\n  Approach","summary":"  Self-supervised features are the cornerstone of modern machine learning\nsystems. They are typically pre-trained on data collections whose construction\nand curation typically require extensive human effort. This manual process has\nsome limitations similar to those encountered in supervised learning, e.g., the\ncrowd-sourced selection of data is costly and time-consuming, preventing\nscaling the dataset size. In this work, we consider the problem of automatic\ncuration of high-quality datasets for self-supervised pre-training. We posit\nthat such datasets should be large, diverse and balanced, and propose a\nclustering-based approach for building ones satisfying all these criteria. Our\nmethod involves successive and hierarchical applications of $k$-means on a\nlarge and diverse data repository to obtain clusters that distribute uniformly\namong data concepts, followed by a hierarchical, balanced sampling step from\nthese clusters. Extensive experiments on three different data domains including\nweb-based images, satellite images and text show that features trained on our\nautomatically curated datasets outperform those trained on uncurated data while\nbeing on par or better than ones trained on manually curated data. Code is\navailable at https://github.com/facebookresearch/ssl-data-curation.\n","authors":["Huy V. Vo","Vasil Khalidov","Timothée Darcet","Théo Moutakanni","Nikita Smetanin","Marc Szafraniec","Hugo Touvron","Camille Couprie","Maxime Oquab","Armand Joulin","Hervé Jégou","Patrick Labatut","Piotr Bojanowski"],"pdf_url":"https://arxiv.org/pdf/2405.15613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19770v1","updated":"2024-06-28T09:17:58Z","published":"2024-06-28T09:17:58Z","title":"Self-Supervised Spatial-Temporal Normality Learning for Time Series\n  Anomaly Detection","summary":"  Time Series Anomaly Detection (TSAD) finds widespread applications across\nvarious domains such as financial markets, industrial production, and\nhealthcare. Its primary objective is to learn the normal patterns of time\nseries data, thereby identifying deviations in test samples. Most existing TSAD\nmethods focus on modeling data from the temporal dimension, while ignoring the\nsemantic information in the spatial dimension. To address this issue, we\nintroduce a novel approach, called Spatial-Temporal Normality learning (STEN).\nSTEN is composed of a sequence Order prediction-based Temporal Normality\nlearning (OTN) module that captures the temporal correlations within sequences,\nand a Distance prediction-based Spatial Normality learning (DSN) module that\nlearns the relative spatial relations between sequences in a feature space. By\nsynthesizing these two modules, STEN learns expressive spatial-temporal\nrepresentations for the normal patterns hidden in the time series data.\nExtensive experiments on five popular TSAD benchmarks show that STEN\nsubstantially outperforms state-of-the-art competing methods. Our code is\navailable at https://github.com/mala-lab/STEN.\n","authors":["Yutong Chen","Hongzuo Xu","Guansong Pang","Hezhe Qiao","Yuan Zhou","Mingsheng Shang"],"pdf_url":"https://arxiv.org/pdf/2406.19770v1.pdf","comment":"18 pages, 4 figures, accepted in ECML PKDD2024"},{"id":"http://arxiv.org/abs/2304.01468v2","updated":"2024-06-28T09:17:31Z","published":"2023-04-04T02:13:46Z","title":"DLRover-RM: Resource Optimization for Deep Recommendation Models\n  Training in the Cloud","summary":"  Deep learning recommendation models (DLRM) rely on large embedding tables to\nmanage categorical sparse features. Expanding such embedding tables can\nsignificantly enhance model performance, but at the cost of increased\nGPU/CPU/memory usage. Meanwhile, tech companies have built extensive\ncloud-based services to accelerate training DLRM models at scale. In this\npaper, we conduct a deep investigation of the DLRM training platforms at\nAntGroup and reveal two critical challenges: low resource utilization due to\nsuboptimal configurations by users and the tendency to encounter abnormalities\ndue to an unstable cloud environment. To overcome them, we introduce\nDLRover-RM, an elastic training framework for DLRMs designed to increase\nresource utilization and handle the instability of a cloud environment.\nDLRover-RM develops a resource-performance model by considering the unique\ncharacteristics of DLRMs and a three-stage heuristic strategy to automatically\nallocate and dynamically adjust resources for DLRM training jobs for higher\nresource utilization. Further, DLRover-RM develops multiple mechanisms to\nensure efficient and reliable execution of DLRM training jobs. Our extensive\nevaluation shows that DLRover-RM reduces job completion times by 31%, increases\nthe job completion rate by 6%, enhances CPU usage by 15%, and improves memory\nutilization by 20%, compared to state-of-the-art resource scheduling\nframeworks. DLRover-RM has been widely deployed at AntGroup and processes\nthousands of DLRM training jobs on a daily basis. DLRover-RM is open-sourced\nand has been adopted by 10+ companies.\n","authors":["Qinlong Wang","Tingfeng Lan","Yinghao Tang","Ziling Huang","Yiheng Du","Haitao Zhang","Jian Sha","Hui Lu","Yuanchun Zhou","Ke Zhang","Mingjie Tang"],"pdf_url":"https://arxiv.org/pdf/2304.01468v2.pdf","comment":"Accepted in VLDB'24"},{"id":"http://arxiv.org/abs/2406.10552v3","updated":"2024-06-28T09:16:28Z","published":"2024-06-15T08:13:47Z","title":"Large Language Model Enhanced Clustering for News Event Detection","summary":"  The news landscape is continuously evolving, with an ever-increasing volume\nof information from around the world. Automated event detection within this\nvast data repository is essential for monitoring, identifying, and categorizing\nsignificant news occurrences across diverse platforms. This paper presents an\nevent detection framework that leverages Large Language Models (LLMs) combined\nwith clustering analysis to detect news events from the Global Database of\nEvents, Language, and Tone (GDELT). The framework enhances event clustering\nthrough both pre-event detection tasks (keyword extraction and text embedding)\nand post-event detection tasks (event summarization and topic labelling). We\nalso evaluate the impact of various textual embeddings on the quality of\nclustering outcomes, ensuring robust news categorization. Additionally, we\nintroduce a novel Cluster Stability Assessment Index (CSAI) to assess the\nvalidity and robustness of clustering results. CSAI utilizes multiple feature\nvectors to provide a new way of measuring clustering quality. Our experiments\nindicate that the use of LLM embedding in the event detection framework has\nsignificantly improved the results, demonstrating greater robustness in terms\nof CSAI scores. Moreover, post-event detection tasks generate meaningful\ninsights, facilitating effective interpretation of event clustering results.\nOverall, our experimental results indicate that the proposed framework offers\nvaluable insights and could enhance the accuracy in news analysis and\nreporting.\n","authors":["Adane Nega Tarekegn"],"pdf_url":"https://arxiv.org/pdf/2406.10552v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19763v1","updated":"2024-06-28T09:06:52Z","published":"2024-06-28T09:06:52Z","title":"xSemAD: Explainable Semantic Anomaly Detection in Event Logs Using\n  Sequence-to-Sequence Models","summary":"  The identification of undesirable behavior in event logs is an important\naspect of process mining that is often addressed by anomaly detection methods.\nTraditional anomaly detection methods tend to focus on statistically rare\nbehavior and neglect the subtle difference between rarity and undesirability.\nThe introduction of semantic anomaly detection has opened a promising avenue by\nidentifying semantically deviant behavior. This work addresses a gap in\nsemantic anomaly detection, which typically indicates the occurrence of an\nanomaly without explaining the nature of the anomaly. We propose xSemAD, an\napproach that uses a sequence-to-sequence model to go beyond pure\nidentification and provides extended explanations. In essence, our approach\nlearns constraints from a given process model repository and then checks\nwhether these constraints hold in the considered event log. This approach not\nonly helps understand the specifics of the undesired behavior, but also\nfacilitates targeted corrective actions. Our experiments demonstrate that our\napproach outperforms existing state-of-the-art semantic anomaly detection\nmethods.\n","authors":["Kiran Busch","Timotheus Kampik","Henrik Leopold"],"pdf_url":"https://arxiv.org/pdf/2406.19763v1.pdf","comment":"Accepted at BPM 2024"},{"id":"http://arxiv.org/abs/2406.15486v2","updated":"2024-06-28T08:55:17Z","published":"2024-06-17T11:05:15Z","title":"SampleAttention: Near-Lossless Acceleration of Long Context LLM\n  Inference with Adaptive Structured Sparse Attention","summary":"  Large language models (LLMs) now support extremely long context windows, but\nthe quadratic complexity of vanilla attention results in significantly long\nTime-to-First-Token (TTFT) latency. Existing approaches to address this\ncomplexity require additional pretraining or finetuning, and often sacrifice\nmodel accuracy. In this paper, we first provide both theoretical and empirical\nfoundations for near-lossless sparse attention. We find dynamically capturing\nhead-specific sparse patterns at runtime with low overhead is crucial. To\naddress this, we propose SampleAttention, an adaptive structured and\nnear-lossless sparse attention. Leveraging observed significant sparse\npatterns, SampleAttention attends to a fixed percentage of adjacent tokens to\ncapture local window patterns, and employs a two-stage query-guided key-value\nfiltering approach, which adaptively select a minimum set of key-values with\nlow overhead, to capture column stripe patterns. Comprehensive evaluations show\nthat SampleAttention can seamlessly replace vanilla attention in off-the-shelf\nLLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$\ncompared with FlashAttention.\n","authors":["Qianchao Zhu","Jiangfei Duan","Chang Chen","Siran Liu","Xiuhong Li","Guanyu Feng","Xin Lv","Huanqi Cao","Xiao Chuanfu","Xingcheng Zhang","Dahua Lin","Chao Yang"],"pdf_url":"https://arxiv.org/pdf/2406.15486v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19756v1","updated":"2024-06-28T08:54:44Z","published":"2024-06-28T08:54:44Z","title":"Structure-aware World Model for Probe Guidance via Large-scale\n  Self-supervised Pre-train","summary":"  The complex structure of the heart leads to significant challenges in\nechocardiography, especially in acquisition cardiac ultrasound images.\nSuccessful echocardiography requires a thorough understanding of the structures\non the two-dimensional plane and the spatial relationships between planes in\nthree-dimensional space. In this paper, we innovatively propose a large-scale\nself-supervised pre-training method to acquire a cardiac structure-aware world\nmodel. The core innovation lies in constructing a self-supervised task that\nrequires structural inference by predicting masked structures on a 2D plane and\nimagining another plane based on pose transformation in 3D space. To support\nlarge-scale pre-training, we collected over 1.36 million echocardiograms from\nten standard views, along with their 3D spatial poses. In the downstream probe\nguidance task, we demonstrate that our pre-trained model consistently reduces\nguidance errors across the ten most common standard views on the test set with\n0.29 million samples from 74 routine clinical scans, indicating that\nstructure-aware pre-training benefits the scanning.\n","authors":["Haojun Jiang","Meng Li","Zhenguo Sun","Ning Jia","Yu Sun","Shaqi Luo","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2406.19756v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2406.19755v1","updated":"2024-06-28T08:54:37Z","published":"2024-06-28T08:54:37Z","title":"Protein Representation Learning with Sequence Information Embedding:\n  Does it Always Lead to a Better Performance?","summary":"  Deep learning has become a crucial tool in studying proteins. While the\nsignificance of modeling protein structure has been discussed extensively in\nthe literature, amino acid types are typically included in the input as a\ndefault operation for many inference tasks. This study demonstrates with\nstructure alignment task that embedding amino acid types in some cases may not\nhelp a deep learning model learn better representation. To this end, we propose\nProtLOCA, a local geometry alignment method based solely on amino acid\nstructure representation. The effectiveness of ProtLOCA is examined by a global\nstructure-matching task on protein pairs with an independent test dataset based\non CATH labels. Our method outperforms existing sequence- and structure-based\nrepresentation learning methods by more quickly and accurately matching\nstructurally consistent protein domains. Furthermore, in local structure\npairing tasks, ProtLOCA for the first time provides a valid solution to\nhighlight common local structures among proteins with different overall\nstructures but the same function. This suggests a new possibility for using\ndeep learning methods to analyze protein structure to infer function.\n","authors":["Yang Tan","Lirong Zheng","Bozitao Zhong","Liang Hong","Bingxin Zhou"],"pdf_url":"https://arxiv.org/pdf/2406.19755v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.00532v2","updated":"2024-06-28T08:48:06Z","published":"2024-05-01T14:05:52Z","title":"ULLER: A Unified Language for Learning and Reasoning","summary":"  The field of neuro-symbolic artificial intelligence (NeSy), which combines\nlearning and reasoning, has recently experienced significant growth. There now\nare a wide variety of NeSy frameworks, each with its own specific language for\nexpressing background knowledge and how to relate it to neural networks. This\nheterogeneity hinders accessibility for newcomers and makes comparing different\nNeSy frameworks challenging. We propose a language for NeSy, which we call\nULLER, a Unfied Language for LEarning and Reasoning. ULLER encompasses a wide\nvariety of settings, while ensuring that knowledge described in it can be used\nin existing NeSy systems. ULLER has a first-order logic syntax specialised for\nNeSy for which we provide example semantics including classical FOL, fuzzy\nlogic, and probabilistic logic. We believe ULLER is a first step towards making\nNeSy research more accessible and comparable, paving the way for libraries that\nstreamline training and evaluation across a multitude of semantics, knowledge\nbases, and NeSy systems.\n","authors":["Emile van Krieken","Samy Badreddine","Robin Manhaeve","Eleonora Giunchiglia"],"pdf_url":"https://arxiv.org/pdf/2405.00532v2.pdf","comment":"Accepted at NeSy 2024"},{"id":"http://arxiv.org/abs/2402.13914v2","updated":"2024-06-28T08:37:28Z","published":"2024-02-21T16:30:24Z","title":"Position: Explain to Question not to Justify","summary":"  Explainable Artificial Intelligence (XAI) is a young but very promising field\nof research. Unfortunately, the progress in this field is currently slowed down\nby divergent and incompatible goals. We separate various threads tangled within\nthe area of XAI into two complementary cultures of human/value-oriented\nexplanations (BLUE XAI) and model/validation-oriented explanations (RED XAI).\nThis position paper argues that the area of RED XAI is currently\nunder-explored, i.e., more methods for explainability are desperately needed to\nquestion models (e.g., extract knowledge from well-performing models as well as\nspotting and fixing bugs in faulty models), and the area of RED XAI hides great\nopportunities and potential for important research necessary to ensure the\nsafety of AI systems. We conclude this paper by presenting promising challenges\nin this area.\n","authors":["Przemyslaw Biecek","Wojciech Samek"],"pdf_url":"https://arxiv.org/pdf/2402.13914v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19741v1","updated":"2024-06-28T08:28:38Z","published":"2024-06-28T08:28:38Z","title":"ROS-LLM: A ROS framework for embodied AI with task feedback and\n  structured reasoning","summary":"  We present a framework for intuitive robot programming by non-experts,\nleveraging natural language prompts and contextual information from the Robot\nOperating System (ROS). Our system integrates large language models (LLMs),\nenabling non-experts to articulate task requirements to the system through a\nchat interface. Key features of the framework include: integration of ROS with\nan AI agent connected to a plethora of open-source and commercial LLMs,\nautomatic extraction of a behavior from the LLM output and execution of ROS\nactions/services, support for three behavior modes (sequence, behavior tree,\nstate machine), imitation learning for adding new robot actions to the library\nof possible actions, and LLM reflection via human and environment feedback.\nExtensive experiments validate the framework, showcasing robustness,\nscalability, and versatility in diverse scenarios, including long-horizon\ntasks, tabletop rearrangements, and remote supervisory control. To facilitate\nthe adoption of our framework and support the reproduction of our results, we\nhave made our code open-source. You can access it at:\nhttps://github.com/huawei-noah/HEBO/tree/master/ROSLLM.\n","authors":["Christopher E. Mower","Yuhui Wan","Hongzhan Yu","Antoine Grosnit","Jonas Gonzalez-Billandon","Matthieu Zimmer","Jinlong Wang","Xinyu Zhang","Yao Zhao","Anbang Zhai","Puze Liu","Davide Tateo","Cesar Cadena","Marco Hutter","Jan Peters","Guangjian Tian","Yuzheng Zhuang","Kun Shao","Xingyue Quan","Jianye Hao","Jun Wang","Haitham Bou-Ammar"],"pdf_url":"https://arxiv.org/pdf/2406.19741v1.pdf","comment":"This document contains 26 pages and 13 figures"},{"id":"http://arxiv.org/abs/2406.19738v1","updated":"2024-06-28T08:26:47Z","published":"2024-06-28T08:26:47Z","title":"Classical Bandit Algorithms for Entanglement Detection in Parameterized\n  Qubit States","summary":"  Entanglement is a key resource for a wide range of tasks in quantum\ninformation and computing. Thus, verifying availability of this quantum\nresource is essential. Extensive research on entanglement detection has led to\nno-go theorems (Lu et al. [Phys. Rev. Lett., 116, 230501 (2016)]) that\nhighlight the need for full state tomography (FST) in the absence of adaptive\nor joint measurements. Recent advancements, as proposed by Zhu, Teo, and\nEnglert [Phys. Rev. A, 81, 052339, 2010], introduce a single-parameter family\nof entanglement witness measurements which are capable of conclusively\ndetecting certain entangled states and only resort to FST when all witness\nmeasurements are inconclusive. We find a variety of realistic noisy two-qubit\nquantum states $\\mathcal{F}$ that yield conclusive results under this witness\nfamily. We solve the problem of detecting entanglement among $K$ quantum states\nin $\\mathcal{F}$, of which $m$ states are entangled, with $m$ potentially\nunknown. We recognize a structural connection of this problem to the Bad Arm\nIdentification problem in stochastic Multi-Armed Bandits (MAB). In contrast to\nexisting quantum bandit frameworks, we establish a new correspondence tailored\nfor entanglement detection and term it the $(m,K)$-quantum Multi-Armed Bandit.\nWe implement two well-known MAB policies for arbitrary states derived from\n$\\mathcal{F}$, present theoretical guarantees on the measurement/sample\ncomplexity and demonstrate the practicality of the policies through numerical\nsimulations. More broadly, this paper highlights the potential for employing\nclassical machine learning techniques for quantum entanglement detection.\n","authors":["Bharati. K","Vikesh Siddhu","Krishna Jagannathan"],"pdf_url":"https://arxiv.org/pdf/2406.19738v1.pdf","comment":"20 pages, 5 figures"},{"id":"http://arxiv.org/abs/2406.19736v1","updated":"2024-06-28T08:25:27Z","published":"2024-06-28T08:25:27Z","title":"MM-Instruct: Generated Visual Instructions for Large Multimodal Model\n  Alignment","summary":"  This paper introduces MM-Instruct, a large-scale dataset of diverse and\nhigh-quality visual instruction data designed to enhance the\ninstruction-following capabilities of large multimodal models (LMMs). While\nexisting visual instruction datasets often focus on question-answering, they\nstruggle to generalize to broader application scenarios such as creative\nwriting, summarization, or image analysis. To address these limitations, we\npropose a novel approach to constructing MM-Instruct that leverages the strong\ninstruction-following capabilities of existing LLMs to generate novel visual\ninstruction data from large-scale but conventional image captioning datasets.\nMM-Instruct first leverages ChatGPT to automatically generate diverse\ninstructions from a small set of seed instructions through augmenting and\nsummarization. It then matches these instructions with images and uses an\nopen-sourced large language model (LLM) to generate coherent answers to the\ninstruction-image pairs. The LLM is grounded by the detailed text descriptions\nof images in the whole answer generation process to guarantee the alignment of\nthe instruction data. Moreover, we introduce a benchmark based on the generated\ninstruction data to evaluate the instruction-following capabilities of existing\nLMMs. We demonstrate the effectiveness of MM-Instruct by training a LLaVA-1.5\nmodel on the generated data, denoted as LLaVA-Instruct, which exhibits\nsignificant improvements in instruction-following capabilities compared to\nLLaVA-1.5 models. The MM-Instruct dataset, benchmark, and pre-trained models\nare available at https://github.com/jihaonew/MM-Instruct.\n","authors":["Jihao Liu","Xin Huang","Jinliang Zheng","Boxiao Liu","Jia Wang","Osamu Yoshie","Yu Liu","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2406.19736v1.pdf","comment":"Dataset and models are available at\n  https://github.com/jihaonew/MM-Instruct"},{"id":"http://arxiv.org/abs/2307.10635v3","updated":"2024-06-28T08:24:13Z","published":"2023-07-20T07:01:57Z","title":"SciBench: Evaluating College-Level Scientific Problem-Solving Abilities\n  of Large Language Models","summary":"  Most of the existing Large Language Model (LLM) benchmarks on scientific\nproblem reasoning focus on problems grounded in high-school subjects and are\nconfined to elementary algebraic operations. To systematically examine the\nreasoning capabilities required for solving complex scientific problems, we\nintroduce an expansive benchmark suite SciBench for LLMs. SciBench contains a\ncarefully curated dataset featuring a range of collegiate-level scientific\nproblems from mathematics, chemistry, and physics domains. Based on the\ndataset, we conduct an in-depth benchmarking study of representative\nopen-source and proprietary LLMs with various prompting strategies. The results\nreveal that the current LLMs fall short of delivering satisfactory performance,\nwith the best overall score of merely 43.22%. Furthermore, through a detailed\nuser study, we categorize the errors made by LLMs into ten problem-solving\nabilities. Our analysis indicates that no single prompting strategy\nsignificantly outperforms the others and some strategies that demonstrate\nimprovements in certain problem-solving skills could result in declines in\nother skills. We envision that SciBench will catalyze further developments in\nthe reasoning abilities of LLMs, thereby ultimately contributing to scientific\nresearch and discovery.\n","authors":["Xiaoxuan Wang","Ziniu Hu","Pan Lu","Yanqiao Zhu","Jieyu Zhang","Satyen Subramaniam","Arjun R. Loomba","Shichang Zhang","Yizhou Sun","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2307.10635v3.pdf","comment":"To appear at ICML 2024"},{"id":"http://arxiv.org/abs/2402.08114v2","updated":"2024-06-28T08:22:01Z","published":"2024-02-12T23:09:00Z","title":"Active Preference Learning for Large Language Models","summary":"  As large language models (LLMs) become more capable, fine-tuning techniques\nfor aligning with human intent are increasingly important. A key consideration\nfor aligning these models is how to most effectively use human resources, or\nmodel resources in the case where LLMs themselves are used as oracles.\nReinforcement learning from Human or AI preferences (RLHF/RLAIF) is the most\nprominent example of such a technique, but is complex and often unstable.\nDirect Preference Optimization (DPO) has recently been proposed as a simpler\nand more stable alternative. In this work, we develop an active learning\nstrategy for DPO to make better use of preference labels. We propose a\npractical acquisition function for prompt/completion pairs based on the\npredictive entropy of the language model and a measure of certainty of the\nimplicit preference model optimized by DPO. We demonstrate how our approach\nimproves both the rate of learning and final performance of fine-tuning on\npairwise preference data.\n","authors":["William Muldrew","Peter Hayes","Mingtian Zhang","David Barber"],"pdf_url":"https://arxiv.org/pdf/2402.08114v2.pdf","comment":"13 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.19720v1","updated":"2024-06-28T08:09:55Z","published":"2024-06-28T08:09:55Z","title":"CUPID: Improving Battle Fairness and Position Satisfaction in Online\n  MOBA Games with a Re-matchmaking System","summary":"  The multiplayer online battle arena (MOBA) genre has gained significant\npopularity and economic success, attracting considerable research interest\nwithin the Human-Computer Interaction community. Enhancing the gaming\nexperience requires a deep understanding of player behavior, and a crucial\naspect of MOBA games is matchmaking, which aims to assemble teams of comparable\nskill levels. However, existing matchmaking systems often neglect important\nfactors such as players' position preferences and team assignment, resulting in\nimbalanced matches and reduced player satisfaction. To address these\nlimitations, this paper proposes a novel framework called CUPID, which\nintroduces a novel process called ``re-matchmaking'' to optimize team and\nposition assignments to improve both fairness and player satisfaction. CUPID\nincorporates a pre-filtering step to ensure a minimum level of matchmaking\nquality, followed by a pre-match win-rate prediction model that evaluates the\nfairness of potential assignments. By simultaneously considering players'\nposition satisfaction and game fairness, CUPID aims to provide an enhanced\nmatchmaking experience. Extensive experiments were conducted on two\nlarge-scale, real-world MOBA datasets to validate the effectiveness of CUPID.\nThe results surpass all existing state-of-the-art baselines, with an average\nrelative improvement of 7.18% in terms of win prediction accuracy. Furthermore,\nCUPID has been successfully deployed in a popular online mobile MOBA game. The\ndeployment resulted in significant improvements in match fairness and player\nsatisfaction, as evidenced by critical Human-Computer Interaction (HCI) metrics\ncovering usability, accessibility, and engagement, observed through A/B\ntesting. To the best of our knowledge, CUPID is the first re-matchmaking system\ndesigned specifically for large-scale MOBA games.\n","authors":["Ge Fan","Chaoyun Zhang","Kai Wang","Yingjie Li","Junyang Chen","Zenglin Xu"],"pdf_url":"https://arxiv.org/pdf/2406.19720v1.pdf","comment":"38 pages, accepted by CSCW 24"},{"id":"http://arxiv.org/abs/2403.09703v2","updated":"2024-06-28T08:03:19Z","published":"2024-03-08T19:07:47Z","title":"Concept-aware Data Construction Improves In-context Learning of Language\n  Models","summary":"  Many recent language models (LMs) are capable of in-context learning (ICL),\nmanifested in the LMs' ability to perform a new task solely from\nnatural-language instruction. Previous work curating in-context learners\nassumes that ICL emerges from a vast over-parametrization or the scale of\nmulti-task training. However, recent theoretical work attributes the ICL\nability to concept-dependent training data and creates functional in-context\nlearners even in small-scale, synthetic settings.\n  In this work, we practically explore this newly identified axis of ICL\nquality. We propose Concept-aware Training (CoAT), a framework for constructing\ntraining scenarios that make it beneficial for the LM to learn to utilize the\nanalogical reasoning concepts from demonstrations. We find that by using CoAT,\npre-trained transformers can learn to better utilise new latent concepts from\ndemonstrations and that such ability makes ICL more robust to the functional\ndeficiencies of the previous models. Finally, we show that concept-aware\nin-context learning is more effective for a majority of new tasks when compared\nto traditional instruction tuning, resulting in a performance comparable to the\nprevious in-context learners using magnitudes of more training data.\n","authors":["Michal Štefánik","Marek Kadlčík","Petr Sojka"],"pdf_url":"https://arxiv.org/pdf/2403.09703v2.pdf","comment":"Long paper to appear in Findings of ACL 2024"},{"id":"http://arxiv.org/abs/2406.19712v1","updated":"2024-06-28T07:47:34Z","published":"2024-06-28T07:47:34Z","title":"Uncertainty Quantification in Large Language Models Through Convex Hull\n  Analysis","summary":"  Uncertainty quantification approaches have been more critical in large\nlanguage models (LLMs), particularly high-risk applications requiring reliable\noutputs. However, traditional methods for uncertainty quantification, such as\nprobabilistic models and ensemble techniques, face challenges when applied to\nthe complex and high-dimensional nature of LLM-generated outputs. This study\nproposes a novel geometric approach to uncertainty quantification using convex\nhull analysis. The proposed method leverages the spatial properties of response\nembeddings to measure the dispersion and variability of model outputs. The\nprompts are categorized into three types, i.e., `easy', `moderate', and\n`confusing', to generate multiple responses using different LLMs at varying\ntemperature settings. The responses are transformed into high-dimensional\nembeddings via a BERT model and subsequently projected into a two-dimensional\nspace using Principal Component Analysis (PCA). The Density-Based Spatial\nClustering of Applications with Noise (DBSCAN) algorithm is utilized to cluster\nthe embeddings and compute the convex hull for each selected cluster. The\nexperimental results indicate that the uncertainty of the model for LLMs\ndepends on the prompt complexity, the model, and the temperature setting.\n","authors":["Ferhat Ozgur Catak","Murat Kuzlu"],"pdf_url":"https://arxiv.org/pdf/2406.19712v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2406.19708v1","updated":"2024-06-28T07:41:31Z","published":"2024-06-28T07:41:31Z","title":"A Differentiable Approach to Multi-scale Brain Modeling","summary":"  We present a multi-scale differentiable brain modeling workflow utilizing\nBrainPy, a unique differentiable brain simulator that combines accurate brain\nsimulation with powerful gradient-based optimization. We leverage this\ncapability of BrainPy across different brain scales. At the single-neuron\nlevel, we implement differentiable neuron models and employ gradient methods to\noptimize their fit to electrophysiological data. On the network level, we\nincorporate connectomic data to construct biologically constrained network\nmodels. Finally, to replicate animal behavior, we train these models on\ncognitive tasks using gradient-based learning rules. Experiments demonstrate\nthat our approach achieves superior performance and speed in fitting\ngeneralized leaky integrate-and-fire and Hodgkin-Huxley single neuron models.\nAdditionally, training a biologically-informed network of excitatory and\ninhibitory spiking neurons on working memory tasks successfully replicates\nobserved neural activity and synaptic weight distributions. Overall, our\ndifferentiable multi-scale simulation approach offers a promising tool to\nbridge neuroscience data across electrophysiological, anatomical, and\nbehavioral scales.\n","authors":["Chaoming Wang","Muyang Lyu","Tianqiu Zhang","Sichao He","Si Wu"],"pdf_url":"https://arxiv.org/pdf/2406.19708v1.pdf","comment":"2nd Differentiable Almost Everything Workshop at ICML 2024"},{"id":"http://arxiv.org/abs/2406.19705v1","updated":"2024-06-28T07:36:31Z","published":"2024-06-28T07:36:31Z","title":"DISCO: Efficient Diffusion Solver for Large-Scale Combinatorial\n  Optimization Problems","summary":"  Combinatorial Optimization (CO) problems are fundamentally crucial in\nnumerous practical applications across diverse industries, characterized by\nentailing enormous solution space and demanding time-sensitive response.\nDespite significant advancements made by recent neural solvers, their limited\nexpressiveness does not conform well to the multi-modal nature of CO\nlandscapes. While some research has pivoted towards diffusion models, they\nrequire simulating a Markov chain with many steps to produce a sample, which is\ntime-consuming and does not meet the efficiency requirement of real\napplications, especially at scale. We propose DISCO, an efficient DIffusion\nSolver for Combinatorial Optimization problems that excels in both solution\nquality and inference speed. DISCO's efficacy is two-pronged: Firstly, it\nachieves rapid denoising of solutions through an analytically solvable form,\nallowing for direct sampling from the solution space with very few reverse-time\nsteps, thereby drastically reducing inference time. Secondly, DISCO enhances\nsolution quality by restricting the sampling space to a more constrained,\nmeaningful domain guided by solution residues, while still preserving the\ninherent multi-modality of the output probabilistic distributions. DISCO\nachieves state-of-the-art results on very large Traveling Salesman Problems\nwith 10000 nodes and challenging Maximal Independent Set benchmarks, with its\nper-instance denoising time up to 44.8 times faster. Through further combining\na divide-and-conquer strategy, DISCO can be generalized to solve\narbitrary-scale problem instances off the shelf, even outperforming models\ntrained specifically on corresponding scales.\n","authors":["Kexiong Yu","Hang Zhao","Yuhang Huang","Renjiao Yi","Kai Xu","Chenyang Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.19705v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18584v2","updated":"2024-06-28T07:34:25Z","published":"2024-06-06T06:22:06Z","title":"Assessment of Sentinel-2 spatial and temporal coverage based on the\n  scene classification layer","summary":"  Since the launch of the Sentinel-2 (S2) satellites, many ML models have used\nthe data for diverse applications. The scene classification layer (SCL) inside\nthe S2 product provides rich information for training, such as filtering images\nwith high cloud coverage. However, there is more potential in this. We propose\na technique to assess the clean optical coverage of a region, expressed by a\nSITS and calculated with the S2-based SCL data. With a manual threshold and\nspecific labels in the SCL, the proposed technique assigns a percentage of\nspatial and temporal coverage across the time series and a high/low assessment.\nBy evaluating the AI4EO challenge for Enhanced Agriculture, we show that the\nassessment is correlated to the predictive results of ML models. The\nclassification results in a region with low spatial and temporal coverage is\nworse than in a region with high coverage. Finally, we applied the technique\nacross all continents of the global dataset LandCoverNet.\n","authors":["Cristhian Sanchez","Francisco Mena","Marcela Charfuelan","Marlon Nuske","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2406.18584v2.pdf","comment":"Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium 2024"},{"id":"http://arxiv.org/abs/2402.11622v2","updated":"2024-06-28T07:20:22Z","published":"2024-02-18T15:28:39Z","title":"Logical Closed Loop: Uncovering Object Hallucinations in Large\n  Vision-Language Models","summary":"  Object hallucination has been an Achilles' heel which hinders the broader\napplications of large vision-language models (LVLMs). Object hallucination\nrefers to the phenomenon that the LVLMs claim non-existent objects in the\nimage. To mitigate the object hallucinations, instruction tuning and external\nmodel-based detection methods have been proposed, which either require\nlarge-scare computational resources or depend on the detection result of\nexternal models. However, there remains an under-explored field to utilize the\nLVLM itself to alleviate object hallucinations. In this work, we adopt the\nintuition that the LVLM tends to respond logically consistently for existent\nobjects but inconsistently for hallucinated objects. Therefore, we propose a\nLogical Closed Loop-based framework for Object Hallucination Detection and\nMitigation, namely LogicCheckGPT. In specific, we devise logical consistency\nprobing to raise questions with logical correlations, inquiring about\nattributes from objects and vice versa. Whether their responses can form a\nlogical closed loop serves as an indicator of object hallucination. As a\nplug-and-play method, it can be seamlessly applied to all existing LVLMs.\nComprehensive experiments conducted on three benchmarks across four LVLMs have\ndemonstrated significant improvements brought by our method, indicating its\neffectiveness and generality.\n","authors":["Junfei Wu","Qiang Liu","Ding Wang","Jinghao Zhang","Shu Wu","Liang Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2402.11622v2.pdf","comment":"Accept to ACL 2024; 19 Pages, 15 Figures, 6 Tables"},{"id":"http://arxiv.org/abs/2406.19690v1","updated":"2024-06-28T07:06:02Z","published":"2024-06-28T07:06:02Z","title":"Deep Fusion Model for Brain Tumor Classification Using Fine-Grained\n  Gradient Preservation","summary":"  Brain tumors are one of the most common diseases that lead to early death if\nnot diagnosed at an early stage. Traditional diagnostic approaches are\nextremely time-consuming and prone to errors. In this context, computer\nvision-based approaches have emerged as an effective tool for accurate brain\ntumor classification. While some of the existing solutions demonstrate\nnoteworthy accuracy, the models become infeasible to deploy in areas where\ncomputational resources are limited. This research addresses the need for\naccurate and fast classification of brain tumors with a priority of deploying\nthe model in technologically underdeveloped regions. The research presents a\nnovel architecture for precise brain tumor classification fusing pretrained\nResNet152V2 and modified VGG16 models. The proposed architecture undergoes a\ndiligent fine-tuning process that ensures fine gradients are preserved in deep\nneural networks, which are essential for effective brain tumor classification.\nThe proposed solution incorporates various image processing techniques to\nimprove image quality and achieves an astounding accuracy of 98.36% and 98.04%\nin Figshare and Kaggle datasets respectively. This architecture stands out for\nhaving a streamlined profile, with only 2.8 million trainable parameters. We\nhave leveraged 8-bit quantization to produce a model of size 73.881 MB,\nsignificantly reducing it from the previous size of 289.45 MB, ensuring smooth\ndeployment in edge devices even in resource-constrained areas. Additionally,\nthe use of Grad-CAM improves the interpretability of the model, offering\ninsightful information regarding its decision-making process. Owing to its high\ndiscriminative ability, this model can be a reliable option for accurate brain\ntumor classification.\n","authors":["Niful Islam","Mohaiminul Islam Bhuiyan","Jarin Tasnim Raya","Nur Shazwani Kamarudin","Khan Md Hasib","M. F. Mridha","Dewan Md. Farid"],"pdf_url":"https://arxiv.org/pdf/2406.19690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19686v1","updated":"2024-06-28T06:51:38Z","published":"2024-06-28T06:51:38Z","title":"Enhancing Radiological Diagnosis: A Collaborative Approach Integrating\n  AI and Human Expertise for Visual Miss Correction","summary":"  Human-AI collaboration to identify and correct perceptual errors in chest\nradiographs has not been previously explored. This study aimed to develop a\ncollaborative AI system, CoRaX, which integrates eye gaze data and radiology\nreports to enhance diagnostic accuracy in chest radiology by pinpointing\nperceptual errors and refining the decision-making process. Using public\ndatasets REFLACX and EGD-CXR, the study retrospectively developed CoRaX,\nemploying a large multimodal model to analyze image embeddings, eye gaze data,\nand radiology reports. The system's effectiveness was evaluated based on its\nreferral-making process, the quality of referrals, and performance in\ncollaborative diagnostic settings. CoRaX was tested on a simulated error\ndataset of 271 samples with 28% (93 of 332) missed abnormalities. The system\ncorrected 21% (71 of 332) of these errors, leaving 7% (22 of 312) unresolved.\nThe Referral-Usefulness score, indicating the accuracy of predicted regions for\nall true referrals, was 0.63 (95% CI 0.59, 0.68). The Total-Usefulness score,\nreflecting the diagnostic accuracy of CoRaX's interactions with radiologists,\nshowed that 84% (237 of 280) of these interactions had a score above 0.40. In\nconclusion, CoRaX efficiently collaborates with radiologists to address\nperceptual errors across various abnormalities, with potential applications in\nthe education and training of novice radiologists.\n","authors":["Akash Awasthi","Ngan Le","Zhigang Deng","Carol C. Wu","Hien Van Nguyen"],"pdf_url":"https://arxiv.org/pdf/2406.19686v1.pdf","comment":"Under Review in Journal"},{"id":"http://arxiv.org/abs/2402.03848v6","updated":"2024-06-28T06:49:39Z","published":"2024-02-06T09:50:08Z","title":"ANLS* -- A Universal Document Processing Metric for Generative Large\n  Language Models","summary":"  Traditionally, discriminative models have been the predominant choice for\ntasks like document classification and information extraction. These models\nmake predictions that fall into a limited number of predefined classes,\nfacilitating a binary true or false evaluation and enabling the direct\ncalculation of metrics such as the F1 score. However, recent advancements in\ngenerative large language models (GLLMs) have prompted a shift in the field due\nto their enhanced zero-shot capabilities, which eliminate the need for a\ndownstream dataset and computationally expensive fine-tuning. However,\nevaluating GLLMs presents a challenge as the binary true or false evaluation\nused for discriminative models is not applicable to the predictions made by\nGLLMs.\n  This paper introduces a new metric for generative models called ANLS* for\nevaluating a wide variety of tasks, including information extraction and\nclassification tasks. The ANLS* metric extends existing ANLS metrics as a\ndrop-in-replacement and is still compatible with previously reported ANLS\nscores. An evaluation of 7 different datasets, and more than 10 different GLLMs\ntogether with 3 different prompting methods using the ANLS* metric is also\nprovided, demonstrating the importance of the proposed metric.\n  We also benchmark a novel approach to generate prompts for documents, called\nSFT, against other prompting techniques such as LATIN. In 6 out of 7 cases, SFT\noutperforms other techniques and improves the state-of-the-art, sometimes by as\nmuch as $10$ percentage points.\n  Sources are available at https://github.com/deepopinion/anls_star_metric\n","authors":["David Peer","Philemon Schöpf","Volckmar Nebendahl","Alexander Rietzler","Sebastian Stabinger"],"pdf_url":"https://arxiv.org/pdf/2402.03848v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19680v1","updated":"2024-06-28T06:40:53Z","published":"2024-06-28T06:40:53Z","title":"MimicMotion: High-Quality Human Motion Video Generation with\n  Confidence-aware Pose Guidance","summary":"  In recent years, generative artificial intelligence has achieved significant\nadvancements in the field of image generation, spawning a variety of\napplications. However, video generation still faces considerable challenges in\nvarious aspects, such as controllability, video length, and richness of\ndetails, which hinder the application and popularization of this technology. In\nthis work, we propose a controllable video generation framework, dubbed\nMimicMotion, which can generate high-quality videos of arbitrary length\nmimicking specific motion guidance. Compared with previous methods, our\napproach has several highlights. Firstly, we introduce confidence-aware pose\nguidance that ensures high frame quality and temporal smoothness. Secondly, we\nintroduce regional loss amplification based on pose confidence, which\nsignificantly reduces image distortion. Lastly, for generating long and smooth\nvideos, we propose a progressive latent fusion strategy. By this means, we can\nproduce videos of arbitrary length with acceptable resource consumption. With\nextensive experiments and user studies, MimicMotion demonstrates significant\nimprovements over previous approaches in various aspects. Detailed results and\ncomparisons are available on our project page:\nhttps://tencent.github.io/MimicMotion .\n","authors":["Yuang Zhang","Jiaxi Gu","Li-Wen Wang","Han Wang","Junqi Cheng","Yuefeng Zhu","Fangyuan Zou"],"pdf_url":"https://arxiv.org/pdf/2406.19680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03640v3","updated":"2024-06-28T06:16:24Z","published":"2024-03-06T11:56:02Z","title":"Apollo: A Lightweight Multilingual Medical LLM towards Democratizing\n  Medical AI to 6B People","summary":"  Despite the vast repository of global medical knowledge predominantly being\nin English, local languages are crucial for delivering tailored healthcare\nservices, particularly in areas with limited medical resources. To extend the\nreach of medical AI advancements to a broader population, we aim to develop\nmedical LLMs across the six most widely spoken languages, encompassing a global\npopulation of 6.1 billion. This effort culminates in the creation of the\nApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the\nmultilingual medical benchmark, the released Apollo models, at various\nrelatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best\nperformance among models of equivalent size. Especially, Apollo-7B is the\nstate-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite\nmodels could be used to improve the multi-lingual medical capabilities of\nlarger models without fine-tuning in a proxy-tuning fashion. We will\nopen-source training corpora, code, model weights and evaluation benchmark.\n","authors":["Xidong Wang","Nuo Chen","Junyin Chen","Yan Hu","Yidong Wang","Xiangbo Wu","Anningzhe Gao","Xiang Wan","Haizhou Li","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2403.03640v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.19670v1","updated":"2024-06-28T05:44:47Z","published":"2024-06-28T05:44:47Z","title":"Function+Data Flow: A Framework to Specify Machine Learning Pipelines\n  for Digital Twinning","summary":"  The development of digital twins (DTs) for physical systems increasingly\nleverages artificial intelligence (AI), particularly for combining data from\ndifferent sources or for creating computationally efficient, reduced-dimension\nmodels. Indeed, even in very different application domains, twinning employs\ncommon techniques such as model order reduction and modelization with hybrid\ndata (that is, data sourced from both physics-based models and sensors).\nDespite this apparent generality, current development practices are ad-hoc,\nmaking the design of AI pipelines for digital twinning complex and\ntime-consuming. Here we propose Function+Data Flow (FDF), a domain-specific\nlanguage (DSL) to describe AI pipelines within DTs. FDF aims to facilitate the\ndesign and validation of digital twins. Specifically, FDF treats functions as\nfirst-class citizens, enabling effective manipulation of models learned with\nAI. We illustrate the benefits of FDF on two concrete use cases from different\ndomains: predicting the plastic strain of a structure and modeling the\nelectromagnetic behavior of a bearing.\n","authors":["Eduardo de Conto","Blaise Genest","Arvind Easwaran"],"pdf_url":"https://arxiv.org/pdf/2406.19670v1.pdf","comment":"10 pages, 5 figures, to be published in AIware'24"},{"id":"http://arxiv.org/abs/2406.11741v3","updated":"2024-06-28T05:28:27Z","published":"2024-06-17T17:00:52Z","title":"Transcendence: Generative Models Can Outperform The Experts That Train\n  Them","summary":"  Generative models are trained with the simple objective of imitating the\nconditional probability distribution induced by the data they are trained on.\nTherefore, when trained on data generated by humans, we may not expect the\nartificial model to outperform the humans on their original objectives. In this\nwork, we study the phenomenon of transcendence: when a generative model\nachieves capabilities that surpass the abilities of the experts generating its\ndata. We demonstrate transcendence by training an autoregressive transformer to\nplay chess from game transcripts, and show that the trained model can sometimes\nachieve better performance than all players in the dataset. We theoretically\nprove that transcendence can be enabled by low-temperature sampling, and\nrigorously assess this claim experimentally. Finally, we discuss other sources\nof transcendence, laying the groundwork for future investigation of this\nphenomenon in a broader setting.\n","authors":["Edwin Zhang","Vincent Zhu","Naomi Saphra","Anat Kleiman","Benjamin L. Edelman","Milind Tambe","Sham M. Kakade","Eran Malach"],"pdf_url":"https://arxiv.org/pdf/2406.11741v3.pdf","comment":"Code, models, and data at https://transcendence.eddie.win"},{"id":"http://arxiv.org/abs/2406.19653v1","updated":"2024-06-28T04:48:05Z","published":"2024-06-28T04:48:05Z","title":"ACES: Automatic Cohort Extraction System for Event-Stream Datasets","summary":"  Reproducibility remains a significant challenge in machine learning (ML) for\nhealthcare. In this field, datasets, model pipelines, and even task/cohort\ndefinitions are often private, leading to a significant barrier in sharing,\niterating, and understanding ML results on electronic health record (EHR)\ndatasets. In this paper, we address a significant part of this problem by\nintroducing the Automatic Cohort Extraction System for Event-Stream Datasets\n(ACES). This tool is designed to simultaneously simplify the development of\ntask/cohorts for ML in healthcare and enable the reproduction of these cohorts,\nboth at an exact level for single datasets and at a conceptual level across\ndatasets. To accomplish this, ACES provides (1) a highly intuitive and\nexpressive configuration language for defining both dataset-specific concepts\nand dataset-agnostic inclusion/exclusion criteria, and (2) a pipeline to\nautomatically extract patient records that meet these defined criteria from\nreal-world data. ACES can be automatically applied to any dataset in either the\nMedical Event Data Standard (MEDS) or EventStreamGPT (ESGPT) formats, or to\n*any* dataset for which the necessary task-specific predicates can be extracted\nin an event-stream form. ACES has the potential to significantly lower the\nbarrier to entry for defining ML tasks, redefine the way researchers interact\nwith EHR datasets, and significantly improve the state of reproducibility for\nML studies in this modality. ACES is available at\nhttps://github.com/justin13601/aces.\n","authors":["Justin Xu","Jack Gallifant","Alistair E. W. Johnson","Matthew B. A. McDermott"],"pdf_url":"https://arxiv.org/pdf/2406.19653v1.pdf","comment":"For ACES Online Documentation, see\n  https://eventstreamaces.readthedocs.io/en/latest/"},{"id":"http://arxiv.org/abs/2406.19651v1","updated":"2024-06-28T04:46:11Z","published":"2024-06-28T04:46:11Z","title":"CANDY: A Benchmark for Continuous Approximate Nearest Neighbor Search\n  with Dynamic Data Ingestion","summary":"  Approximate K Nearest Neighbor (AKNN) algorithms play a pivotal role in\nvarious AI applications, including information retrieval, computer vision, and\nnatural language processing. Although numerous AKNN algorithms and benchmarks\nhave been developed recently to evaluate their effectiveness, the dynamic\nnature of real-world data presents significant challenges that existing\nbenchmarks fail to address. Traditional benchmarks primarily assess retrieval\neffectiveness in static contexts and often overlook update efficiency, which is\ncrucial for handling continuous data ingestion. This limitation results in an\nincomplete assessment of an AKNN algorithms ability to adapt to changing data\npatterns, thereby restricting insights into their performance in dynamic\nenvironments. To address these gaps, we introduce CANDY, a benchmark tailored\nfor Continuous Approximate Nearest Neighbor Search with Dynamic Data Ingestion.\nCANDY comprehensively assesses a wide range of AKNN algorithms, integrating\nadvanced optimizations such as machine learning-driven inference to supplant\ntraditional heuristic scans, and improved distance computation methods to\nreduce computational overhead. Our extensive evaluations across diverse\ndatasets demonstrate that simpler AKNN baselines often surpass more complex\nalternatives in terms of recall and latency. These findings challenge\nestablished beliefs about the necessity of algorithmic complexity for high\nperformance. Furthermore, our results underscore existing challenges and\nilluminate future research opportunities. We have made the datasets and\nimplementation methods available at: https://github.com/intellistream/candy.\n","authors":["Xianzhi Zeng","Zhuoyan Wu","Xinjing Hu","Xuanhua Shi","Shixuan Sun","Shuhao Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.19651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19648v1","updated":"2024-06-28T04:33:41Z","published":"2024-06-28T04:33:41Z","title":"Designing and Evaluating Multi-Chatbot Interface for Human-AI\n  Communication: Preliminary Findings from a Persuasion Task","summary":"  The dynamics of human-AI communication have been reshaped by language models\nsuch as ChatGPT. However, extant research has primarily focused on dyadic\ncommunication, leaving much to be explored regarding the dynamics of human-AI\ncommunication in group settings. The availability of multiple language model\nchatbots presents a unique opportunity for scholars to better understand the\ninteraction between humans and multiple chatbots. This study examines the\nimpact of multi-chatbot communication in a specific persuasion setting:\npromoting charitable donations. We developed an online environment that enables\nmulti-chatbot communication and conducted a pilot experiment utilizing two\nGPT-based chatbots, Save the Children and UNICEF chatbots, to promote\ncharitable donations. In this study, we present our development process of the\nmulti-chatbot interface and present preliminary findings from a pilot\nexperiment. Analysis of qualitative and quantitative feedback are presented,\nand limitations are addressed.\n","authors":["Sion Yoon","Tae Eun Kim","Yoo Jung Oh"],"pdf_url":"https://arxiv.org/pdf/2406.19648v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19644v1","updated":"2024-06-28T04:21:24Z","published":"2024-06-28T04:21:24Z","title":"Beyond Human Preferences: Exploring Reinforcement Learning Trajectory\n  Evaluation and Improvement through LLMs","summary":"  Reinforcement learning (RL) faces challenges in evaluating policy\ntrajectories within intricate game tasks due to the difficulty in designing\ncomprehensive and precise reward functions. This inherent difficulty curtails\nthe broader application of RL within game environments characterized by diverse\nconstraints. Preference-based reinforcement learning (PbRL) presents a\npioneering framework that capitalizes on human preferences as pivotal reward\nsignals, thereby circumventing the need for meticulous reward engineering.\nHowever, obtaining preference data from human experts is costly and\ninefficient, especially under conditions marked by complex constraints. To\ntackle this challenge, we propose a LLM-enabled automatic preference generation\nframework named LLM4PG , which harnesses the capabilities of large language\nmodels (LLMs) to abstract trajectories, rank preferences, and reconstruct\nreward functions to optimize conditioned policies. Experiments on tasks with\ncomplex language constraints demonstrated the effectiveness of our LLM-enabled\nreward functions, accelerating RL convergence and overcoming stagnation caused\nby slow or absent progress under original reward structures. This approach\nmitigates the reliance on specialized human knowledge and demonstrates the\npotential of LLMs to enhance RL's effectiveness in complex environments in the\nwild.\n","authors":["Zichao Shen","Tianchen Zhu","Qingyun Sun","Shiqi Gao","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2406.19644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19643v1","updated":"2024-06-28T04:21:20Z","published":"2024-06-28T04:21:20Z","title":"Unlocking Varied Perspectives: A Persona-Based Multi-Agent Framework\n  with Debate-Driven Text Planning for Argument Generation","summary":"  Writing persuasive arguments is a challenging task for both humans and\nmachines. It entails incorporating high-level beliefs from various perspectives\non the topic, along with deliberate reasoning and planning to construct a\ncoherent narrative. Current language models often generate surface tokens\nautoregressively, lacking explicit integration of these underlying controls,\nresulting in limited output diversity and coherence. In this work, we propose a\npersona-based multi-agent framework for argument writing. Inspired by the human\ndebate, we first assign each agent a persona representing its high-level\nbeliefs from a unique perspective, and then design an agent interaction process\nso that the agents can collaboratively debate and discuss the idea to form an\noverall plan for argument writing. Such debate process enables fluid and\nnonlinear development of ideas. We evaluate our framework on argumentative\nessay writing. The results show that our framework can generate more diverse\nand persuasive arguments through both automatic and human evaluations.\n","authors":["Zhe Hu","Hou Pong Chan","Jing Li","Yu Yin"],"pdf_url":"https://arxiv.org/pdf/2406.19643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03181v2","updated":"2024-06-28T04:15:33Z","published":"2024-03-05T18:19:29Z","title":"Behavior Generation with Latent Actions","summary":"  Generative modeling of complex behaviors from labeled datasets has been a\nlongstanding problem in decision making. Unlike language or image generation,\ndecision making requires modeling actions - continuous-valued vectors that are\nmultimodal in their distribution, potentially drawn from uncurated sources,\nwhere generation errors can compound in sequential prediction. A recent class\nof models called Behavior Transformers (BeT) addresses this by discretizing\nactions using k-means clustering to capture different modes. However, k-means\nstruggles to scale for high-dimensional action spaces or long sequences, and\nlacks gradient information, and thus BeT suffers in modeling long-range\nactions. In this work, we present Vector-Quantized Behavior Transformer\n(VQ-BeT), a versatile model for behavior generation that handles multimodal\naction prediction, conditional generation, and partial observations. VQ-BeT\naugments BeT by tokenizing continuous actions with a hierarchical vector\nquantization module. Across seven environments including simulated\nmanipulation, autonomous driving, and robotics, VQ-BeT improves on\nstate-of-the-art models such as BeT and Diffusion Policies. Importantly, we\ndemonstrate VQ-BeT's improved ability to capture behavior modes while\naccelerating inference speed 5x over Diffusion Policies. Videos and code can be\nfound https://sjlee.cc/vq-bet\n","authors":["Seungjae Lee","Yibin Wang","Haritheja Etukuru","H. Jin Kim","Nur Muhammad Mahi Shafiullah","Lerrel Pinto"],"pdf_url":"https://arxiv.org/pdf/2403.03181v2.pdf","comment":"Github repo: https://github.com/jayLEE0301/vq_bet_official"},{"id":"http://arxiv.org/abs/2406.12058v3","updated":"2024-06-28T04:08:12Z","published":"2024-06-17T19:50:40Z","title":"WellDunn: On the Robustness and Explainability of Language Models and\n  Large Language Models in Identifying Wellness Dimensions","summary":"  Language Models (LMs) are being proposed for mental health applications where\nthe heightened risk of adverse outcomes means predictive performance may not be\na sufficient litmus test of a model's utility in clinical practice. A model\nthat can be trusted for practice should have a correspondence between\nexplanation and clinical determination, yet no prior research has examined the\nattention fidelity of these models and their effect on ground truth\nexplanations. We introduce an evaluation design that focuses on the robustness\nand explainability of LMs in identifying Wellness Dimensions (WD). We focus on\ntwo mental health and well-being datasets: (a) Multi-label Classification-based\nMultiWD, and (b) WellXplain for evaluating attention mechanism veracity against\nexpert-labeled explanations. The labels are based on Halbert Dunn's theory of\nwellness, which gives grounding to our evaluation. We reveal four surprising\nresults about LMs/LLMs: (1) Despite their human-like capabilities, GPT-3.5/4\nlag behind RoBERTa, and MedAlpaca, a fine-tuned LLM fails to deliver any\nremarkable improvements in performance or explanations. (2) Re-examining LMs'\npredictions based on a confidence-oriented loss function reveals a significant\nperformance drop. (3) Across all LMs/LLMs, the alignment between attention and\nexplanations remains low, with LLMs scoring a dismal 0.0. (4) Most mental\nhealth-specific LMs/LLMs overlook domain-specific knowledge and undervalue\nexplanations, causing these discrepancies. This study highlights the need for\nfurther research into their consistency and explanations in mental health and\nwell-being.\n","authors":["Seyedali Mohammadi","Edward Raff","Jinendra Malekar","Vedant Palit","Francis Ferraro","Manas Gaur"],"pdf_url":"https://arxiv.org/pdf/2406.12058v3.pdf","comment":"26 pages, including reference and appendix sections, 8 figures, and\n  16 tables"},{"id":"http://arxiv.org/abs/2406.02105v2","updated":"2024-06-28T04:05:53Z","published":"2024-06-04T08:33:56Z","title":"Kernel vs. Kernel: Exploring How the Data Structure Affects Neural\n  Collapse","summary":"  Recently, a vast amount of literature has focused on the \"Neural Collapse\"\n(NC) phenomenon, which emerges when training neural network (NN) classifiers\nbeyond the zero training error point. The core component of NC is the decrease\nin the within class variability of the network's deepest features, dubbed as\nNC1. The theoretical works that study NC are typically based on simplified\nunconstrained features models (UFMs) that mask any effect of the data on the\nextent of collapse. In this paper, we provide a kernel-based analysis that does\nnot suffer from this limitation. First, given a kernel function, we establish\nexpressions for the traces of the within- and between-class covariance matrices\nof the samples' features (and consequently an NC1 metric). Then, we turn to\nfocus on kernels associated with shallow NNs. First, we consider the NN\nGaussian Process kernel (NNGP), associated with the network at initialization,\nand the complement Neural Tangent Kernel (NTK), associated with its training in\nthe \"lazy regime\". Interestingly, we show that the NTK does not represent more\ncollapsed features than the NNGP for prototypical data models. As NC emerges\nfrom training, we then consider an alternative to NTK: the recently proposed\nadaptive kernel, which generalizes NNGP to model the feature mapping learned\nfrom the training data. Contrasting our NC1 analysis for these two kernels\nenables gaining insights into the effect of data distribution on the extent of\ncollapse, which are empirically aligned with the behavior observed with\npractical training of NNs.\n","authors":["Vignesh Kothapalli","Tom Tirer"],"pdf_url":"https://arxiv.org/pdf/2406.02105v2.pdf","comment":"34 pages, 14 figures"},{"id":"http://arxiv.org/abs/2405.16141v3","updated":"2024-06-28T03:59:15Z","published":"2024-05-25T09:21:43Z","title":"AIGB: Generative Auto-bidding via Diffusion Modeling","summary":"  Auto-bidding plays a crucial role in facilitating online advertising by\nautomatically providing bids for advertisers. Reinforcement learning (RL) has\ngained popularity for auto-bidding. However, most current RL auto-bidding\nmethods are modeled through the Markovian Decision Process (MDP), which assumes\nthe Markovian state transition. This assumption restricts the ability to\nperform in long horizon scenarios and makes the model unstable when dealing\nwith highly random online advertising environments. To tackle this issue, this\npaper introduces AI-Generated Bidding (AIGB), a novel paradigm for auto-bidding\nthrough generative modeling. In this paradigm, we propose DiffBid, a\nconditional diffusion modeling approach for bid generation. DiffBid directly\nmodels the correlation between the return and the entire trajectory,\neffectively avoiding error propagation across time steps in long horizons.\nAdditionally, DiffBid offers a versatile approach for generating trajectories\nthat maximize given targets while adhering to specific constraints. Extensive\nexperiments conducted on the real-world dataset and online A/B test on Alibaba\nadvertising platform demonstrate the effectiveness of DiffBid, achieving 2.81%\nincrease in GMV and 3.36% increase in ROI.\n","authors":["Jiayan Guo","Yusen Huo","Zhilin Zhang","Tianyu Wang","Chuan Yu","Jian Xu","Yan Zhang","Bo Zheng"],"pdf_url":"https://arxiv.org/pdf/2405.16141v3.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2406.19638v1","updated":"2024-06-28T03:58:02Z","published":"2024-06-28T03:58:02Z","title":"Precision matters: Precision-aware ensemble for weakly supervised\n  semantic segmentation","summary":"  Weakly Supervised Semantic Segmentation (WSSS) employs weak supervision, such\nas image-level labels, to train the segmentation model. Despite the impressive\nachievement in recent WSSS methods, we identify that introducing weak labels\nwith high mean Intersection of Union (mIoU) does not guarantee high\nsegmentation performance. Existing studies have emphasized the importance of\nprioritizing precision and reducing noise to improve overall performance. In\nthe same vein, we propose ORANDNet, an advanced ensemble approach tailored for\nWSSS. ORANDNet combines Class Activation Maps (CAMs) from two different\nclassifiers to increase the precision of pseudo-masks (PMs). To further\nmitigate small noise in the PMs, we incorporate curriculum learning. This\ninvolves training the segmentation model initially with pairs of smaller-sized\nimages and corresponding PMs, gradually transitioning to the original-sized\npairs. By combining the original CAMs of ResNet-50 and ViT, we significantly\nimprove the segmentation performance over the single-best model and the naive\nensemble model, respectively. We further extend our ensemble method to CAMs\nfrom AMN (ResNet-like) and MCTformer (ViT-like) models, achieving performance\nbenefits in advanced WSSS models. It highlights the potential of our ORANDNet\nas a final add-on module for WSSS models.\n","authors":["Junsung Park","Hyunjung Shim"],"pdf_url":"https://arxiv.org/pdf/2406.19638v1.pdf","comment":"5 pages, 5 figures, accepted in AAAI 2024 Edge Intelligence Workshop"},{"id":"http://arxiv.org/abs/2405.12807v7","updated":"2024-06-28T03:55:48Z","published":"2024-05-21T13:58:17Z","title":"FAdam: Adam is a natural gradient optimizer using diagonal empirical\n  Fisher information","summary":"  This paper establishes a mathematical foundation for the Adam optimizer,\nelucidating its connection to natural gradient descent through Riemannian and\ninformation geometry. We rigorously analyze the diagonal empirical Fisher\ninformation matrix (FIM) in Adam, clarifying all detailed approximations and\nadvocating for the use of log probability functions as loss, which should be\nbased on discrete distributions, due to the limitations of empirical FIM. Our\nanalysis uncovers flaws in the original Adam algorithm, leading to proposed\ncorrections such as enhanced momentum calculations, adjusted bias corrections,\nadaptive epsilon, and gradient clipping. We refine the weight decay term based\non our theoretical framework. Our modified algorithm, Fisher Adam (FAdam),\ndemonstrates superior performance across diverse domains including LLM, ASR,\nand VQ-VAE, achieving state-of-the-art results in ASR.\n","authors":["Dongseong Hwang"],"pdf_url":"https://arxiv.org/pdf/2405.12807v7.pdf","comment":"21 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.04607v4","updated":"2024-06-28T03:53:21Z","published":"2024-06-07T03:31:58Z","title":"MeGA: Merging Multiple Independently Trained Neural Networks Based on\n  Genetic Algorithm","summary":"  In this paper, we introduce a novel method for merging the weights of\nmultiple pre-trained neural networks using a genetic algorithm called MeGA.\nTraditional techniques, such as weight averaging and ensemble methods, often\nfail to fully harness the capabilities of pre-trained networks. Our approach\nleverages a genetic algorithm with tournament selection, crossover, and\nmutation to optimize weight combinations, creating a more effective fusion.\nThis technique allows the merged model to inherit advantageous features from\nboth parent models, resulting in enhanced accuracy and robustness. Through\nexperiments on the CIFAR-10 dataset, we demonstrate that our genetic\nalgorithm-based weight merging method improves test accuracy compared to\nindividual models and conventional methods. This approach provides a scalable\nsolution for integrating multiple pre-trained networks across various deep\nlearning applications. Github is available at:\nhttps://github.com/YUNBLAK/MeGA-Merging-Multiple-Independently-Trained-Neural-Networks-Based-on-Genetic-Algorithm\n","authors":["Daniel Yun"],"pdf_url":"https://arxiv.org/pdf/2406.04607v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19630v1","updated":"2024-06-28T03:36:38Z","published":"2024-06-28T03:36:38Z","title":"Optimal Video Compression using Pixel Shift Tracking","summary":"  The Video comprises approximately ~85\\% of all internet traffic, but video\nencoding/compression is being historically done with hard coded rules, which\nhas worked well but only to a certain limit. We have seen a surge in video\ncompression algorithms using ML-based models in the last few years and many of\nthem have outperformed several legacy codecs. The models range from encoding\nvideo end to end using an ML approach or replacing some intermediate steps in\nlegacy codecs using ML models to increase the efficiency of those steps.\n  Optimizing video storage is an essential aspect of video processing, so we\nare proposing one of the possible approaches to achieve it is by avoiding\nredundant data at each frame. In this paper, we want to introduce the approach\nof redundancies removal in subsequent frames for a given video as a main\napproach for video compression. We call this method Redundancy Removal using\nShift (R\\textsuperscript2S). This method can be utilized across various Machine\nLearning model algorithms, and make the compression more accessible and\nadaptable. In this study, we have utilized a computer vision-based pixel point\ntracking method to identify redundant pixels to encode video for optimal\nstorage.\n","authors":["Hitesh Saai Mananchery Panneerselvam","Smit Anand"],"pdf_url":"https://arxiv.org/pdf/2406.19630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13650v3","updated":"2024-06-28T03:33:28Z","published":"2023-05-23T03:47:32Z","title":"Robust Model-Based Optimization for Challenging Fitness Landscapes","summary":"  Protein design, a grand challenge of the day, involves optimization on a\nfitness landscape, and leading methods adopt a model-based approach where a\nmodel is trained on a training set (protein sequences and fitness) and proposes\ncandidates to explore next. These methods are challenged by sparsity of\nhigh-fitness samples in the training set, a problem that has been in the\nliterature. A less recognized but equally important problem stems from the\ndistribution of training samples in the design space: leading methods are not\ndesigned for scenarios where the desired optimum is in a region that is not\nonly poorly represented in training data, but also relatively far from the\nhighly represented low-fitness regions. We show that this problem of\n\"separation\" in the design space is a significant bottleneck in existing\nmodel-based optimization tools and propose a new approach that uses a novel VAE\nas its search model to overcome the problem. We demonstrate its advantage over\nprior methods in robustly finding improved samples, regardless of the imbalance\nand separation between low- and high-fitness samples. Our comprehensive\nbenchmark on real and semi-synthetic protein datasets as well as solution\ndesign for physics-informed neural networks, showcases the generality of our\napproach in discrete and continuous design spaces. Our implementation is\navailable at https://github.com/sabagh1994/PGVAE.\n","authors":["Saba Ghaffari","Ehsan Saleh","Alexander G. Schwing","Yu-Xiong Wang","Martin D. Burke","Saurabh Sinha"],"pdf_url":"https://arxiv.org/pdf/2305.13650v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19626v1","updated":"2024-06-28T03:29:33Z","published":"2024-06-28T03:29:33Z","title":"Safety through feedback in Constrained RL","summary":"  In safety-critical RL settings, the inclusion of an additional cost function\nis often favoured over the arduous task of modifying the reward function to\nensure the agent's safe behaviour. However, designing or evaluating such a cost\nfunction can be prohibitively expensive. For instance, in the domain of\nself-driving, designing a cost function that encompasses all unsafe behaviours\n(e.g. aggressive lane changes) is inherently complex. In such scenarios, the\ncost function can be learned from feedback collected offline in between\ntraining rounds. This feedback can be system generated or elicited from a human\nobserving the training process. Previous approaches have not been able to scale\nto complex environments and are constrained to receiving feedback at the state\nlevel which can be expensive to collect. To this end, we introduce an approach\nthat scales to more complex domains and extends to beyond state-level feedback,\nthus, reducing the burden on the evaluator. Inferring the cost function in such\nsettings poses challenges, particularly in assigning credit to individual\nstates based on trajectory-level feedback. To address this, we propose a\nsurrogate objective that transforms the problem into a state-level supervised\nclassification task with noisy labels, which can be solved efficiently.\nAdditionally, it is often infeasible to collect feedback on every trajectory\ngenerated by the agent, hence, two fundamental questions arise: (1) Which\ntrajectories should be presented to the human? and (2) How many trajectories\nare necessary for effective learning? To address these questions, we introduce\n\\textit{novelty-based sampling} that selectively involves the evaluator only\nwhen the the agent encounters a \\textit{novel} trajectory. We showcase the\nefficiency of our method through experimentation on several benchmark Safety\nGymnasium environments and realistic self-driving scenarios.\n","authors":["Shashank Reddy Chirra","Pradeep Varakantham","Praveen Paruchuri"],"pdf_url":"https://arxiv.org/pdf/2406.19626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16537v2","updated":"2024-06-28T03:21:15Z","published":"2024-06-24T11:16:37Z","title":"Character-Adapter: Prompt-Guided Region Control for High-Fidelity\n  Character Customization","summary":"  Customized image generation, which seeks to synthesize images with consistent\ncharacters, holds significant relevance for applications such as storytelling,\nportrait generation, and character design. However, previous approaches have\nencountered challenges in preserving characters with high-fidelity consistency\ndue to inadequate feature extraction and concept confusion of reference\ncharacters. Therefore, we propose Character-Adapter, a plug-and-play framework\ndesigned to generate images that preserve the details of reference characters,\nensuring high-fidelity consistency. Character-Adapter employs prompt-guided\nsegmentation to ensure fine-grained regional features of reference characters\nand dynamic region-level adapters to mitigate concept confusion. Extensive\nexperiments are conducted to validate the effectiveness of Character-Adapter.\nBoth quantitative and qualitative results demonstrate that Character-Adapter\nachieves the state-of-the-art performance of consistent character generation,\nwith an improvement of 24.8% compared with other methods. Our code will be\nreleased at https://github.com/Character-Adapter/Character-Adapte\n","authors":["Yuhang Ma","Wenting Xu","Jiji Tang","Qinfeng Jin","Rongsheng Zhang","Zeng Zhao","Changjie Fan","Zhipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2406.16537v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11160v3","updated":"2024-06-28T03:20:22Z","published":"2024-06-17T02:59:19Z","title":"Context Graph","summary":"  Knowledge Graphs (KGs) are foundational structures in many AI applications,\nrepresenting entities and their interrelations through triples. However,\ntriple-based KGs lack the contextual information of relational knowledge, like\ntemporal dynamics and provenance details, which are crucial for comprehensive\nknowledge representation and effective reasoning. Instead, \\textbf{Context\nGraphs} (CGs) expand upon the conventional structure by incorporating\nadditional information such as time validity, geographic location, and source\nprovenance. This integration provides a more nuanced and accurate understanding\nof knowledge, enabling KGs to offer richer insights and support more\nsophisticated reasoning processes. In this work, we first discuss the inherent\nlimitations of triple-based KGs and introduce the concept of CGs, highlighting\ntheir advantages in knowledge representation and reasoning. We then present a\ncontext graph reasoning \\textbf{CGR$^3$} paradigm that leverages large language\nmodels (LLMs) to retrieve candidate entities and related contexts, rank them\nbased on the retrieved information, and reason whether sufficient information\nhas been obtained to answer a query. Our experimental results demonstrate that\nCGR$^3$ significantly improves performance on KG completion (KGC) and KG\nquestion answering (KGQA) tasks, validating the effectiveness of incorporating\ncontextual information on KG representation and reasoning.\n","authors":["Chengjin Xu","Muzhi Li","Cehao Yang","Xuhui Jiang","Lumingyuan Tang","Yiyan Qi","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2406.11160v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19622v1","updated":"2024-06-28T03:10:36Z","published":"2024-06-28T03:10:36Z","title":"Data-Driven Lipschitz Continuity: A Cost-Effective Approach to Improve\n  Adversarial Robustness","summary":"  The security and robustness of deep neural networks (DNNs) have become\nincreasingly concerning. This paper aims to provide both a theoretical\nfoundation and a practical solution to ensure the reliability of DNNs. We\nexplore the concept of Lipschitz continuity to certify the robustness of DNNs\nagainst adversarial attacks, which aim to mislead the network with adding\nimperceptible perturbations into inputs. We propose a novel algorithm that\nremaps the input domain into a constrained range, reducing the Lipschitz\nconstant and potentially enhancing robustness. Unlike existing adversarially\ntrained models, where robustness is enhanced by introducing additional examples\nfrom other datasets or generative models, our method is almost cost-free as it\ncan be integrated with existing models without requiring re-training.\nExperimental results demonstrate the generalizability of our method, as it can\nbe combined with various models and achieve enhancements in robustness.\nFurthermore, our method achieves the best robust accuracy for CIFAR10,\nCIFAR100, and ImageNet datasets on the RobustBench leaderboard.\n","authors":["Erh-Chung Chen","Pin-Yu Chen","I-Hsin Chung","Che-Rung Lee"],"pdf_url":"https://arxiv.org/pdf/2406.19622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06777v3","updated":"2024-06-28T03:07:29Z","published":"2024-06-10T20:25:18Z","title":"MolX: Enhancing Large Language Models for Molecular Learning with A\n  Multi-Modal Extension","summary":"  Recently, Large Language Models (LLMs) with their strong task-handling\ncapabilities have shown remarkable advancements across a spectrum of fields,\nmoving beyond natural language understanding. However, their proficiency within\nthe chemistry domain remains restricted, especially in solving professional\nmolecule-related tasks. This challenge is attributed to their inherent\nlimitations in comprehending molecules using only common textual\nrepresentations, i.e., SMILES strings. In this study, we seek to enhance the\nability of LLMs to comprehend molecules by designing and equipping them with a\nmulti-modal external module, namely MolX. In particular, instead of directly\nusing a SMILES string to represent a molecule, we utilize specific encoders to\nextract fine-grained features from both SMILES string and 2D molecular graph\nrepresentations for feeding into an LLM. Moreover, a human-defined molecular\nfingerprint is incorporated to leverage its embedded domain knowledge. Then, to\nestablish an alignment between MolX and the LLM's textual input space, the\nwhole model in which the LLM is frozen, is pre-trained with a versatile\nstrategy including a diverse set of tasks. Extensive experimental evaluations\ndemonstrate that our proposed method only introduces a small number of\ntrainable parameters while outperforming baselines on various downstream\nmolecule-related tasks ranging from molecule-to-text translation to\nretrosynthesis, with and without fine-tuning the LLM.\n","authors":["Khiem Le","Zhichun Guo","Kaiwen Dong","Xiaobao Huang","Bozhao Nan","Roshni Iyer","Xiangliang Zhang","Olaf Wiest","Wei Wang","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2406.06777v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18841v2","updated":"2024-06-28T02:56:09Z","published":"2024-05-14T15:03:05Z","title":"Navigating LLM Ethics: Advancements, Challenges, and Future Directions","summary":"  This study addresses ethical issues surrounding Large Language Models (LLMs)\nwithin the field of artificial intelligence. It explores the common ethical\nchallenges posed by both LLMs and other AI systems, such as privacy and\nfairness, as well as ethical challenges uniquely arising from LLMs. It\nhighlights challenges such as hallucination, verifiable accountability, and\ndecoding censorship complexity, which are unique to LLMs and distinct from\nthose encountered in traditional AI systems. The study underscores the need to\ntackle these complexities to ensure accountability, reduce biases, and enhance\ntransparency in the influential role that LLMs play in shaping information\ndissemination. It proposes mitigation strategies and future directions for LLM\nethics, advocating for interdisciplinary collaboration. It recommends ethical\nframeworks tailored to specific domains and dynamic auditing systems adapted to\ndiverse contexts. This roadmap aims to guide responsible development and\nintegration of LLMs, envisioning a future where ethical considerations govern\nAI advancements in society.\n","authors":["Junfeng Jiao","Saleh Afroogh","Yiming Xu","Connor Phillips"],"pdf_url":"https://arxiv.org/pdf/2406.18841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18842v2","updated":"2024-06-28T02:54:06Z","published":"2024-05-26T15:28:24Z","title":"The global landscape of academic guidelines for generative AI and Large\n  Language Models","summary":"  The integration of Generative Artificial Intelligence (GAI) and Large\nLanguage Models (LLMs) in academia has spurred a global discourse on their\npotential pedagogical benefits and ethical considerations. Positive reactions\nhighlight some potential, such as collaborative creativity, increased access to\neducation, and empowerment of trainers and trainees. However, negative\nreactions raise concerns about ethical complexities, balancing innovation and\nacademic integrity, unequal access, and misinformation risks. Through a\nsystematic survey and text-mining-based analysis of global and national\ndirectives, insights from independent research, and eighty university-level\nguidelines, this study provides a nuanced understanding of the opportunities\nand challenges posed by GAI and LLMs in education. It emphasizes the importance\nof balanced approaches that harness the benefits of these technologies while\naddressing ethical considerations and ensuring equitable access and educational\noutcomes. The paper concludes with recommendations for fostering responsible\ninnovation and ethical practices to guide the integration of GAI and LLMs in\nacademia.\n","authors":["Junfeng Jiao","Saleh Afroogh","Kevin Chen","David Atkinson","Amit Dhurandhar"],"pdf_url":"https://arxiv.org/pdf/2406.18842v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19614v1","updated":"2024-06-28T02:41:33Z","published":"2024-06-28T02:41:33Z","title":"A Survey on Data Quality Dimensions and Tools for Machine Learning","summary":"  Machine learning (ML) technologies have become substantial in practically all\naspects of our society, and data quality (DQ) is critical for the performance,\nfairness, robustness, safety, and scalability of ML models. With the large and\ncomplex data in data-centric AI, traditional methods like exploratory data\nanalysis (EDA) and cross-validation (CV) face challenges, highlighting the\nimportance of mastering DQ tools. In this survey, we review 17 DQ evaluation\nand improvement tools in the last 5 years. By introducing the DQ dimensions,\nmetrics, and main functions embedded in these tools, we compare their strengths\nand limitations and propose a roadmap for developing open-source DQ tools for\nML. Based on the discussions on the challenges and emerging trends, we further\nhighlight the potential applications of large language models (LLMs) and\ngenerative AI in DQ evaluation and improvement for ML. We believe this\ncomprehensive survey can enhance understanding of DQ in ML and could drive\nprogress in data-centric AI. A complete list of the literature investigated in\nthis survey is available on GitHub at:\nhttps://github.com/haihua0913/awesome-dq4ml.\n","authors":["Yuhan Zhou","Fengjiao Tu","Kewei Sha","Junhua Ding","Haihua Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19614v1.pdf","comment":"This paper has been accepted by The 6th IEEE International Conference\n  on Artificial Intelligence Testing (IEEE AITest 2024) as an invited paper"},{"id":"http://arxiv.org/abs/2403.02990v3","updated":"2024-06-28T02:35:38Z","published":"2024-03-05T14:11:54Z","title":"Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and\n  Challenges","summary":"  In the rapidly evolving field of large language models (LLMs), data\naugmentation (DA) has emerged as a pivotal technique for enhancing model\nperformance by diversifying training examples without the need for additional\ndata collection. This survey explores the transformative impact of LLMs on DA,\nparticularly addressing the unique challenges and opportunities they present in\nthe context of natural language processing (NLP) and beyond. From both data and\nlearning perspectives, we examine various strategies that utilize LLMs for data\naugmentation, including a novel exploration of learning paradigms where\nLLM-generated data is used for diverse forms of further training. Additionally,\nthis paper highlights the primary open challenges faced in this domain, ranging\nfrom controllable data augmentation to multi-modal data augmentation. This\nsurvey highlights a paradigm shift introduced by LLMs in DA, and aims to serve\nas a comprehensive guide for researchers and practitioners.\n","authors":["Bosheng Ding","Chengwei Qin","Ruochen Zhao","Tianze Luo","Xinze Li","Guizhen Chen","Wenhan Xia","Junjie Hu","Anh Tuan Luu","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2403.02990v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19611v1","updated":"2024-06-28T02:35:05Z","published":"2024-06-28T02:35:05Z","title":"Multimodal Data Integration for Precision Oncology: Challenges and\n  Future Directions","summary":"  The essence of precision oncology lies in its commitment to tailor targeted\ntreatments and care measures to each patient based on the individual\ncharacteristics of the tumor. The inherent heterogeneity of tumors necessitates\ngathering information from diverse data sources to provide valuable insights\nfrom various perspectives, fostering a holistic comprehension of the tumor.\nOver the past decade, multimodal data integration technology for precision\noncology has made significant strides, showcasing remarkable progress in\nunderstanding the intricate details within heterogeneous data modalities. These\nstrides have exhibited tremendous potential for improving clinical\ndecision-making and model interpretation, contributing to the advancement of\ncancer care and treatment. Given the rapid progress that has been achieved, we\nprovide a comprehensive overview of about 300 papers detailing cutting-edge\nmultimodal data integration techniques in precision oncology. In addition, we\nconclude the primary clinical applications that have reaped significant\nbenefits, including early assessment, diagnosis, prognosis, and biomarker\ndiscovery. Finally, derived from the findings of this survey, we present an\nin-depth analysis that explores the pivotal challenges and reveals essential\npathways for future research in the field of multimodal data integration for\nprecision oncology.\n","authors":["Huajun Zhou","Fengtao Zhou","Chenyu Zhao","Yingxue Xu","Luyang Luo","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2406.19611v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2402.00808v2","updated":"2024-06-28T02:16:13Z","published":"2024-02-01T17:44:46Z","title":"Exploring the Dynamics between Cobot's Production Rhythm, Locus of\n  Control and Emotional State in a Collaborative Assembly Scenario","summary":"  In industrial scenarios, there is widespread use of collaborative robots\n(cobots), and growing interest is directed at evaluating and measuring the\nimpact of some characteristics of the cobot on the human factor. In the present\npilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 -\nAdapted to the participant's pace) of a cobot has on the Experiential Locus of\nControl (ELoC) and the emotional state of 31 participants has been examined.\nThe operators' performance, the degree of basic internal Locus of Control, and\nthe attitude towards the robots were also considered. No difference was found\nregarding the emotional state and the ELoC in the three conditions, but\nconsidering the other psychological variables, a more complex situation\nemerges. Overall, results seem to indicate a need to consider the person's\npsychological characteristics to offer a differentiated and optimal interaction\nexperience.\n","authors":["Marta Mondellini","Matteo Lavit Nicora","Pooja Prajod","Elisabeth André","Rocco Vertechy","Alessandro Antonietti","Matteo Malosio"],"pdf_url":"https://arxiv.org/pdf/2402.00808v2.pdf","comment":"Accepted to 4th IEEE International Conference on Human-Machine\n  Systems"},{"id":"http://arxiv.org/abs/2406.19596v1","updated":"2024-06-28T01:37:46Z","published":"2024-06-28T01:37:46Z","title":"Optimizing Cyber Defense in Dynamic Active Directories through\n  Reinforcement Learning","summary":"  This paper addresses a significant gap in Autonomous Cyber Operations (ACO)\nliterature: the absence of effective edge-blocking ACO strategies in dynamic,\nreal-world networks. It specifically targets the cybersecurity vulnerabilities\nof organizational Active Directory (AD) systems. Unlike the existing literature\non edge-blocking defenses which considers AD systems as static entities, our\nstudy counters this by recognizing their dynamic nature and developing advanced\nedge-blocking defenses through a Stackelberg game model between attacker and\ndefender. We devise a Reinforcement Learning (RL)-based attack strategy and an\nRL-assisted Evolutionary Diversity Optimization-based defense strategy, where\nthe attacker and defender improve each other strategy via parallel gameplay. To\naddress the computational challenges of training attacker-defender strategies\non numerous dynamic AD graphs, we propose an RL Training Facilitator that\nprunes environments and neural networks to eliminate irrelevant elements,\nenabling efficient and scalable training for large graphs. We extensively train\nthe attacker strategy, as a sophisticated attacker model is essential for a\nrobust defense. Our empirical results successfully demonstrate that our\nproposed approach enhances defender's proficiency in hardening dynamic AD\ngraphs while ensuring scalability for large-scale AD.\n","authors":["Diksha Goel","Kristen Moore","Mingyu Guo","Derui Wang","Minjune Kim","Seyit Camtepe"],"pdf_url":"https://arxiv.org/pdf/2406.19596v1.pdf","comment":"The manuscript has been accepted as full paper at European Symposium\n  on Research in Computer Security (ESORICS) 2024"},{"id":"http://arxiv.org/abs/2405.05480v2","updated":"2024-06-28T00:05:14Z","published":"2024-05-09T00:37:56Z","title":"FloorSet -- a VLSI Floorplanning Dataset with Design Constraints of\n  Real-World SoCs","summary":"  Floorplanning for systems-on-a-chip (SoCs) and its sub-systems is a crucial\nand non-trivial step of the physical design flow. It represents a difficult\ncombinatorial optimization problem. A typical large scale SoC with 120\npartitions generates a search-space of nearly 10E250. As novel machine learning\n(ML) approaches emerge to tackle such problems, there is a growing need for a\nmodern benchmark that comprises a large training dataset and performance\nmetrics that better reflect real-world constraints and objectives compared to\nexisting benchmarks. To address this need, we present FloorSet -- two\ncomprehensive datasets of synthetic fixed-outline floorplan layouts that\nreflect the distribution of real SoCs. Each dataset has 1M training samples and\n100 test samples where each sample is a synthetic floor-plan. FloorSet-Prime\ncomprises fully-abutted rectilinear partitions and near-optimal wire-length. A\nsimplified dataset that reflects early design phases, FloorSet-Lite comprises\nrectangular partitions, with under 5 percent white-space and near-optimal\nwire-length. Both datasets define hard constraints seen in modern design flows\nsuch as shape constraints, edge-affinity, grouping constraints, and\npre-placement constraints. FloorSet is intended to spur fundamental research on\nlarge-scale constrained optimization problems. Crucially, FloorSet alleviates\nthe core issue of reproducibility in modern ML driven solutions to such\nproblems. FloorSet is available as an open-source repository for the research\ncommunity.\n","authors":["Uday Mallappa","Hesham Mostafa","Mikhail Galkin","Mariano Phielipp","Somdeb Majumdar"],"pdf_url":"https://arxiv.org/pdf/2405.05480v2.pdf","comment":"10 pages, 11 figures"}]},"2024-06-27T00:00:00Z":{"Image and Video Processing":[{"id":"http://arxiv.org/abs/2406.19574v1","updated":"2024-06-27T23:26:57Z","published":"2024-06-27T23:26:57Z","title":"Deep Temporal Sequence Classification and Mathematical Modeling for Cell\n  Tracking in Dense 3D Microscopy Videos of Bacterial Biofilms","summary":"  Automatic cell tracking in dense environments is plagued by inaccurate\ncorrespondences and misidentification of parent-offspring relationships. In\nthis paper, we introduce a novel cell tracking algorithm named DenseTrack,\nwhich integrates deep learning with mathematical model-based strategies to\neffectively establish correspondences between consecutive frames and detect\ncell division events in crowded scenarios. We formulate the cell tracking\nproblem as a deep learning-based temporal sequence classification task followed\nby solving a constrained one-to-one matching optimization problem exploiting\nthe classifier's confidence scores. Additionally, we present an\neigendecomposition-based cell division detection strategy that leverages\nknowledge of cellular geometry. The performance of the proposed approach has\nbeen evaluated by tracking densely packed cells in 3D time-lapse image\nsequences of bacterial biofilm development. The experimental results on\nsimulated as well as experimental fluorescence image sequences suggest that the\nproposed tracking method achieves superior performance in terms of both\nqualitative and quantitative evaluation measures compared to recent\nstate-of-the-art cell tracking approaches.\n","authors":["Tanjin Taher Toma","Yibo Wang","Andreas Gahlmann","Scott T. Acton"],"pdf_url":"https://arxiv.org/pdf/2406.19574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19560v1","updated":"2024-06-27T22:19:19Z","published":"2024-06-27T22:19:19Z","title":"Cost-efficient Active Illumination Camera For Hyper-spectral\n  Reconstruction","summary":"  Hyper-spectral imaging has recently gained increasing attention for use in\ndifferent applications, including agricultural investigation, ground tracking,\nremote sensing and many other. However, the high cost, large physical size and\ncomplicated operation process stop hyperspectral cameras from being employed\nfor various applications and research fields. In this paper, we introduce a\ncost-efficient, compact and easy to use active illumination camera that may\nbenefit many applications. We developed a fully functional prototype of such\ncamera. With the hope of helping with agricultural research, we tested our\ncamera for plant root imaging. In addition, a U-Net model for spectral\nreconstruction was trained by using a reference hyperspectral camera's data as\nground truth and our camera's data as input. We demonstrated our camera's\nability to obtain additional information over a typical RGB camera. In\naddition, the ability to reconstruct hyperspectral data from multi-spectral\ninput makes our device compatible to models and algorithms developed for\nhyperspectral applications with no modifications required.\n","authors":["Yuxuan Zhang","T. M. Sazzad","Yangyang Song","Spencer J. Chang","Ritesh Chowdhry","Tomas Mejia","Anna Hampton","Shelby Kucharski","Stefan Gerber","Barry Tillman","Marcio F. R. Resende","William M. Hammond","Chris H. Wilson","Alina Zare","Sanjeev J. Koppal"],"pdf_url":"https://arxiv.org/pdf/2406.19560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19557v1","updated":"2024-06-27T22:17:49Z","published":"2024-06-27T22:17:49Z","title":"Robustness Testing of Black-Box Models Against CT Degradation Through\n  Test-Time Augmentation","summary":"  Deep learning models for medical image segmentation and object detection are\nbecoming increasingly available as clinical products. However, as details are\nrarely provided about the training data, models may unexpectedly fail when\ncases differ from those in the training distribution. An approach allowing\npotential users to independently test the robustness of a model, treating it as\na black box and using only a few cases from their own site, is key for\nadoption. To address this, a method to test the robustness of these models\nagainst CT image quality variation is presented. In this work we present this\nframework by demonstrating that given the same training data, the model\narchitecture and data pre processing greatly affect the robustness of several\nfrequently used segmentation and object detection methods to simulated CT\nimaging artifacts and degradation. Our framework also addresses the concern\nabout the sustainability of deep learning models in clinical use, by\nconsidering future shifts in image quality due to scanner deterioration or\nimaging protocol changes which are not reflected in a limited local test\ndataset.\n","authors":["Jack Highton","Quok Zong Chong","Samuel Finestone","Arian Beqiri","Julia A. Schnabel","Kanwal K. Bhatia"],"pdf_url":"https://arxiv.org/pdf/2406.19557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19556v1","updated":"2024-06-27T22:16:53Z","published":"2024-06-27T22:16:53Z","title":"BOrg: A Brain Organoid-Based Mitosis Dataset for Automatic Analysis of\n  Brain Diseases","summary":"  Recent advances have enabled the study of human brain development using brain\norganoids derived from stem cells. Quantifying cellular processes like mitosis\nin these organoids offers insights into neurodevelopmental disorders, but the\nmanual analysis is time-consuming, and existing datasets lack specific details\nfor brain organoid studies. We introduce BOrg, a dataset designed to study\nmitotic events in the embryonic development of the brain using confocal\nmicroscopy images of brain organoids. BOrg utilizes an efficient annotation\npipeline with sparse point annotations and techniques that minimize expert\neffort, overcoming limitations of standard deep learning approaches on sparse\ndata. We adapt and benchmark state-of-the-art object detection and cell\ncounting models on BOrg for detecting and analyzing mitotic cells across\nprophase, metaphase, anaphase, and telophase stages. Our results demonstrate\nthese adapted models significantly improve mitosis analysis efficiency and\naccuracy for brain organoid research compared to existing methods. BOrg\nfacilitates the development of automated tools to quantify statistics like\nmitosis rates, aiding mechanistic studies of neurodevelopmental processes and\ndisorders. Data and code are available at https://github.com/awaisrauf/borg.\n","authors":["Muhammad Awais","Mehaboobathunnisa Sahul Hameed","Bidisha Bhattacharya","Orly Reiner","Rao Muhammad Anwer"],"pdf_url":"https://arxiv.org/pdf/2406.19556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15727v2","updated":"2024-06-27T20:13:34Z","published":"2024-06-22T04:32:50Z","title":"Semi-supervised variational autoencoder for cell feature extraction in\n  multiplexed immunofluorescence images","summary":"  Advancements in digital imaging technologies have sparked increased interest\nin using multiplexed immunofluorescence (mIF) images to visualise and identify\nthe interactions between specific immunophenotypes with the tumour\nmicroenvironment at the cellular level. Current state-of-the-art multiplexed\nimmunofluorescence image analysis pipelines depend on cell feature\nrepresentations characterised by morphological and stain intensity-based\nmetrics generated using simple statistical and machine learning-based tools.\nHowever, these methods are not capable of generating complex representations of\ncells. We propose a deep learning-based cell feature extraction model using a\nvariational autoencoder with supervision using a latent subspace to extract\ncell features in mIF images. We perform cell phenotype classification using a\ncohort of more than 44,000 multiplexed immunofluorescence cell image patches\nextracted across 1,093 tissue microarray cores of breast cancer patients, to\ndemonstrate the success of our model against current and alternative methods.\n","authors":["Piumi Sandarenu","Julia Chen","Iveta Slapetova","Lois Browne","Peter H. Graham","Alexander Swarbrick","Ewan K. A. Millar","Yang Song","Erik Meijering"],"pdf_url":"https://arxiv.org/pdf/2406.15727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19492v1","updated":"2024-06-27T19:16:57Z","published":"2024-06-27T19:16:57Z","title":"High-resolution segmentations of the hypothalamus and its subregions for\n  training of segmentation models","summary":"  Segmentation of brain structures on magnetic resonance imaging (MRI) is a\nhighly relevant neuroimaging topic, as it is a prerequisite for different\nanalyses such as volumetry or shape analysis. Automated segmentation\nfacilitates the study of brain structures in larger cohorts when compared with\nmanual segmentation, which is time-consuming. However, the development of most\nautomated methods relies on large and manually annotated datasets, which limits\nthe generalizability of these methods. Recently, new techniques using synthetic\nimages have emerged, reducing the need for manual annotation. Here we provide\nHELM, Hypothalamic ex vivo Label Maps, a dataset composed of label maps built\nfrom publicly available ultra-high resolution ex vivo MRI from 10 whole\nhemispheres, which can be used to develop segmentation methods using synthetic\ndata. The label maps are obtained with a combination of manual labels for the\nhypothalamic regions and automated segmentations for the rest of the brain, and\nmirrored to simulate entire brains. We also provide the pre-processed ex vivo\nscans, as this dataset can support future projects to include other structures\nafter these are manually segmented.\n","authors":["Livia Rodrigues","Martina Bocchetta","Oula Puonti","Douglas Greve","Ana Carolina Londe","Marcondes França","Simone Appenzeller","Leticia Rittner","Juan Eugenio Iglesias"],"pdf_url":"https://arxiv.org/pdf/2406.19492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19485v1","updated":"2024-06-27T18:58:41Z","published":"2024-06-27T18:58:41Z","title":"GAPNet: Granularity Attention Network with Anatomy-Prior-Constraint for\n  Carotid Artery Segmentation","summary":"  Atherosclerosis is a chronic, progressive disease that primarily affects the\narterial walls. It is one of the major causes of cardiovascular disease.\nMagnetic Resonance (MR) black-blood vessel wall imaging (BB-VWI) offers crucial\ninsights into vascular disease diagnosis by clearly visualizing vascular\nstructures. However, the complex anatomy of the neck poses challenges in\ndistinguishing the carotid artery (CA) from surrounding structures, especially\nwith changes like atherosclerosis. In order to address these issues, we propose\nGAPNet, which is a consisting of a novel geometric prior deduced from.\n","authors":["Lin Zhang","Chenggang Lu","Xin-yang Shi","Caifeng Shan","Jiong Zhang","Da Chen","Laurent D. Cohen"],"pdf_url":"https://arxiv.org/pdf/2406.19485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13040v2","updated":"2024-06-27T17:27:13Z","published":"2024-03-19T17:35:17Z","title":"Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping","summary":"  Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify\ncolor Doppler in cardiac imaging. In this study, we propose novel alternatives\nto the traditional iVFM optimization scheme by utilizing physics-informed\nneural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.\nWhen evaluated on simulated color Doppler images derived from a\npatient-specific computational fluid dynamics model and in vivo Doppler\nacquisitions, both approaches demonstrate comparable reconstruction performance\nto the original iVFM algorithm. The efficiency of PINNs is boosted through\ndual-stage optimization and pre-optimized weights. On the other hand, the\nnnU-Net method excels in generalizability and real-time capabilities. Notably,\nnnU-Net shows superior robustness on sparse and truncated Doppler data while\nmaintaining independence from explicit boundary conditions. Overall, our\nresults highlight the effectiveness of these methods in reconstructing\nintraventricular vector blood flow. The study also suggests potential\napplications of PINNs in ultrafast color Doppler imaging and the incorporation\nof fluid dynamics equations to derive biomarkers for cardiovascular diseases\nbased on blood flow.\n","authors":["Hang Jung Ling","Salomé Bru","Julia Puig","Florian Vixège","Simon Mendez","Franck Nicoud","Pierre-Yves Courand","Olivier Bernard","Damien Garcia"],"pdf_url":"https://arxiv.org/pdf/2403.13040v2.pdf","comment":"12 pages, accepted for publication in IEEE TUFFC; camera ready\n  corrections, corrected acknowledgments"},{"id":"http://arxiv.org/abs/2312.17293v3","updated":"2024-06-27T16:38:18Z","published":"2023-12-28T13:59:43Z","title":"$μ$GUIDE: a framework for quantitative imaging via generalized\n  uncertainty-driven inference using deep learning","summary":"  This work proposes $\\mu$GUIDE: a general Bayesian framework to estimate\nposterior distributions of tissue microstructure parameters from any given\nbiophysical model or MRI signal representation, with exemplar demonstration in\ndiffusion-weighted MRI. Harnessing a new deep learning architecture for\nautomatic signal feature selection combined with simulation-based inference and\nefficient sampling of the posterior distributions, $\\mu$GUIDE bypasses the high\ncomputational and time cost of conventional Bayesian approaches and does not\nrely on acquisition constraints to define model-specific summary statistics.\nThe obtained posterior distributions allow to highlight degeneracies present in\nthe model definition and quantify the uncertainty and ambiguity of the\nestimated parameters.\n","authors":["Maëliss Jallais","Marco Palombo"],"pdf_url":"https://arxiv.org/pdf/2312.17293v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06748v2","updated":"2024-06-27T15:24:23Z","published":"2024-03-11T14:14:52Z","title":"Shortcut Learning in Medical Image Segmentation","summary":"  Shortcut learning is a phenomenon where machine learning models prioritize\nlearning simple, potentially misleading cues from data that do not generalize\nwell beyond the training set. While existing research primarily investigates\nthis in the realm of image classification, this study extends the exploration\nof shortcut learning into medical image segmentation. We demonstrate that\nclinical annotations such as calipers, and the combination of zero-padded\nconvolutions and center-cropped training sets in the dataset can inadvertently\nserve as shortcuts, impacting segmentation accuracy. We identify and evaluate\nthe shortcut learning on two different but common medical image segmentation\ntasks. In addition, we suggest strategies to mitigate the influence of shortcut\nlearning and improve the generalizability of the segmentation models. By\nuncovering the presence and implications of shortcuts in medical image\nsegmentation, we provide insights and methodologies for evaluating and\novercoming this pervasive challenge and call for attention in the community for\nshortcuts in segmentation. Our code is public at\nhttps://github.com/nina-weng/shortcut_skinseg .\n","authors":["Manxi Lin","Nina Weng","Kamil Mikolaj","Zahra Bashir","Morten Bo Søndergaard Svendsen","Martin Tolsgaard","Anders Nymark Christensen","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2403.06748v2.pdf","comment":"11 pages, 6 figures, accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.19239v1","updated":"2024-06-27T15:02:04Z","published":"2024-06-27T15:02:04Z","title":"ALMA: a mathematics-driven approach for determining tuning parameters in\n  generalized LASSO problems, with applications to MRI","summary":"  Magnetic Resonance Imaging (MRI) is a powerful technique employed for\nnon-invasive in vivo visualization of internal structures. Sparsity is often\ndeployed to accelerate the signal acquisition or overcome the presence of\nmotion artifacts, improving the quality of image reconstruction. Image\nreconstruction algorithms use TV-regularized LASSO (Total Variation-regularized\nLASSO) to retrieve the missing information of undersampled signals, by cleaning\nthe data of noise and while optimizing sparsity. A tuning parameter moderates\nthe balance between these two aspects; its choice affecting the quality of the\nreconstructions. Currently, there is a lack of general deterministic techniques\nto choose these parameters, which are oftentimes manually selected and thus\nhinder the reliability of the reconstructions. Here, we present ALMA (Algorithm\nfor Lagrange Multipliers Approximation), an iterative mathematics-inspired\ntechnique that computes tuning parameters for generalized LASSO problems during\nMRI reconstruction. We analyze quantitatively the performance of these\nparameters for imaging reconstructions via TV-LASSO in an MRI context on\nphantoms. Although our study concentrates on TV-LASSO, the techniques developed\nhere hold significant promise for a wide array of applications. ALMA is not\nonly adaptable to more generalized LASSO problems but is also robust to\naccommodate other forms of regularization beyond total variation. Moreover, it\nextends effectively to handle non-Cartesian sampling trajectories, broadening\nits utility in complex data reconstruction scenarios. More generally, ALMA\nprovides a powerful tool for numerically solving constrained optimization\nproblems across various disciplines, offering a versatile and impactful\nsolution for advanced computational challenges.\n","authors":["Gianluca Giacchi","Isidoros Iakovidis","Bastien Milani","Matthias Stuber","Micah Murray","Benedetta Franceschiello"],"pdf_url":"https://arxiv.org/pdf/2406.19239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04723v2","updated":"2024-06-27T12:46:42Z","published":"2024-06-07T08:07:20Z","title":"A Deep Automotive Radar Detector using the RaDelft Dataset","summary":"  The detection of multiple extended targets in complex environments using\nhigh-resolution automotive radar is considered. A data-driven approach is\nproposed where unlabeled synchronized lidar data is used as ground truth to\ntrain a neural network with only radar data as input. To this end, the novel,\nlarge-scale, real-life, and multi-sensor RaDelft dataset has been recorded\nusing a demonstrator vehicle in different locations in the city of Delft. The\ndataset, as well as the documentation and example code, is publicly available\nfor those researchers in the field of automotive radar or machine perception.\nThe proposed data-driven detector is able to generate lidar-like point clouds\nusing only radar data from a high-resolution system, which preserves the shape\nand size of extended targets. The results are compared against conventional\nCFAR detectors as well as variations of the method to emulate the available\napproaches in the literature, using the probability of detection, the\nprobability of false alarm, and the Chamfer distance as performance metrics.\nMoreover, an ablation study was carried out to assess the impact of Doppler and\ntemporal information on detection performance. The proposed method outperforms\nthe different baselines in terms of Chamfer distance, achieving a reduction of\n75% against conventional CFAR detectors and 10% against the modified\nstate-of-the-art deep learning-based approaches.\n","authors":["Ignacio Roldan","Andras Palffy","Julian F. P. Kooij","Dariu M. Gavrila","Francesco Fioranelli","Alexander Yarovoy"],"pdf_url":"https://arxiv.org/pdf/2406.04723v2.pdf","comment":"Under review at IEEE Transaction on Radar Systems"},{"id":"http://arxiv.org/abs/2406.19081v1","updated":"2024-06-27T11:08:42Z","published":"2024-06-27T11:08:42Z","title":"Unsupervised Latent Stain Adaption for Digital Pathology","summary":"  In digital pathology, deep learning (DL) models for tasks such as\nsegmentation or tissue classification are known to suffer from domain shifts\ndue to different staining techniques. Stain adaptation aims to reduce the\ngeneralization error between different stains by training a model on source\nstains that generalizes to target stains. Despite the abundance of target stain\ndata, a key challenge is the lack of annotations. To address this, we propose a\njoint training between artificially labeled and unlabeled data including all\navailable stained images called Unsupervised Latent Stain Adaption (ULSA). Our\nmethod uses stain translation to enrich labeled source images with synthetic\ntarget images in order to increase supervised signals. Moreover, we leverage\nunlabeled target stain images using stain-invariant feature consistency\nlearning. With ULSA we present a semi-supervised strategy for efficient stain\nadaption without access to annotated target stain data. Remarkably, ULSA is\ntask agnostic in patch-level analysis for whole slide images (WSIs). Through\nextensive evaluation on external datasets, we demonstrate that ULSA achieves\nstate-of-the-art (SOTA) performance in kidney tissue segmentation and breast\ncancer classification across a spectrum of staining variations. Our findings\nsuggest that ULSA is an important framework towards stain adaption in digital\npathology.\n","authors":["Daniel Reisenbüchler","Lucas Luttner","Nadine S. Schaadt","Friedrich Feuerhake","Dorit Merhof"],"pdf_url":"https://arxiv.org/pdf/2406.19081v1.pdf","comment":"Accepted in MICCAI2024"},{"id":"http://arxiv.org/abs/2406.19043v1","updated":"2024-06-27T09:50:20Z","published":"2024-06-27T09:50:20Z","title":"CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting\n  Universal Machine Learning for Accelerated Cardiac MRI","summary":"  Cardiac magnetic resonance imaging (MRI) has emerged as a clinically\ngold-standard technique for diagnosing cardiac diseases, thanks to its ability\nto provide diverse information with multiple modalities and anatomical views.\nAccelerated cardiac MRI is highly expected to achieve time-efficient and\npatient-friendly imaging, and then advanced image reconstruction approaches are\nrequired to recover high-quality, clinically interpretable images from\nundersampled measurements. However, the lack of publicly available cardiac MRI\nk-space dataset in terms of both quantity and diversity has severely hindered\nsubstantial technological progress, particularly for data-driven artificial\nintelligence. Here, we provide a standardized, diverse, and high-quality\nCMRxRecon2024 dataset to facilitate the technical development, fair evaluation,\nand clinical transfer of cardiac MRI reconstruction approaches, towards\npromoting the universal frameworks that enable fast and robust reconstructions\nacross different cardiac MRI protocols in clinical practice. To the best of our\nknowledge, the CMRxRecon2024 dataset is the largest and most diverse publicly\navailable cardiac k-space dataset. It is acquired from 330 healthy volunteers,\ncovering commonly used modalities, anatomical views, and acquisition\ntrajectories in clinical cardiac MRI workflows. Besides, an open platform with\ntutorials, benchmarks, and data processing tools is provided to facilitate data\nusage, advanced method development, and fair performance evaluation.\n","authors":["Zi Wang","Fanwen Wang","Chen Qin","Jun Lyu","Ouyang Cheng","Shuo Wang","Yan Li","Mengyao Yu","Haoyu Zhang","Kunyuan Guo","Zhang Shi","Qirong Li","Ziqiang Xu","Yajing Zhang","Hao Li","Sha Hua","Binghua Chen","Longyu Sun","Mengting Sun","Qin Li","Ying-Hua Chu","Wenjia Bai","Jing Qin","Xiahai Zhuang","Claudia Prieto","Alistair Young","Michael Markl","He Wang","Lianming Wu","Guang Yang","Xiaobo Qu","Chengyan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19043v1.pdf","comment":"19 pages, 3 figures, 2 tables"},{"id":"http://arxiv.org/abs/2406.18361v2","updated":"2024-06-27T08:53:25Z","published":"2024-06-26T14:01:07Z","title":"Stable Diffusion Segmentation for Biomedical Images with Single-step\n  Reverse Process","summary":"  Diffusion models have demonstrated their effectiveness across various\ngenerative tasks. However, when applied to medical image segmentation, these\nmodels encounter several challenges, including significant resource and time\nrequirements. They also necessitate a multi-step reverse process and multiple\nsamples to produce reliable predictions. To address these challenges, we\nintroduce the first latent diffusion segmentation model, named SDSeg, built\nupon stable diffusion (SD). SDSeg incorporates a straightforward latent\nestimation strategy to facilitate a single-step reverse process and utilizes\nlatent fusion concatenation to remove the necessity for multiple samples.\nExtensive experiments indicate that SDSeg surpasses existing state-of-the-art\nmethods on five benchmark datasets featuring diverse imaging modalities.\nRemarkably, SDSeg is capable of generating stable predictions with a solitary\nreverse step and sample, epitomizing the model's stability as implied by its\nname. The code is available at\nhttps://github.com/lin-tianyu/Stable-Diffusion-Seg\n","authors":["Tianyu Lin","Zhiguang Chen","Zhonghao Yan","Weijiang Yu","Fudan Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.18361v2.pdf","comment":"Accepted at MICCAI 2024. Code and citation info see\n  https://github.com/lin-tianyu/Stable-Diffusion-Seg"},{"id":"http://arxiv.org/abs/2403.02311v3","updated":"2024-06-27T08:21:51Z","published":"2024-03-04T18:47:56Z","title":"Bayesian Uncertainty Estimation by Hamiltonian Monte Carlo: Applications\n  to Cardiac MRI Segmentation","summary":"  Deep learning (DL)-based methods have achieved state-of-the-art performance\nfor many medical image segmentation tasks. Nevertheless, recent studies show\nthat deep neural networks (DNNs) can be miscalibrated and overconfident,\nleading to \"silent failures\" that are risky for clinical applications. Bayesian\nDL provides an intuitive approach to DL failure detection, based on posterior\nprobability estimation. However, the posterior is intractable for large medical\nimage segmentation DNNs. To tackle this challenge, we propose a Bayesian\nlearning framework using Hamiltonian Monte Carlo (HMC), tempered by cold\nposterior (CP) to accommodate medical data augmentation, named HMC-CP. For HMC\ncomputation, we further propose a cyclical annealing strategy, capturing both\nlocal and global geometries of the posterior distribution, enabling highly\nefficient Bayesian DNN training with the same computational budget as training\na single DNN. The resulting Bayesian DNN outputs an ensemble segmentation along\nwith the segmentation uncertainty. We evaluate the proposed HMC-CP extensively\non cardiac magnetic resonance image (MRI) segmentation, using in-domain\nsteady-state free precession (SSFP) cine images as well as out-of-domain\ndatasets of quantitative T1 and T2 mapping. Our results show that the proposed\nmethod improves both segmentation accuracy and uncertainty estimation for in-\nand out-of-domain data, compared with well-established baseline methods such as\nMonte Carlo Dropout and Deep Ensembles. Additionally, we establish a conceptual\nlink between HMC and the commonly known stochastic gradient descent (SGD) and\nprovide general insight into the uncertainty of DL. This uncertainty is\nimplicitly encoded in the training dynamics but often overlooked. With reliable\nuncertainty estimation, our method provides a promising direction toward\ntrustworthy DL in clinical applications.\n","authors":["Yidong Zhao","Joao Tourais","Iain Pierce","Christian Nitsche","Thomas A. Treibel","Sebastian Weingärtner","Artur M. Schweidtmann","Qian Tao"],"pdf_url":"https://arxiv.org/pdf/2403.02311v3.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:011"},{"id":"http://arxiv.org/abs/2406.09327v2","updated":"2024-06-27T08:09:27Z","published":"2024-06-13T17:06:15Z","title":"Towards AI Lesion Tracking in PET/CT Imaging: A Siamese-based CNN\n  Pipeline applied on PSMA PET/CT Scans","summary":"  Assessing tumor response to systemic therapies is one of the main\napplications of PET/CT. Routinely, only a small subset of index lesions out of\nmultiple lesions is analyzed. However, this operator dependent selection may\nbias the results due to possible significant inter-metastatic heterogeneity of\nresponse to therapy. Automated, AI based approaches for lesion tracking hold\npromise in enabling the analysis of many more lesions and thus providing a\nbetter assessment of tumor response. This work introduces a Siamese CNN\napproach for lesion tracking between PET/CT scans. Our approach is applied on\nthe laborious task of tracking a high number of bone lesions in full-body\nbaseline and follow-up [68Ga]Ga- or [18F]F-PSMA PET/CT scans after two cycles\nof [177Lu]Lu-PSMA therapy of metastatic castration resistant prostate cancer\npatients. Data preparation includes lesion segmentation and affine\nregistration. Our algorithm extracts suitable lesion patches and forwards them\ninto a Siamese CNN trained to classify the lesion patch pairs as corresponding\nor non-corresponding lesions. Experiments have been performed with different\ninput patch types and a Siamese network in 2D and 3D. The CNN model\nsuccessfully learned to classify lesion assignments, reaching a lesion tracking\naccuracy of 83 % in its best configuration with an AUC = 0.91. For remaining\nlesions the pipeline accomplished a re-identification rate of 89 %. We proved\nthat a CNN may facilitate the tracking of multiple lesions in PSMA PET/CT\nscans. Future clinical studies are necessary if this improves the prediction of\nthe outcome of therapies.\n","authors":["Stefan P. Hein","Manuel Schultheiss","Andrei Gafita","Raphael Zaum","Farid Yagubbayli","Robert Tauber","Isabel Rauscher","Matthias Eiber","Franz Pfeiffer","Wolfgang A. Weber"],"pdf_url":"https://arxiv.org/pdf/2406.09327v2.pdf","comment":"25 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2401.06517v3","updated":"2024-06-27T07:50:25Z","published":"2024-01-12T11:33:20Z","title":"LiDAR Depth Map Guided Image Compression Model","summary":"  The incorporation of LiDAR technology into some high-end smartphones has\nunlocked numerous possibilities across various applications, including\nphotography, image restoration, augmented reality, and more. In this paper, we\nintroduce a novel direction that harnesses LiDAR depth maps to enhance the\ncompression of the corresponding RGB camera images. To the best of our\nknowledge, this represents the initial exploration in this particular research\ndirection. Specifically, we propose a Transformer-based learned image\ncompression system capable of achieving variable-rate compression using a\nsingle model while utilizing the LiDAR depth map as supplementary information\nfor both the encoding and decoding processes. Experimental results demonstrate\nthat integrating LiDAR yields an average PSNR gain of 0.83 dB and an average\nbitrate reduction of 16% as compared to its absence.\n","authors":["Alessandro Gnutti","Stefano Della Fiore","Mattia Savardi","Yi-Hsin Chen","Riccardo Leonardi","Wen-Hsiao Peng"],"pdf_url":"https://arxiv.org/pdf/2401.06517v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18950v1","updated":"2024-06-27T07:30:54Z","published":"2024-06-27T07:30:54Z","title":"MMR-Mamba: Multi-Contrast MRI Reconstruction with Mamba and\n  Spatial-Frequency Information Fusion","summary":"  Multi-contrast MRI acceleration has become prevalent in MR imaging, enabling\nthe reconstruction of high-quality MR images from under-sampled k-space data of\nthe target modality, using guidance from a fully-sampled auxiliary modality.\nThe main crux lies in efficiently and comprehensively integrating complementary\ninformation from the auxiliary modality. Existing methods either suffer from\nquadratic computational complexity or fail to capture long-range correlated\nfeatures comprehensively. In this work, we propose MMR-Mamba, a novel framework\nthat achieves comprehensive integration of multi-contrast features through\nMamba and spatial-frequency information fusion. Firstly, we design the\n\\textit{Target modality-guided Cross Mamba} (TCM) module in the spatial domain,\nwhich maximally restores the target modality information by selectively\nabsorbing useful information from the auxiliary modality. Secondly, leveraging\nglobal properties of the Fourier domain, we introduce the \\textit{Selective\nFrequency Fusion} (SFF) module to efficiently integrate global information in\nthe frequency domain and recover high-frequency signals for the reconstruction\nof structure details. Additionally, we present the \\textit{Adaptive\nSpatial-Frequency Fusion} (ASFF) module, which enhances fused features by\nsupplementing less informative features from one domain with corresponding\nfeatures from the other domain. These innovative strategies ensure efficient\nfeature fusion across spatial and frequency domains, avoiding the introduction\nof redundant information and facilitating the reconstruction of high-quality\ntarget images. Extensive experiments on the BraTS and fastMRI knee datasets\ndemonstrate the superiority of the proposed MMR-Mamba over state-of-the-art MRI\nreconstruction methods.\n","authors":["Jing Zou","Lanqing Liu","Qi Chen","Shujun Wang","Xiaohan Xing","Jing Qin"],"pdf_url":"https://arxiv.org/pdf/2406.18950v1.pdf","comment":"10 pages, 5 figure"},{"id":"http://arxiv.org/abs/2310.02792v2","updated":"2024-06-27T07:29:09Z","published":"2023-10-04T13:11:20Z","title":"Continuous 3D Myocardial Motion Tracking via Echocardiography","summary":"  Myocardial motion tracking stands as an essential clinical tool in the\nprevention and detection of cardiovascular diseases (CVDs), the foremost cause\nof death globally. However, current techniques suffer from incomplete and\ninaccurate motion estimation of the myocardium in both spatial and temporal\ndimensions, hindering the early identification of myocardial dysfunction. To\naddress these challenges, this paper introduces the Neural Cardiac Motion Field\n(NeuralCMF). NeuralCMF leverages implicit neural representation (INR) to model\nthe 3D structure and the comprehensive 6D forward/backward motion of the heart.\nThis method surpasses pixel-wise limitations by offering the capability to\ncontinuously query the precise shape and motion of the myocardium at any\nspecific point throughout the cardiac cycle, enhancing the detailed analysis of\ncardiac dynamics beyond traditional speckle tracking. Notably, NeuralCMF\noperates without the need for paired datasets, and its optimization is\nself-supervised through the physics knowledge priors in both space and time\ndimensions, ensuring compatibility with both 2D and 3D echocardiogram video\ninputs. Experimental validations across three representative datasets support\nthe robustness and innovative nature of the NeuralCMF, marking significant\nadvantages over existing state-of-the-art methods in cardiac imaging and motion\ntracking.\n","authors":["Chengkang Shen","Hao Zhu","You Zhou","Yu Liu","Si Yi","Lili Dong","Weipeng Zhao","David J. Brady","Xun Cao","Zhan Ma","Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2310.02792v2.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2406.18919v1","updated":"2024-06-27T06:22:02Z","published":"2024-06-27T06:22:02Z","title":"Classification of Carotid Plaque with Jellyfish Sign Through\n  Convolutional and Recurrent Neural Networks Utilizing Plaque Surface Edges","summary":"  In carotid arteries, plaque can develop as localized elevated lesions. The\nJellyfish sign, marked by fluctuating plaque surfaces with blood flow\npulsation, is a dynamic characteristic of these plaques that has recently\nattracted attention. Detecting this sign is vital, as it is often associated\nwith cerebral infarction. This paper proposes an ultrasound video-based\nclassification method for the Jellyfish sign, using deep neural networks. The\nproposed method first preprocesses carotid ultrasound videos to separate the\nmovement of the vascular wall from plaque movements. These preprocessed videos\nare then combined with plaque surface information and fed into a deep learning\nmodel comprising convolutional and recurrent neural networks, enabling the\nefficient classification of the Jellyfish sign. The proposed method was\nverified using ultrasound video images from 200 patients. Ablation studies\ndemonstrated the effectiveness of each component of the proposed method.\n","authors":["Takeshi Yoshidomi","Shinji Kume","Hiroaki Aizawa","Akira Furui"],"pdf_url":"https://arxiv.org/pdf/2406.18919v1.pdf","comment":"4 pages, 3 figures, accepted at IEEE EMBC 2024"},{"id":"http://arxiv.org/abs/2312.11580v2","updated":"2024-06-27T02:25:31Z","published":"2023-12-18T10:55:11Z","title":"PlaNet-S: Automatic Semantic Segmentation of Placenta","summary":"  [Purpose] To develop a fully automated semantic placenta segmentation model\nthat integrates the U-Net and SegNeXt architectures through ensemble learning.\n[Methods] A total of 218 pregnant women with suspected placental anomalies who\nunderwent magnetic resonance imaging (MRI) were enrolled, yielding 1090\nannotated images for developing a deep learning model for placental\nsegmentation. The images were standardized and divided into training and test\nsets. The performance of PlaNet-S, which integrates U-Net and SegNeXt within an\nensemble framework, was assessed using Intersection over Union (IoU) and\ncounting connected components (CCC) against the U-Net model. [Results] PlaNet-S\nhad significantly higher IoU (0.73 +/- 0.13) than that of U-Net (0.78 +/-\n0.010) (p<0.01). The CCC for PlaNet-S was significantly higher than that for\nU-Net (p<0.01), matching the ground truth in 86.0\\% and 56.7\\% of the cases,\nrespectively. [Conclusion]PlaNet-S performed better than the traditional U-Net\nin placental segmentation tasks. This model addresses the challenges of\ntime-consuming physician-assisted manual segmentation and offers the potential\nfor diverse applications in placental imaging analyses.\n","authors":["Shinnosuke Yamamoto","Isso Saito","Eichi Takaya","Ayaka Harigai","Tomomi Sato","Tomoya Kobayashi","Kei Takase","Takuya Ueda"],"pdf_url":"https://arxiv.org/pdf/2312.11580v2.pdf","comment":"11 pages, 5 figures, Shinnosuke Yamamoto and Isso Saito equally\n  contributed to this work. In the original submission, there was a\n  typographical error in the reported standard deviation for the Intersection\n  over Union (IoU) values of the PlaNet-S model. The standard deviation was\n  incorrectly listed as 0.01 instead of the correct value of 0.1. This has been\n  corrected in the revised version"},{"id":"http://arxiv.org/abs/2406.18840v1","updated":"2024-06-27T02:19:47Z","published":"2024-06-27T02:19:47Z","title":"Shorter SPECT Scans Using Self-supervised Coordinate Learning to\n  Synthesize Skipped Projection Views","summary":"  Purpose: This study addresses the challenge of extended SPECT imaging\nduration under low-count conditions, as encountered in Lu-177 SPECT imaging, by\ndeveloping a self-supervised learning approach to synthesize skipped SPECT\nprojection views, thus shortening scan times in clinical settings. Methods: We\nemployed a self-supervised coordinate-based learning technique, adapting the\nneural radiance field (NeRF) concept in computer vision to synthesize\nunder-sampled SPECT projection views. For each single scan, we used\nself-supervised coordinate learning to estimate skipped SPECT projection views.\nThe method was tested with various down-sampling factors (DFs=2, 4, 8) on both\nLu-177 phantom SPECT/CT measurements and clinical SPECT/CT datasets, from 11\npatients undergoing Lu-177 DOTATATE and 6 patients undergoing Lu-177 PSMA-617\nradiopharmaceutical therapy. Results: For SPECT reconstructions, our method\noutperformed the use of linearly interpolated projections and partial\nprojection views in relative contrast-to-noise-ratios (RCNR) averaged across\ndifferent downsampling factors: 1) DOTATATE: 83% vs. 65% vs. 67% for lesions\nand 86% vs. 70% vs. 67% for kidney, 2) PSMA: 76% vs. 69% vs. 68% for lesions\nand 75% vs. 55% vs. 66% for organs, including kidneys, lacrimal glands, parotid\nglands, and submandibular glands. Conclusion: The proposed method enables\nreduction in acquisition time (by factors of 2, 4, or 8) while maintaining\nquantitative accuracy in clinical SPECT protocols by allowing for the\ncollection of fewer projections. Importantly, the self-supervised nature of\nthis NeRF-based approach eliminates the need for extensive training data,\ninstead learning from each patient's projection data alone. The reduction in\nacquisition time is particularly relevant for imaging under low-count\nconditions and for protocols that require multiple-bed positions such as\nwhole-body imaging.\n","authors":["Zongyu Li","Yixuan Jia","Xiaojian Xu","Jason Hu","Jeffrey A. Fessler","Yuni K. Dewaraja"],"pdf_url":"https://arxiv.org/pdf/2406.18840v1.pdf","comment":"25 pages, 5568 words"},{"id":"http://arxiv.org/abs/2308.16676v2","updated":"2024-06-27T02:12:46Z","published":"2023-08-31T12:28:09Z","title":"Twofold Structured Features-Based Siamese Network for Infrared Target\n  Tracking","summary":"  Nowadays, infrared target tracking has been a critical technology in the\nfield of computer vision and has many applications, such as motion analysis,\npedestrian surveillance, intelligent detection, and so forth. Unfortunately,\ndue to the lack of color, texture and other detailed information, tracking\ndrift often occurs when the tracker encounters infrared targets that vary in\nsize or shape. To address this issue, we present a twofold structured\nfeatures-based Siamese network for infrared target tracking. First of all, in\norder to improve the discriminative capacity for infrared targets, a novel\nfeature fusion network is proposed to fuse both shallow spatial information and\ndeep semantic information into the extracted features in a comprehensive\nmanner. Then, a multi-template update module based on template update mechanism\nis designed to effectively deal with interferences from target appearance\nchanges which are prone to cause early tracking failures. Finally, both\nqualitative and quantitative experiments are carried out on VOT-TIR 2016\ndataset, which demonstrates that our method achieves the balance of promising\ntracking performance and real-time tracking speed against other out-of-the-art\ntrackers.\n","authors":["Wei-Jie Yan","Yun-Kai Xu","Qian Chen","Xiao-Fang Kong","Guo-Hua Gu","A-Jun Shao","Min-Jie Wan"],"pdf_url":"https://arxiv.org/pdf/2308.16676v2.pdf","comment":"13 pages,9 figures,references added"},{"id":"http://arxiv.org/abs/2311.12070v2","updated":"2024-06-27T00:45:18Z","published":"2023-11-19T19:44:44Z","title":"FDDM: Unsupervised Medical Image Translation with a Frequency-Decoupled\n  Diffusion Model","summary":"  Diffusion models have demonstrated significant potential in producing\nhigh-quality images in medical image translation to aid disease diagnosis,\nlocalization, and treatment. Nevertheless, current diffusion models have\nlimited success in achieving faithful image translations that can accurately\npreserve the anatomical structures of medical images, especially for unpaired\ndatasets. The preservation of structural and anatomical details is essential to\nreliable medical diagnosis and treatment planning, as structural mismatches can\nlead to disease misidentification and treatment errors. In this study, we\nintroduce the Frequency Decoupled Diffusion Model (FDDM) for MR-to-CT\nconversion. FDDM first obtains the anatomical information of the CT image from\nthe MR image through an initial conversion module. This anatomical information\nthen guides a subsequent diffusion model to generate high-quality CT images.\nOur diffusion model uses a dual-path reverse diffusion process for\nlow-frequency and high-frequency information, achieving a better balance\nbetween image quality and anatomical accuracy. We extensively evaluated FDDM\nusing public datasets for brain MR-to-CT and pelvis MR-to-CT translations,\ndemonstrating its superior performance to other GAN-based, VAE-based, and\ndiffusion-based models. The evaluation metrics included Frechet Inception\nDistance (FID), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity\nIndex Measure (SSIM). FDDM achieved the best scores on all metrics for both\ndatasets, particularly excelling in FID, with scores of 25.9 for brain data and\n29.2 for pelvis data, significantly outperforming other methods. These results\ndemonstrate that FDDM can generate high-quality target domain images while\nmaintaining the accuracy of translated anatomical structures.\n","authors":["Yunxiang Li","Hua-Chieh Shao","Xiaoxue Qian","You Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.12070v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2406.19578v1","updated":"2024-06-27T23:43:36Z","published":"2024-06-27T23:43:36Z","title":"PathAlign: A vision-language model for whole slide images in\n  histopathology","summary":"  Microscopic interpretation of histopathology images underlies many important\ndiagnostic and treatment decisions. While advances in vision-language modeling\nraise new opportunities for analysis of such images, the gigapixel-scale size\nof whole slide images (WSIs) introduces unique challenges. Additionally,\npathology reports simultaneously highlight key findings from small regions\nwhile also aggregating interpretation across multiple slides, often making it\ndifficult to create robust image-text pairs. As such, pathology reports remain\na largely untapped source of supervision in computational pathology, with most\nefforts relying on region-of-interest annotations or self-supervision at the\npatch-level. In this work, we develop a vision-language model based on the\nBLIP-2 framework using WSIs paired with curated text from pathology reports.\nThis enables applications utilizing a shared image-text embedding space, such\nas text or image retrieval for finding cases of interest, as well as\nintegration of the WSI encoder with a frozen large language model (LLM) for\nWSI-based generative text capabilities such as report generation or\nAI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000\nWSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure\ntypes, and tissue types. We present pathologist evaluation of text generation\nand text retrieval using WSI embeddings, as well as results for WSI\nclassification and workflow prioritization (slide-level triaging).\nModel-generated text for WSIs was rated by pathologists as accurate, without\nclinically significant error or omission, for 78% of WSIs on average. This work\ndemonstrates exciting potential capabilities for language-aligned WSI\nembeddings.\n","authors":["Faruk Ahmed","Andrew Sellergren","Lin Yang","Shawn Xu","Boris Babenko","Abbi Ward","Niels Olson","Arash Mohtashamian","Yossi Matias","Greg S. Corrado","Quang Duong","Dale R. Webster","Shravya Shetty","Daniel Golden","Yun Liu","David F. Steiner","Ellery Wulczyn"],"pdf_url":"https://arxiv.org/pdf/2406.19578v1.pdf","comment":"9 main pages and 19 pages of supplemental material; 3 main tables, 3\n  main figures and 11 supplemental tables, 7 supplemental figures"},{"id":"http://arxiv.org/abs/2404.11819v2","updated":"2024-06-27T23:16:58Z","published":"2024-04-18T00:41:32Z","title":"Utilizing Adversarial Examples for Bias Mitigation and Accuracy\n  Enhancement","summary":"  We propose a novel approach to mitigate biases in computer vision models by\nutilizing counterfactual generation and fine-tuning. While counterfactuals have\nbeen used to analyze and address biases in DNN models, the counterfactuals\nthemselves are often generated from biased generative models, which can\nintroduce additional biases or spurious correlations. To address this issue, we\npropose using adversarial images, that is images that deceive a deep neural\nnetwork but not humans, as counterfactuals for fair model training. Our\napproach leverages a curriculum learning framework combined with a fine-grained\nadversarial loss to fine-tune the model using adversarial examples. By\nincorporating adversarial images into the training data, we aim to prevent\nbiases from propagating through the pipeline. We validate our approach through\nboth qualitative and quantitative assessments, demonstrating improved bias\nmitigation and accuracy compared to existing methods. Qualitatively, our\nresults indicate that post-training, the decisions made by the model are less\ndependent on the sensitive attribute and our model better disentangles the\nrelationship between sensitive attributes and classification variables.\n","authors":["Pushkar Shukla","Dhruv Srikanth","Lee Cohen","Matthew Turk"],"pdf_url":"https://arxiv.org/pdf/2404.11819v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19568v1","updated":"2024-06-27T23:03:58Z","published":"2024-06-27T23:03:58Z","title":"What Matters in Detecting AI-Generated Videos like Sora?","summary":"  Recent advancements in diffusion-based video generation have showcased\nremarkable results, yet the gap between synthetic and real-world videos remains\nunder-explored. In this study, we examine this gap from three fundamental\nperspectives: appearance, motion, and geometry, comparing real-world videos\nwith those generated by a state-of-the-art AI model, Stable Video Diffusion. To\nachieve this, we train three classifiers using 3D convolutional networks, each\ntargeting distinct aspects: vision foundation model features for appearance,\noptical flow for motion, and monocular depth for geometry. Each classifier\nexhibits strong performance in fake video detection, both qualitatively and\nquantitatively. This indicates that AI-generated videos are still easily\ndetectable, and a significant gap between real and fake videos persists.\nFurthermore, utilizing the Grad-CAM, we pinpoint systematic failures of\nAI-generated videos in appearance, motion, and geometry. Finally, we propose an\nEnsemble-of-Experts model that integrates appearance, optical flow, and depth\ninformation for fake video detection, resulting in enhanced robustness and\ngeneralization ability. Our model is capable of detecting videos generated by\nSora with high accuracy, even without exposure to any Sora videos during\ntraining. This suggests that the gap between real and fake videos can be\ngeneralized across various video generative models. Project page:\nhttps://justin-crchang.github.io/3DCNNDetection.github.io/\n","authors":["Chirui Chang","Zhengzhe Liu","Xiaoyang Lyu","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2406.19568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19560v1","updated":"2024-06-27T22:19:19Z","published":"2024-06-27T22:19:19Z","title":"Cost-efficient Active Illumination Camera For Hyper-spectral\n  Reconstruction","summary":"  Hyper-spectral imaging has recently gained increasing attention for use in\ndifferent applications, including agricultural investigation, ground tracking,\nremote sensing and many other. However, the high cost, large physical size and\ncomplicated operation process stop hyperspectral cameras from being employed\nfor various applications and research fields. In this paper, we introduce a\ncost-efficient, compact and easy to use active illumination camera that may\nbenefit many applications. We developed a fully functional prototype of such\ncamera. With the hope of helping with agricultural research, we tested our\ncamera for plant root imaging. In addition, a U-Net model for spectral\nreconstruction was trained by using a reference hyperspectral camera's data as\nground truth and our camera's data as input. We demonstrated our camera's\nability to obtain additional information over a typical RGB camera. In\naddition, the ability to reconstruct hyperspectral data from multi-spectral\ninput makes our device compatible to models and algorithms developed for\nhyperspectral applications with no modifications required.\n","authors":["Yuxuan Zhang","T. M. Sazzad","Yangyang Song","Spencer J. Chang","Ritesh Chowdhry","Tomas Mejia","Anna Hampton","Shelby Kucharski","Stefan Gerber","Barry Tillman","Marcio F. R. Resende","William M. Hammond","Chris H. Wilson","Alina Zare","Sanjeev J. Koppal"],"pdf_url":"https://arxiv.org/pdf/2406.19560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19557v1","updated":"2024-06-27T22:17:49Z","published":"2024-06-27T22:17:49Z","title":"Robustness Testing of Black-Box Models Against CT Degradation Through\n  Test-Time Augmentation","summary":"  Deep learning models for medical image segmentation and object detection are\nbecoming increasingly available as clinical products. However, as details are\nrarely provided about the training data, models may unexpectedly fail when\ncases differ from those in the training distribution. An approach allowing\npotential users to independently test the robustness of a model, treating it as\na black box and using only a few cases from their own site, is key for\nadoption. To address this, a method to test the robustness of these models\nagainst CT image quality variation is presented. In this work we present this\nframework by demonstrating that given the same training data, the model\narchitecture and data pre processing greatly affect the robustness of several\nfrequently used segmentation and object detection methods to simulated CT\nimaging artifacts and degradation. Our framework also addresses the concern\nabout the sustainability of deep learning models in clinical use, by\nconsidering future shifts in image quality due to scanner deterioration or\nimaging protocol changes which are not reflected in a limited local test\ndataset.\n","authors":["Jack Highton","Quok Zong Chong","Samuel Finestone","Arian Beqiri","Julia A. Schnabel","Kanwal K. Bhatia"],"pdf_url":"https://arxiv.org/pdf/2406.19557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19556v1","updated":"2024-06-27T22:16:53Z","published":"2024-06-27T22:16:53Z","title":"BOrg: A Brain Organoid-Based Mitosis Dataset for Automatic Analysis of\n  Brain Diseases","summary":"  Recent advances have enabled the study of human brain development using brain\norganoids derived from stem cells. Quantifying cellular processes like mitosis\nin these organoids offers insights into neurodevelopmental disorders, but the\nmanual analysis is time-consuming, and existing datasets lack specific details\nfor brain organoid studies. We introduce BOrg, a dataset designed to study\nmitotic events in the embryonic development of the brain using confocal\nmicroscopy images of brain organoids. BOrg utilizes an efficient annotation\npipeline with sparse point annotations and techniques that minimize expert\neffort, overcoming limitations of standard deep learning approaches on sparse\ndata. We adapt and benchmark state-of-the-art object detection and cell\ncounting models on BOrg for detecting and analyzing mitotic cells across\nprophase, metaphase, anaphase, and telophase stages. Our results demonstrate\nthese adapted models significantly improve mitosis analysis efficiency and\naccuracy for brain organoid research compared to existing methods. BOrg\nfacilitates the development of automated tools to quantify statistics like\nmitosis rates, aiding mechanistic studies of neurodevelopmental processes and\ndisorders. Data and code are available at https://github.com/awaisrauf/borg.\n","authors":["Muhammad Awais","Mehaboobathunnisa Sahul Hameed","Bidisha Bhattacharya","Orly Reiner","Rao Muhammad Anwer"],"pdf_url":"https://arxiv.org/pdf/2406.19556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19540v1","updated":"2024-06-27T21:34:51Z","published":"2024-06-27T21:34:51Z","title":"Weighted Circle Fusion: Ensembling Circle Representation from Different\n  Object Detection Results","summary":"  Recently, the use of circle representation has emerged as a method to improve\nthe identification of spherical objects (such as glomeruli, cells, and nuclei)\nin medical imaging studies. In traditional bounding box-based object detection,\ncombining results from multiple models improves accuracy, especially when\nreal-time processing isn't crucial. Unfortunately, this widely adopted strategy\nis not readily available for combining circle representations. In this paper,\nwe propose Weighted Circle Fusion (WCF), a simple approach for merging\npredictions from various circle detection models. Our method leverages\nconfidence scores associated with each proposed bounding circle to generate\naveraged circles. Our method undergoes thorough evaluation on a proprietary\ndataset for glomerular detection in object detection within whole slide imaging\n(WSI). The findings reveal a performance gain of 5 %, respectively, compared to\nexisting ensemble methods. Furthermore, the Weighted Circle Fusion technique\nnot only improves the precision of object detection in medical images but also\nnotably decreases false detections, presenting a promising direction for future\nresearch and application in pathological image analysis.\n","authors":["Jialin Yue","Tianyuan Yao","Ruining Deng","Quan Liu","Juming Xiong","Haichun Yang","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2406.19540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19520v1","updated":"2024-06-27T20:41:49Z","published":"2024-06-27T20:41:49Z","title":"Comparative Analysis Of Color Models For Human Perception And Visual\n  Color Difference","summary":"  Color is integral to human experience, influencing emotions, decisions, and\nperceptions. This paper presents a comparative analysis of various color\nmodels' alignment with human visual perception. The study evaluates color\nmodels such as RGB, HSV, HSL, XYZ, CIELAB, and CIELUV to assess their\neffectiveness in accurately representing how humans perceive color. We evaluate\neach model based on its ability to accurately reflect visual color differences\nand dominant palette extraction compatible with the human eye. In image\nprocessing, accurate assessment of color difference is essential for\napplications ranging from digital design to quality control. Current color\ndifference metrics do not always match how people see colors, causing issues in\naccurately judging subtle differences. Understanding how different color models\nalign with human visual perception is crucial for various applications in image\nprocessing, digital media, and design.\n","authors":["Aruzhan Burambekova","Pakizar Shamoi"],"pdf_url":"https://arxiv.org/pdf/2406.19520v1.pdf","comment":"The paper has been submitted to EJMCA journal for consideration.\n  Current version is a preprint"},{"id":"http://arxiv.org/abs/2406.15727v2","updated":"2024-06-27T20:13:34Z","published":"2024-06-22T04:32:50Z","title":"Semi-supervised variational autoencoder for cell feature extraction in\n  multiplexed immunofluorescence images","summary":"  Advancements in digital imaging technologies have sparked increased interest\nin using multiplexed immunofluorescence (mIF) images to visualise and identify\nthe interactions between specific immunophenotypes with the tumour\nmicroenvironment at the cellular level. Current state-of-the-art multiplexed\nimmunofluorescence image analysis pipelines depend on cell feature\nrepresentations characterised by morphological and stain intensity-based\nmetrics generated using simple statistical and machine learning-based tools.\nHowever, these methods are not capable of generating complex representations of\ncells. We propose a deep learning-based cell feature extraction model using a\nvariational autoencoder with supervision using a latent subspace to extract\ncell features in mIF images. We perform cell phenotype classification using a\ncohort of more than 44,000 multiplexed immunofluorescence cell image patches\nextracted across 1,093 tissue microarray cores of breast cancer patients, to\ndemonstrate the success of our model against current and alternative methods.\n","authors":["Piumi Sandarenu","Julia Chen","Iveta Slapetova","Lois Browne","Peter H. Graham","Alexander Swarbrick","Ewan K. A. Millar","Yang Song","Erik Meijering"],"pdf_url":"https://arxiv.org/pdf/2406.15727v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10916v2","updated":"2024-06-27T19:43:52Z","published":"2024-03-16T12:44:08Z","title":"FishNet: Deep Neural Networks for Low-Cost Fish Stock Estimation","summary":"  Fish stock assessment often involves manual fish counting by taxonomy\nspecialists, which is both time-consuming and costly. We propose FishNet, an\nautomated computer vision system for both taxonomic classification and fish\nsize estimation from images captured with a low-cost digital camera. The system\nfirst performs object detection and segmentation using a Mask R-CNN to identify\nindividual fish from images containing multiple fish, possibly consisting of\ndifferent species. Then each fish species is classified and the length is\npredicted using separate machine learning models. To develop the model, we use\na dataset of 300,000 hand-labeled images containing 1.2M fish of 163 different\nspecies and ranging in length from 10cm to 250cm, with additional annotations\nand quality control methods used to curate high-quality training data. On\nheld-out test data sets, our system achieves a 92% intersection over union on\nthe fish segmentation task, a 89% top-1 classification accuracy on single fish\nspecies classification, and a 2.3cm mean absolute error on the fish length\nestimation task.\n","authors":["Moseli Mots'oehli","Anton Nikolaev","Wawan B. IGede","John Lynham","Peter J. Mous","Peter Sadowski"],"pdf_url":"https://arxiv.org/pdf/2403.10916v2.pdf","comment":"IEEE COINS 2024"},{"id":"http://arxiv.org/abs/2406.19498v1","updated":"2024-06-27T19:27:37Z","published":"2024-06-27T19:27:37Z","title":"Stereo Vision Based Robot for Remote Monitoring with VR Support","summary":"  The machine vision systems have been playing a significant role in visual\nmonitoring systems. With the help of stereovision and machine learning, it will\nbe able to mimic human-like visual system and behaviour towards the\nenvironment. In this paper, we present a stereo vision based 3-DOF robot which\nwill be used to monitor places from remote using cloud server and internet\ndevices. The 3-DOF robot will transmit human-like head movements, i.e., yaw,\npitch, roll and produce 3D stereoscopic video and stream it in Real-time. This\nvideo stream is sent to the user through any generic internet devices with VR\nbox support, i.e., smartphones giving the user a First-person real-time 3D\nexperience and transfers the head motion of the user to the robot also in\nReal-time. The robot will also be able to track moving objects and faces as a\ntarget using deep neural networks which enables it to be a standalone\nmonitoring robot. The user will be able to choose specific subjects to monitor\nin a space. The stereovision enables us to track the depth information of\ndifferent objects detected and will be used to track human interest objects\nwith its distances and sent to the cloud. A full working prototype is developed\nwhich showcases the capabilities of a monitoring system based on stereo vision,\nrobotics, and machine learning.\n","authors":["Mohamed Fazil M. S.","Arockia Selvakumar A.","Daniel Schilberg"],"pdf_url":"https://arxiv.org/pdf/2406.19498v1.pdf","comment":"6 Pages, 10 Figures"},{"id":"http://arxiv.org/abs/2406.19492v1","updated":"2024-06-27T19:16:57Z","published":"2024-06-27T19:16:57Z","title":"High-resolution segmentations of the hypothalamus and its subregions for\n  training of segmentation models","summary":"  Segmentation of brain structures on magnetic resonance imaging (MRI) is a\nhighly relevant neuroimaging topic, as it is a prerequisite for different\nanalyses such as volumetry or shape analysis. Automated segmentation\nfacilitates the study of brain structures in larger cohorts when compared with\nmanual segmentation, which is time-consuming. However, the development of most\nautomated methods relies on large and manually annotated datasets, which limits\nthe generalizability of these methods. Recently, new techniques using synthetic\nimages have emerged, reducing the need for manual annotation. Here we provide\nHELM, Hypothalamic ex vivo Label Maps, a dataset composed of label maps built\nfrom publicly available ultra-high resolution ex vivo MRI from 10 whole\nhemispheres, which can be used to develop segmentation methods using synthetic\ndata. The label maps are obtained with a combination of manual labels for the\nhypothalamic regions and automated segmentations for the rest of the brain, and\nmirrored to simulate entire brains. We also provide the pre-processed ex vivo\nscans, as this dataset can support future projects to include other structures\nafter these are manually segmented.\n","authors":["Livia Rodrigues","Martina Bocchetta","Oula Puonti","Douglas Greve","Ana Carolina Londe","Marcondes França","Simone Appenzeller","Leticia Rittner","Juan Eugenio Iglesias"],"pdf_url":"https://arxiv.org/pdf/2406.19492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16222v2","updated":"2024-06-27T19:02:24Z","published":"2024-04-24T21:49:59Z","title":"Step Differences in Instructional Video","summary":"  Comparing a user video to a reference how-to video is a key requirement for\nAR/VR technology delivering personalized assistance tailored to the user's\nprogress. However, current approaches for language-based assistance can only\nanswer questions about a single video. We propose an approach that first\nautomatically generates large amounts of visual instruction tuning data\ninvolving pairs of videos from HowTo100M by leveraging existing step\nannotations and accompanying narrations, and then trains a video-conditioned\nlanguage model to jointly reason across multiple raw videos. Our model achieves\nstate-of-the-art performance at identifying differences between video pairs and\nranking videos based on the severity of these differences, and shows promising\nability to perform general reasoning over multiple videos. Project page:\nhttps://github.com/facebookresearch/stepdiff\n","authors":["Tushar Nagarajan","Lorenzo Torresani"],"pdf_url":"https://arxiv.org/pdf/2404.16222v2.pdf","comment":"CVPR 2024"},{"id":"http://arxiv.org/abs/2406.19485v1","updated":"2024-06-27T18:58:41Z","published":"2024-06-27T18:58:41Z","title":"GAPNet: Granularity Attention Network with Anatomy-Prior-Constraint for\n  Carotid Artery Segmentation","summary":"  Atherosclerosis is a chronic, progressive disease that primarily affects the\narterial walls. It is one of the major causes of cardiovascular disease.\nMagnetic Resonance (MR) black-blood vessel wall imaging (BB-VWI) offers crucial\ninsights into vascular disease diagnosis by clearly visualizing vascular\nstructures. However, the complex anatomy of the neck poses challenges in\ndistinguishing the carotid artery (CA) from surrounding structures, especially\nwith changes like atherosclerosis. In order to address these issues, we propose\nGAPNet, which is a consisting of a novel geometric prior deduced from.\n","authors":["Lin Zhang","Chenggang Lu","Xin-yang Shi","Caifeng Shan","Jiong Zhang","Da Chen","Laurent D. Cohen"],"pdf_url":"https://arxiv.org/pdf/2406.19485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19464v1","updated":"2024-06-27T18:06:38Z","published":"2024-06-27T18:06:38Z","title":"ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data","summary":"  Audio signals provide rich information for the robot interaction and object\nproperties through contact. These information can surprisingly ease the\nlearning of contact-rich robot manipulation skills, especially when the visual\ninformation alone is ambiguous or incomplete. However, the usage of audio data\nin robot manipulation has been constrained to teleoperated demonstrations\ncollected by either attaching a microphone to the robot or object, which\nsignificantly limits its usage in robot learning pipelines. In this work, we\nintroduce ManiWAV: an 'ear-in-hand' data collection device to collect\nin-the-wild human demonstrations with synchronous audio and visual feedback,\nand a corresponding policy interface to learn robot manipulation policy\ndirectly from the demonstrations. We demonstrate the capabilities of our system\nthrough four contact-rich manipulation tasks that require either passively\nsensing the contact events and modes, or actively sensing the object surface\nmaterials and states. In addition, we show that our system can generalize to\nunseen in-the-wild environments, by learning from diverse in-the-wild human\ndemonstrations. Project website: https://mani-wav.github.io/\n","authors":["Zeyi Liu","Cheng Chi","Eric Cousineau","Naveen Kuppuswamy","Benjamin Burchfiel","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2406.19464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19461v1","updated":"2024-06-27T18:03:06Z","published":"2024-06-27T18:03:06Z","title":"Efficient and Distributed Large-Scale 3D Map Registration using\n  Tomographic Features","summary":"  A robust, resource-efficient, distributed, and minimally parameterized 3D map\nmatching and merging algorithm is proposed. The suggested algorithm utilizes\ntomographic features from 2D projections of horizontal cross-sections of\ngravity-aligned local maps, and matches these projection slices at all possible\nheight differences, enabling the estimation of four degrees of freedom in an\nefficient and parallelizable manner. The advocated algorithm improves\nstate-of-the-art feature extraction and registration pipelines by an order of\nmagnitude in memory use and execution time. Experimental studies are offered to\ninvestigate the efficiency of this 3D map merging scheme.\n","authors":["Halil Utku Unlu","Anthony Tzes","Prashanth Krishnamurthy","Farshad Khorrami"],"pdf_url":"https://arxiv.org/pdf/2406.19461v1.pdf","comment":"Submitted to Elsevier Journal: Robotics and Autonomous Systems (RAS)"},{"id":"http://arxiv.org/abs/2310.06389v3","updated":"2024-06-27T18:02:06Z","published":"2023-10-10T07:52:30Z","title":"Learning Stackable and Skippable LEGO Bricks for Efficient,\n  Reconfigurable, and Variable-Resolution Diffusion Modeling","summary":"  Diffusion models excel at generating photo-realistic images but come with\nsignificant computational costs in both training and sampling. While various\ntechniques address these computational challenges, a less-explored issue is\ndesigning an efficient and adaptable network backbone for iterative refinement.\nCurrent options like U-Net and Vision Transformer often rely on\nresource-intensive deep networks and lack the flexibility needed for generating\nimages at variable resolutions or with a smaller network than used in training.\nThis study introduces LEGO bricks, which seamlessly integrate Local-feature\nEnrichment and Global-content Orchestration. These bricks can be stacked to\ncreate a test-time reconfigurable diffusion backbone, allowing selective\nskipping of bricks to reduce sampling costs and generate higher-resolution\nimages than the training data. LEGO bricks enrich local regions with an MLP and\ntransform them using a Transformer block while maintaining a consistent\nfull-resolution image across all bricks. Experimental results demonstrate that\nLEGO bricks enhance training efficiency, expedite convergence, and facilitate\nvariable-resolution image generation while maintaining strong generative\nperformance. Moreover, LEGO significantly reduces sampling time compared to\nother methods, establishing it as a valuable enhancement for diffusion models.\nOur code and project page are available at\nhttps://jegzheng.github.io/LEGODiffusion.\n","authors":["Huangjie Zheng","Zhendong Wang","Jianbo Yuan","Guanghan Ning","Pengcheng He","Quanzeng You","Hongxia Yang","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2310.06389v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19395v1","updated":"2024-06-27T17:59:53Z","published":"2024-06-27T17:59:53Z","title":"Dataset Size Recovery from LoRA Weights","summary":"  Model inversion and membership inference attacks aim to reconstruct and\nverify the data which a model was trained on. However, they are not guaranteed\nto find all training samples as they do not know the size of the training set.\nIn this paper, we introduce a new task: dataset size recovery, that aims to\ndetermine the number of samples used to train a model, directly from its\nweights. We then propose DSiRe, a method for recovering the number of images\nused to fine-tune a model, in the common case where fine-tuning uses LoRA. We\ndiscover that both the norm and the spectrum of the LoRA matrices are closely\nlinked to the fine-tuning dataset size; we leverage this finding to propose a\nsimple yet effective prediction algorithm. To evaluate dataset size recovery of\nLoRA weights, we develop and release a new benchmark, LoRA-WiSE, consisting of\nover 25000 weight snapshots from more than 2000 diverse LoRA fine-tuned models.\nOur best classifier can predict the number of fine-tuning images with a mean\nabsolute error of 0.36 images, establishing the feasibility of this attack.\n","authors":["Mohammad Salama","Jonathan Kahana","Eliahu Horwitz","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2406.19395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19394v1","updated":"2024-06-27T17:59:49Z","published":"2024-06-27T17:59:49Z","title":"HUWSOD: Holistic Self-training for Unified Weakly Supervised Object\n  Detection","summary":"  Most WSOD methods rely on traditional object proposals to generate candidate\nregions and are confronted with unstable training, which easily gets stuck in a\npoor local optimum. In this paper, we introduce a unified, high-capacity weakly\nsupervised object detection (WSOD) network called HUWSOD, which utilizes a\ncomprehensive self-training framework without needing external modules or\nadditional supervision. HUWSOD innovatively incorporates a self-supervised\nproposal generator and an autoencoder proposal generator with a multi-rate\nresampling pyramid to replace traditional object proposals, enabling end-to-end\nWSOD training and inference. Additionally, we implement a holistic\nself-training scheme that refines detection scores and coordinates through\nstep-wise entropy minimization and consistency-constraint regularization,\nensuring consistent predictions across stochastic augmentations of the same\nimage. Extensive experiments on PASCAL VOC and MS COCO demonstrate that HUWSOD\ncompetes with state-of-the-art WSOD methods, eliminating the need for offline\nproposals and additional data. The peak performance of HUWSOD approaches that\nof fully-supervised Faster R-CNN. Our findings also indicate that randomly\ninitialized boxes, although significantly different from well-designed offline\nobject proposals, are effective for WSOD training.\n","authors":["Liujuan Cao","Jianghang Lin","Zebo Hong","Yunhang Shen","Shaohui Lin","Chao Chen","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2406.19394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19435v1","updated":"2024-06-27T17:59:49Z","published":"2024-06-27T17:59:49Z","title":"A Sanity Check for AI-generated Image Detection","summary":"  With the rapid development of generative models, discerning AI-generated\ncontent has evoked increasing attention from both industry and academia. In\nthis paper, we conduct a sanity check on \"whether the task of AI-generated\nimage detection has been solved\". To start with, we present Chameleon dataset,\nconsisting AIgenerated images that are genuinely challenging for human\nperception. To quantify the generalization of existing methods, we evaluate 9\noff-the-shelf AI-generated image detectors on Chameleon dataset. Upon analysis,\nalmost all models classify AI-generated images as real ones. Later, we propose\nAIDE (AI-generated Image DEtector with Hybrid Features), which leverages\nmultiple experts to simultaneously extract visual artifacts and noise patterns.\nSpecifically, to capture the high-level semantics, we utilize CLIP to compute\nthe visual embedding. This effectively enables the model to discern\nAI-generated images based on semantics or contextual information; Secondly, we\nselect the highest frequency patches and the lowest frequency patches in the\nimage, and compute the low-level patchwise features, aiming to detect\nAI-generated images by low-level artifacts, for example, noise pattern,\nanti-aliasing, etc. While evaluating on existing benchmarks, for example,\nAIGCDetectBenchmark and GenImage, AIDE achieves +3.5% and +4.6% improvements to\nstate-of-the-art methods, and on our proposed challenging Chameleon benchmarks,\nit also achieves the promising results, despite this problem for detecting\nAI-generated images is far from being solved. The dataset, codes, and pre-train\nmodels will be published at https://github.com/shilinyan99/AIDE.\n","authors":["Shilin Yan","Ouxiang Li","Jiayin Cai","Yanbin Hao","Xiaolong Jiang","Yao Hu","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2406.19435v1.pdf","comment":"Project page: https://shilinyan99.github.io/AIDE Code:\n  https://github.com/shilinyan99/AIDE"},{"id":"http://arxiv.org/abs/2406.19393v1","updated":"2024-06-27T17:59:46Z","published":"2024-06-27T17:59:46Z","title":"Looking 3D: Anomaly Detection with 2D-3D Alignment","summary":"  Automatic anomaly detection based on visual cues holds practical significance\nin various domains, such as manufacturing and product quality assessment. This\npaper introduces a new conditional anomaly detection problem, which involves\nidentifying anomalies in a query image by comparing it to a reference shape. To\naddress this challenge, we have created a large dataset, BrokenChairs-180K,\nconsisting of around 180K images, with diverse anomalies, geometries, and\ntextures paired with 8,143 reference 3D shapes. To tackle this task, we have\nproposed a novel transformer-based approach that explicitly learns the\ncorrespondence between the query image and reference 3D shape via feature\nalignment and leverages a customized attention mechanism for anomaly detection.\nOur approach has been rigorously evaluated through comprehensive experiments,\nserving as a benchmark for future research in this domain.\n","authors":["Ankan Bhunia","Changjian Li","Hakan Bilen"],"pdf_url":"https://arxiv.org/pdf/2406.19393v1.pdf","comment":"Accepted at CVPR'24. Codes & dataset available at\n  https://github.com/VICO-UoE/Looking3D"},{"id":"http://arxiv.org/abs/2406.19392v1","updated":"2024-06-27T17:59:45Z","published":"2024-06-27T17:59:45Z","title":"ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos","summary":"  We introduce ReXTime, a benchmark designed to rigorously test AI models'\nability to perform temporal reasoning within video events. Specifically,\nReXTime focuses on reasoning across time, i.e. human-like understanding when\nthe question and its corresponding answer occur in different video segments.\nThis form of reasoning, requiring advanced understanding of cause-and-effect\nrelationships across video segments, poses significant challenges to even the\nfrontier multimodal large language models. To facilitate this evaluation, we\ndevelop an automated pipeline for generating temporal reasoning question-answer\npairs, significantly reducing the need for labor-intensive manual annotations.\nOur benchmark includes 921 carefully vetted validation samples and 2,143 test\nsamples, each manually curated for accuracy and relevance. Evaluation results\nshow that while frontier large language models outperform academic models, they\nstill lag behind human performance by a significant 14.3% accuracy gap.\nAdditionally, our pipeline creates a training dataset of 9,695 machine\ngenerated samples without manual effort, which empirical studies suggest can\nenhance the across-time reasoning via fine-tuning.\n","authors":["Jr-Jen Chen","Yu-Chien Liao","Hsi-Che Lin","Yu-Chu Yu","Yen-Chun Chen","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19391v1","updated":"2024-06-27T17:59:40Z","published":"2024-06-27T17:59:40Z","title":"Fibottention: Inceptive Visual Representation Learning with Diverse\n  Attention Across Heads","summary":"  Visual perception tasks are predominantly solved by Vision Transformer (ViT)\narchitectures, which, despite their effectiveness, encounter a computational\nbottleneck due to the quadratic complexity of computing self-attention. This\ninefficiency is largely due to the self-attention heads capturing redundant\ntoken interactions, reflecting inherent redundancy within visual data. Many\nworks have aimed to reduce the computational complexity of self-attention in\nViTs, leading to the development of efficient and sparse transformer\narchitectures. In this paper, viewing through the efficiency lens, we realized\nthat introducing any sparse self-attention strategy in ViTs can keep the\ncomputational overhead low. However, these strategies are sub-optimal as they\noften fail to capture fine-grained visual details. This observation leads us to\npropose a general, efficient, sparse architecture, named Fibottention, for\napproximating self-attention with superlinear complexity that is built upon\nFibonacci sequences. The key strategies in Fibottention include: it excludes\nproximate tokens to reduce redundancy, employs structured sparsity by design to\ndecrease computational demands, and incorporates inception-like diversity\nacross attention heads. This diversity ensures the capture of complementary\ninformation through non-overlapping token interactions, optimizing both\nperformance and resource utilization in ViTs for visual representation\nlearning. We embed our Fibottention mechanism into multiple state-of-the-art\ntransformer architectures dedicated to visual tasks. Leveraging only 2-6% of\nthe elements in the self-attention heads, Fibottention in conjunction with ViT\nand its variants, consistently achieves significant performance boosts compared\nto standard ViTs in nine datasets across three domains $\\unicode{x2013}$ image\nclassification, video understanding, and robot learning tasks.\n","authors":["Ali Khaleghi Rahimian","Manish Kumar Govind","Subhajit Maity","Dominick Reilly","Christian Kümmerle","Srijan Das","Aritra Dutta"],"pdf_url":"https://arxiv.org/pdf/2406.19391v1.pdf","comment":"The code is publicly available at\n  https://github.com/Charlotte-CharMLab/Fibottention"},{"id":"http://arxiv.org/abs/2406.19390v1","updated":"2024-06-27T17:59:06Z","published":"2024-06-27T17:59:06Z","title":"SALVe: Semantic Alignment Verification for Floorplan Reconstruction from\n  Sparse Panoramas","summary":"  We propose a new system for automatic 2D floorplan reconstruction that is\nenabled by SALVe, our novel pairwise learned alignment verifier. The inputs to\nour system are sparsely located 360$^\\circ$ panoramas, whose semantic features\n(windows, doors, and openings) are inferred and used to hypothesize pairwise\nroom adjacency or overlap. SALVe initializes a pose graph, which is\nsubsequently optimized using GTSAM. Once the room poses are computed, room\nlayouts are inferred using HorizonNet, and the floorplan is constructed by\nstitching the most confident layout boundaries. We validate our system\nqualitatively and quantitatively as well as through ablation studies, showing\nthat it outperforms state-of-the-art SfM systems in completeness by over 200%,\nwithout sacrificing accuracy. Our results point to the significance of our\nwork: poses of 81% of panoramas are localized in the first 2 connected\ncomponents (CCs), and 89% in the first 3 CCs. Code and models are publicly\navailable at https://github.com/zillow/salve.\n","authors":["John Lambert","Yuguang Li","Ivaylo Boyadzhiev","Lambert Wixson","Manjunath Narayana","Will Hutchcroft","James Hays","Frank Dellaert","Sing Bing Kang"],"pdf_url":"https://arxiv.org/pdf/2406.19390v1.pdf","comment":"Accepted at ECCV 2022"},{"id":"http://arxiv.org/abs/2406.19389v1","updated":"2024-06-27T17:59:01Z","published":"2024-06-27T17:59:01Z","title":"OMG-LLaVA: Bridging Image-level, Object-level, Pixel-level Reasoning and\n  Understanding","summary":"  Current universal segmentation methods demonstrate strong capabilities in\npixel-level image and video understanding. However, they lack reasoning\nabilities and cannot be controlled via text instructions. In contrast, large\nvision-language multimodal models exhibit powerful vision-based conversation\nand reasoning capabilities but lack pixel-level understanding and have\ndifficulty accepting visual prompts for flexible user interaction. This paper\nproposes OMG-LLaVA, a new and elegant framework combining powerful pixel-level\nvision understanding with reasoning abilities. It can accept various visual and\ntext prompts for flexible user interaction. Specifically, we use a universal\nsegmentation method as the visual encoder, integrating image information,\nperception priors, and visual prompts into visual tokens provided to the LLM.\nThe LLM is responsible for understanding the user's text instructions and\nproviding text responses and pixel-level segmentation results based on the\nvisual information. We propose perception prior embedding to better integrate\nperception priors with image features. OMG-LLaVA achieves image-level,\nobject-level, and pixel-level reasoning and understanding in a single model,\nmatching or surpassing the performance of specialized methods on multiple\nbenchmarks. Rather than using LLM to connect each specialist, our work aims at\nend-to-end training on one encoder, one decoder, and one LLM. The code and\nmodel have been released for further research.\n","authors":["Tao Zhang","Xiangtai Li","Hao Fei","Haobo Yuan","Shengqiong Wu","Shunping Ji","Chen Change Loy","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2406.19389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19388v1","updated":"2024-06-27T17:58:54Z","published":"2024-06-27T17:58:54Z","title":"Taming Data and Transformers for Audio Generation","summary":"  Generating ambient sounds and effects is a challenging problem due to data\nscarcity and often insufficient caption quality, making it difficult to employ\nlarge-scale generative models for the task. In this work, we tackle the problem\nby introducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. We show that by leveraging metadata\navailable with the audio modality, we can substantially improve the quality of\ncaptions. AutoCap reaches CIDEr score of 83.2, marking a 3.2% improvement from\nthe best available captioning model at four times faster inference speed. We\nthen use AutoCap to caption clips from existing datasets, obtaining 761,000\naudio clips with high-quality captions, forming the largest available\naudio-text dataset. Second, we propose GenAu, a scalable transformer-based\naudio generation architecture that we scale up to 1.25B parameters and train\nwith our new dataset. When compared to state-of-the-art audio generators, GenAu\nobtains significant improvements of 15.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. This shows that the quality of data is often as\nimportant as its quantity. Besides, since AutoCap is fully automatic, new audio\nsamples can be added to the training dataset, unlocking the training of even\nlarger generative models for audio synthesis.\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Guha Balakrishnan","Sergey Tulyakov","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2406.19388v1.pdf","comment":"Project Webpage: https://snap-research.github.io/GenAU/"},{"id":"http://arxiv.org/abs/2406.19369v1","updated":"2024-06-27T17:49:25Z","published":"2024-06-27T17:49:25Z","title":"Mamba or RWKV: Exploring High-Quality and High-Efficiency Segment\n  Anything Model","summary":"  Transformer-based segmentation methods face the challenge of efficient\ninference when dealing with high-resolution images. Recently, several linear\nattention architectures, such as Mamba and RWKV, have attracted much attention\nas they can process long sequences efficiently. In this work, we focus on\ndesigning an efficient segment-anything model by exploring these different\narchitectures. Specifically, we design a mixed backbone that contains\nconvolution and RWKV operation, which achieves the best for both accuracy and\nefficiency. In addition, we design an efficient decoder to utilize the\nmultiscale tokens to obtain high-quality masks. We denote our method as\nRWKV-SAM, a simple, effective, fast baseline for SAM-like models. Moreover, we\nbuild a benchmark containing various high-quality segmentation datasets and\njointly train one efficient yet high-quality segmentation model using this\nbenchmark. Based on the benchmark results, our RWKV-SAM achieves outstanding\nperformance in efficiency and segmentation quality compared to transformers and\nother linear attention models. For example, compared with the same-scale\ntransformer model, RWKV-SAM achieves more than 2x speedup and can achieve\nbetter segmentation performance on various datasets. In addition, RWKV-SAM\noutperforms recent vision Mamba models with better classification and semantic\nsegmentation results. Code and models will be publicly available.\n","authors":["Haobo Yuan","Xiangtai Li","Lu Qi","Tao Zhang","Ming-Hsuan Yang","Shuicheng Yan","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2406.19369v1.pdf","comment":"16 pages; 8 figures"},{"id":"http://arxiv.org/abs/2406.19362v1","updated":"2024-06-27T17:43:35Z","published":"2024-06-27T17:43:35Z","title":"STAL3D: Unsupervised Domain Adaptation for 3D Object Detection via\n  Collaborating Self-Training and Adversarial Learning","summary":"  Existing 3D object detection suffers from expensive annotation costs and poor\ntransferability to unknown data due to the domain gap, Unsupervised Domain\nAdaptation (UDA) aims to generalize detection models trained in labeled source\ndomains to perform robustly on unexplored target domains, providing a promising\nsolution for cross-domain 3D object detection. Although Self-Training (ST)\nbased cross-domain 3D detection methods with the assistance of pseudo-labeling\ntechniques have achieved remarkable progress, they still face the issue of\nlow-quality pseudo-labels when there are significant domain disparities due to\nthe absence of a process for feature distribution alignment. While Adversarial\nLearning (AL) based methods can effectively align the feature distributions of\nthe source and target domains, the inability to obtain labels in the target\ndomain forces the adoption of asymmetric optimization losses, resulting in a\nchallenging issue of source domain bias. To overcome these limitations, we\npropose a novel unsupervised domain adaptation framework for 3D object\ndetection via collaborating ST and AL, dubbed as STAL3D, unleashing the\ncomplementary advantages of pseudo labels and feature distribution alignment.\nAdditionally, a Background Suppression Adversarial Learning (BS-AL) module and\na Scale Filtering Module (SFM) are designed tailored for 3D cross-domain\nscenes, effectively alleviating the issues of the large proportion of\nbackground interference and source domain size bias. Our STAL3D achieves\nstate-of-the-art performance on multiple cross-domain tasks and even surpasses\nthe Oracle results on Waymo $\\rightarrow$ KITTI and Waymo $\\rightarrow$\nKITTI-rain.\n","authors":["Yanan Zhang","Chao Zhou","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2406.19362v1.pdf","comment":"Accepted by IEEE-TIV"},{"id":"http://arxiv.org/abs/2406.05127v2","updated":"2024-06-27T17:35:45Z","published":"2024-06-07T17:55:43Z","title":"Towards Semantic Equivalence of Tokenization in Multimodal LLM","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated exceptional\ncapabilities in processing vision-language tasks. One of the crux of MLLMs lies\nin vision tokenization, which involves efficiently transforming input visual\nsignals into feature representations that are most beneficial for LLMs.\nHowever, existing vision tokenizers, essential for semantic alignment between\nvision and language, remain problematic. Existing methods aggressively fragment\nvisual input, corrupting the visual semantic integrity. To address this, this\npaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),\nwhich groups visual features into semantic units via a dynamic clustering\nalgorithm, flexibly determining the number of tokens based on image complexity.\nThe resulting vision tokens effectively preserve semantic integrity and capture\nboth low-frequency and high-frequency visual features. The proposed MLLM\n(Setokim) equipped with SeTok significantly demonstrates superior performance\nacross various tasks, as evidenced by our experimental results. The project\npage is at https://chocowu.github.io/SeTok-web/.\n","authors":["Shengqiong Wu","Hao Fei","Xiangtai Li","Jiayi Ji","Hanwang Zhang","Tat-Seng Chua","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2406.05127v2.pdf","comment":"Technical Report. The project page:\n  https://chocowu.github.io/SeTok-web/"},{"id":"http://arxiv.org/abs/2406.19353v1","updated":"2024-06-27T17:32:18Z","published":"2024-06-27T17:32:18Z","title":"CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative\n  Object REarrangement","summary":"  Understanding how humans cooperatively rearrange household objects is\ncritical for VR/AR and human-robot interaction. However, in-depth studies on\nmodeling these behaviors are under-researched due to the lack of relevant\ndatasets. We fill this gap by presenting CORE4D, a novel large-scale 4D\nhuman-object-human interaction dataset focusing on collaborative object\nrearrangement, which encompasses diverse compositions of various object\ngeometries, collaboration modes, and 3D scenes. With 1K human-object-human\nmotion sequences captured in the real world, we enrich CORE4D by contributing\nan iterative collaboration retargeting strategy to augment motions to a variety\nof novel objects. Leveraging this approach, CORE4D comprises a total of 11K\ncollaboration sequences spanning 3K real and virtual object shapes. Benefiting\nfrom extensive motion patterns provided by CORE4D, we benchmark two tasks\naiming at generating human-object interaction: human-object motion forecasting\nand interaction synthesis. Extensive experiments demonstrate the effectiveness\nof our collaboration retargeting strategy and indicate that CORE4D has posed\nnew challenges to existing human-object interaction generation methodologies.\nOur dataset and code are available at\nhttps://github.com/leolyliu/CORE4D-Instructions.\n","authors":["Chengwen Zhang","Yun Liu","Ruofan Xing","Bingda Tang","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2406.19353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13040v2","updated":"2024-06-27T17:27:13Z","published":"2024-03-19T17:35:17Z","title":"Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping","summary":"  Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify\ncolor Doppler in cardiac imaging. In this study, we propose novel alternatives\nto the traditional iVFM optimization scheme by utilizing physics-informed\nneural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.\nWhen evaluated on simulated color Doppler images derived from a\npatient-specific computational fluid dynamics model and in vivo Doppler\nacquisitions, both approaches demonstrate comparable reconstruction performance\nto the original iVFM algorithm. The efficiency of PINNs is boosted through\ndual-stage optimization and pre-optimized weights. On the other hand, the\nnnU-Net method excels in generalizability and real-time capabilities. Notably,\nnnU-Net shows superior robustness on sparse and truncated Doppler data while\nmaintaining independence from explicit boundary conditions. Overall, our\nresults highlight the effectiveness of these methods in reconstructing\nintraventricular vector blood flow. The study also suggests potential\napplications of PINNs in ultrafast color Doppler imaging and the incorporation\nof fluid dynamics equations to derive biomarkers for cardiovascular diseases\nbased on blood flow.\n","authors":["Hang Jung Ling","Salomé Bru","Julia Puig","Florian Vixège","Simon Mendez","Franck Nicoud","Pierre-Yves Courand","Olivier Bernard","Damien Garcia"],"pdf_url":"https://arxiv.org/pdf/2403.13040v2.pdf","comment":"12 pages, accepted for publication in IEEE TUFFC; camera ready\n  corrections, corrected acknowledgments"},{"id":"http://arxiv.org/abs/2406.19341v1","updated":"2024-06-27T17:16:23Z","published":"2024-06-27T17:16:23Z","title":"Learning Visual Conditioning Tokens to Correct Domain Shift for Fully\n  Test-time Adaptation","summary":"  Fully test-time adaptation aims to adapt the network model based on\nsequential analysis of input samples during the inference stage to address the\ncross-domain performance degradation problem of deep neural networks. This work\nis based on the following interesting finding: in transformer-based image\nclassification, the class token at the first transformer encoder layer can be\nlearned to capture the domain-specific characteristics of target samples during\ntest-time adaptation. This learned token, when combined with input image patch\nembeddings, is able to gradually remove the domain-specific information from\nthe feature representations of input samples during the transformer encoding\nprocess, thereby significantly improving the test-time adaptation performance\nof the source model across different domains. We refer to this class token as\nvisual conditioning token (VCT). To successfully learn the VCT, we propose a\nbi-level learning approach to capture the long-term variations of\ndomain-specific characteristics while accommodating local variations of\ninstance-specific characteristics. Experimental results on the benchmark\ndatasets demonstrate that our proposed bi-level visual conditioning token\nlearning method is able to achieve significantly improved test-time adaptation\nperformance by up to 1.9%.\n","authors":["Yushun Tang","Shuoshuo Chen","Zhehan Kan","Yi Zhang","Qinghai Guo","Zhihai He"],"pdf_url":"https://arxiv.org/pdf/2406.19341v1.pdf","comment":"accepted by TMM"},{"id":"http://arxiv.org/abs/2406.13444v2","updated":"2024-06-27T17:09:24Z","published":"2024-06-19T11:09:16Z","title":"VDebugger: Harnessing Execution Feedback for Debugging Visual Programs","summary":"  Visual programs are executable code generated by large language models to\naddress visual reasoning problems. They decompose complex questions into\nmultiple reasoning steps and invoke specialized models for each step to solve\nthe problems. However, these programs are prone to logic errors, with our\npreliminary evaluation showing that 58% of the total errors are caused by\nprogram logic errors. Debugging complex visual programs remains a major\nbottleneck for visual reasoning. To address this, we introduce VDebugger, a\nnovel critic-refiner framework trained to localize and debug visual programs by\ntracking execution step by step. VDebugger identifies and corrects program\nerrors leveraging detailed execution feedback, improving interpretability and\naccuracy. The training data is generated through an automated pipeline that\ninjects errors into correct visual programs using a novel mask-best decoding\ntechnique. Evaluations on six datasets demonstrate VDebugger's effectiveness,\nshowing performance improvements of up to 3.2% in downstream task accuracy.\nFurther studies show VDebugger's ability to generalize to unseen tasks,\nbringing a notable improvement of 2.3% on the unseen COVR task. Code, data and\nmodels are made publicly available at https://github.com/shirley-wu/vdebugger/\n","authors":["Xueqing Wu","Zongyu Lin","Songyan Zhao","Te-Lin Wu","Pan Lu","Nanyun Peng","Kai-Wei Chang"],"pdf_url":"https://arxiv.org/pdf/2406.13444v2.pdf","comment":"update reference"},{"id":"http://arxiv.org/abs/2406.19320v1","updated":"2024-06-27T16:54:12Z","published":"2024-06-27T16:54:12Z","title":"Efficient World Models with Context-Aware Tokenization","summary":"  Scaling up deep Reinforcement Learning (RL) methods presents a significant\nchallenge. Following developments in generative modelling, model-based RL\npositions itself as a strong contender. Recent advances in sequence modelling\nhave led to effective transformer-based world models, albeit at the price of\nheavy computations due to the long sequences of tokens required to accurately\nsimulate environments. In this work, we propose $\\Delta$-IRIS, a new agent with\na world model architecture composed of a discrete autoencoder that encodes\nstochastic deltas between time steps and an autoregressive transformer that\npredicts future deltas by summarizing the current state of the world with\ncontinuous tokens. In the Crafter benchmark, $\\Delta$-IRIS sets a new state of\nthe art at multiple frame budgets, while being an order of magnitude faster to\ntrain than previous attention-based approaches. We release our code and models\nat https://github.com/vmicheli/delta-iris.\n","authors":["Vincent Micheli","Eloi Alonso","François Fleuret"],"pdf_url":"https://arxiv.org/pdf/2406.19320v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2406.19316v1","updated":"2024-06-27T16:52:01Z","published":"2024-06-27T16:52:01Z","title":"Enhanced Data Transfer Cooperating with Artificial Triplets for Scene\n  Graph Generation","summary":"  This work focuses on training dataset enhancement of informative relational\ntriplets for Scene Graph Generation (SGG). Due to the lack of effective\nsupervision, the current SGG model predictions perform poorly for informative\nrelational triplets with inadequate training samples. Therefore, we propose two\nnovel training dataset enhancement modules: Feature Space Triplet Augmentation\n(FSTA) and Soft Transfer. FSTA leverages a feature generator trained to\ngenerate representations of an object in relational triplets. The biased\nprediction based sampling in FSTA efficiently augments artificial triplets\nfocusing on the challenging ones. In addition, we introduce Soft Transfer,\nwhich assigns soft predicate labels to general relational triplets to make more\nsupervisions for informative predicate classes effectively. Experimental\nresults show that integrating FSTA and Soft Transfer achieve high levels of\nboth Recall and mean Recall in Visual Genome dataset. The mean of Recall and\nmean Recall is the highest among all the existing model-agnostic methods.\n","authors":["KuanChao Chu","Satoshi Yamazaki","Hideki Nakayama"],"pdf_url":"https://arxiv.org/pdf/2406.19316v1.pdf","comment":"Accepted to IEICE Transactions on Information and Systems in April\n  2024"},{"id":"http://arxiv.org/abs/2406.13642v2","updated":"2024-06-27T16:30:48Z","published":"2024-06-19T15:41:30Z","title":"SpatialBot: Precise Spatial Understanding with Vision Language Models","summary":"  Vision Language Models (VLMs) have achieved impressive performance in 2D\nimage understanding, however they are still struggling with spatial\nunderstanding which is the foundation of Embodied AI. In this paper, we propose\nSpatialBot for better spatial understanding by feeding both RGB and depth\nimages. Additionally, we have constructed the SpatialQA dataset, which involves\nmulti-level depth-related questions to train VLMs for depth understanding.\nFinally, we present SpatialBench to comprehensively evaluate VLMs' capabilities\nin spatial understanding at different levels. Extensive experiments on our\nspatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks,\ndemonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The\nmodel, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.\n","authors":["Wenxiao Cai","Yaroslav Ponomarenko","Jianhao Yuan","Xiaoqi Li","Wankou Yang","Hao Dong","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.13642v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19302v1","updated":"2024-06-27T16:17:33Z","published":"2024-06-27T16:17:33Z","title":"Mapping Land Naturalness from Sentinel-2 using Deep Contextual and\n  Geographical Priors","summary":"  In recent decades, the causes and consequences of climate change have\naccelerated, affecting our planet on an unprecedented scale. This change is\nclosely tied to the ways in which humans alter their surroundings. As our\nactions continue to impact natural areas, using satellite images to observe and\nmeasure these effects has become crucial for understanding and combating\nclimate change. Aiming to map land naturalness on the continuum of modern human\npressure, we have developed a multi-modal supervised deep learning framework\nthat addresses the unique challenges of satellite data and the task at hand. We\nincorporate contextual and geographical priors, represented by corresponding\ncoordinate information and broader contextual information, including and\nsurrounding the immediate patch to be predicted. Our framework improves the\nmodel's predictive performance in mapping land naturalness from Sentinel-2\ndata, a type of multi-spectral optical satellite imagery. Recognizing that our\nprotective measures are only as effective as our understanding of the\necosystem, quantifying naturalness serves as a crucial step toward enhancing\nour environmental stewardship.\n","authors":["Burak Ekim","Michael Schmitt"],"pdf_url":"https://arxiv.org/pdf/2406.19302v1.pdf","comment":"6 pages, 3 figures, ICLR 2024 Tackling Climate Change with Machine\n  Learning Workshop"},{"id":"http://arxiv.org/abs/2406.19299v1","updated":"2024-06-27T16:15:22Z","published":"2024-06-27T16:15:22Z","title":"PNeRV: A Polynomial Neural Representation for Videos","summary":"  Extracting Implicit Neural Representations (INRs) on video data poses unique\nchallenges due to the additional temporal dimension. In the context of videos,\nINRs have predominantly relied on a frame-only parameterization, which\nsacrifices the spatiotemporal continuity observed in pixel-level (spatial)\nrepresentations. To mitigate this, we introduce Polynomial Neural\nRepresentation for Videos (PNeRV), a parameter-wise efficient, patch-wise INR\nfor videos that preserves spatiotemporal continuity. PNeRV leverages the\nmodeling capabilities of Polynomial Neural Networks to perform the modulation\nof a continuous spatial (patch) signal with a continuous time (frame) signal.\nWe further propose a custom Hierarchical Patch-wise Spatial Sampling Scheme\nthat ensures spatial continuity while retaining parameter efficiency. We also\nemploy a carefully designed Positional Embedding methodology to further enhance\nPNeRV's performance. Our extensive experimentation demonstrates that PNeRV\noutperforms the baselines in conventional Implicit Neural Representation tasks\nlike compression along with downstream applications that require spatiotemporal\ncontinuity in the underlying representation. PNeRV not only addresses the\nchallenges posed by video data in the realm of INRs but also opens new avenues\nfor advanced video processing and analysis.\n","authors":["Sonam Gupta","Snehal Singh Tomar","Grigorios G Chrysos","Sukhendu Das","A. N. Rajagopalan"],"pdf_url":"https://arxiv.org/pdf/2406.19299v1.pdf","comment":"25 pages, 17 figures, published at TMLR, Feb 2024"},{"id":"http://arxiv.org/abs/2406.19298v1","updated":"2024-06-27T16:13:34Z","published":"2024-06-27T16:13:34Z","title":"Compositional Image Decomposition with Diffusion Models","summary":"  Given an image of a natural scene, we are able to quickly decompose it into a\nset of components such as objects, lighting, shadows, and foreground. We can\nthen envision a scene where we combine certain components with those from other\nimages, for instance a set of objects from our bedroom and animals from a zoo\nunder the lighting conditions of a forest, even if we have never encountered\nsuch a scene before. In this paper, we present a method to decompose an image\ninto such compositional components. Our approach, Decomp Diffusion, is an\nunsupervised method which, when given a single image, infers a set of different\ncomponents in the image, each represented by a diffusion model. We demonstrate\nhow components can capture different factors of the scene, ranging from global\nscene descriptors like shadows or facial expression to local scene descriptors\nlike constituent objects. We further illustrate how inferred factors can be\nflexibly composed, even with factors inferred from other models, to generate a\nvariety of scenes sharply different than those seen in training time. Website\nand code at https://energy-based-model.github.io/decomp-diffusion.\n","authors":["Jocelin Su","Nan Liu","Yanbo Wang","Joshua B. Tenenbaum","Yilun Du"],"pdf_url":"https://arxiv.org/pdf/2406.19298v1.pdf","comment":"ICML 2024, Webpage:\n  https://energy-based-model.github.io/decomp-diffusion"},{"id":"http://arxiv.org/abs/2406.19297v1","updated":"2024-06-27T16:12:57Z","published":"2024-06-27T16:12:57Z","title":"Enhancing Continual Learning in Visual Question Answering with\n  Modality-Aware Feature Distillation","summary":"  Continual learning focuses on incrementally training a model on a sequence of\ntasks with the aim of learning new tasks while minimizing performance drop on\nprevious tasks. Existing approaches at the intersection of Continual Learning\nand Visual Question Answering (VQA) do not study how the multimodal nature of\nthe input affects the learning dynamics of a model. In this paper, we\ndemonstrate that each modality evolves at different rates across a continuum of\ntasks and that this behavior occurs in established encoder-only models as well\nas modern recipes for developing Vision & Language (VL) models. Motivated by\nthis observation, we propose a modality-aware feature distillation (MAFED)\napproach which outperforms existing baselines across models of varying scale in\nthree multimodal continual learning settings. Furthermore, we provide ablations\nshowcasing that modality-aware distillation complements experience replay.\nOverall, our results emphasize the importance of addressing modality-specific\ndynamics to prevent forgetting in multimodal continual learning.\n","authors":["Malvina Nikandrou","Georgios Pantazopoulos","Ioannis Konstas","Alessandro Suglia"],"pdf_url":"https://arxiv.org/pdf/2406.19297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19290v1","updated":"2024-06-27T16:04:41Z","published":"2024-06-27T16:04:41Z","title":"Human Modelling and Pose Estimation Overview","summary":"  Human modelling and pose estimation stands at the crossroads of Computer\nVision, Computer Graphics, and Machine Learning. This paper presents a thorough\ninvestigation of this interdisciplinary field, examining various algorithms,\nmethodologies, and practical applications. It explores the diverse range of\nsensor technologies relevant to this domain and delves into a wide array of\napplication areas. Additionally, we discuss the challenges and advancements in\n2D and 3D human modelling methodologies, along with popular datasets, metrics,\nand future research directions. The main contribution of this paper lies in its\nup-to-date comparison of state-of-the-art (SOTA) human pose estimation\nalgorithms in both 2D and 3D domains. By providing this comprehensive overview,\nthe paper aims to enhance understanding of 3D human modelling and pose\nestimation, offering insights into current SOTA achievements, challenges, and\nfuture prospects within the field.\n","authors":["Pawel Knap"],"pdf_url":"https://arxiv.org/pdf/2406.19290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19280v1","updated":"2024-06-27T15:50:41Z","published":"2024-06-27T15:50:41Z","title":"HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale","summary":"  The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.\n","authors":["Junying Chen","Ruyi Ouyang","Anningzhe Gao","Shunian Chen","Guiming Hardy Chen","Xidong Wang","Ruifei Zhang","Zhenyang Cai","Ke Ji","Guangjun Yu","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15847v3","updated":"2024-06-27T15:38:17Z","published":"2024-01-29T02:43:40Z","title":"Muffin or Chihuahua? Challenging Multimodal Large Language Models with\n  Multipanel VQA","summary":"  Multipanel images, commonly seen as web screenshots, posters, etc., pervade\nour daily lives. These images, characterized by their composition of multiple\nsubfigures in distinct layouts, effectively convey information to people.\nToward building advanced multimodal AI applications, such as agents that\nunderstand complex scenes and navigate through webpages, the skill of\nmultipanel visual reasoning is essential, and a comprehensive evaluation of\nmodels in this regard is important. Therefore, we introduce Multipanel Visual\nQuestion Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets\nof questions, answers, and multipanel images that specifically challenge models\nin comprehending multipanel images. Our evaluation shows that questions in the\nMultipanelVQA benchmark pose significant challenges to the state-of-the-art\nMultimodal Large Language Models (MLLMs) tested, even though humans can attain\napproximately 99% accuracy on these questions. Distinctively, the MultipanelVQA\nbenchmark features synthetically generated multipanel images specifically\ncrafted to isolate and assess the impact of various factors, such as the\nlayout, on MLLMs' multipanel image comprehension abilities. As a result, in\naddition to benchmarking the capabilities of MLLMs in understanding multipanel\nimages, we analyze various factors of the multipanel image that affect MLLMs'\nperformance with synthetic data and offer insights for enhancement. Code and\ndata are released at https://sites.google.com/view/multipanelvqa/home.\n","authors":["Yue Fan","Jing Gu","Kaiwen Zhou","Qianqi Yan","Shan Jiang","Ching-Chen Kuo","Xinze Guan","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2401.15847v3.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2406.19263v1","updated":"2024-06-27T15:34:16Z","published":"2024-06-27T15:34:16Z","title":"Read Anywhere Pointed: Layout-aware GUI Screen Reading with Tree-of-Lens\n  Grounding","summary":"  Graphical User Interfaces (GUIs) are central to our interaction with digital\ndevices. Recently, growing efforts have been made to build models for various\nGUI understanding tasks. However, these efforts largely overlook an important\nGUI-referring task: screen reading based on user-indicated points, which we\nname the Screen Point-and-Read (SPR) task. This task is predominantly handled\nby rigid accessible screen reading tools, in great need of new models driven by\nadvancements in Multimodal Large Language Models (MLLMs). In this paper, we\npropose a Tree-of-Lens (ToL) agent, utilizing a novel ToL grounding mechanism,\nto address the SPR task. Based on the input point coordinate and the\ncorresponding GUI screenshot, our ToL agent constructs a Hierarchical Layout\nTree. Based on the tree, our ToL agent not only comprehends the content of the\nindicated area but also articulates the layout and spatial relationships\nbetween elements. Such layout information is crucial for accurately\ninterpreting information on the screen, distinguishing our ToL agent from other\nscreen reading tools. We also thoroughly evaluate the ToL agent against other\nbaselines on a newly proposed SPR benchmark, which includes GUIs from mobile,\nweb, and operating systems. Last but not least, we test the ToL agent on mobile\nGUI navigation tasks, demonstrating its utility in identifying incorrect\nactions along the path of agent execution trajectories. Code and data:\nscreen-point-and-read.github.io\n","authors":["Yue Fan","Lei Ding","Ching-Chen Kuo","Shan Jiang","Yang Zhao","Xinze Guan","Jie Yang","Yi Zhang","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06748v2","updated":"2024-06-27T15:24:23Z","published":"2024-03-11T14:14:52Z","title":"Shortcut Learning in Medical Image Segmentation","summary":"  Shortcut learning is a phenomenon where machine learning models prioritize\nlearning simple, potentially misleading cues from data that do not generalize\nwell beyond the training set. While existing research primarily investigates\nthis in the realm of image classification, this study extends the exploration\nof shortcut learning into medical image segmentation. We demonstrate that\nclinical annotations such as calipers, and the combination of zero-padded\nconvolutions and center-cropped training sets in the dataset can inadvertently\nserve as shortcuts, impacting segmentation accuracy. We identify and evaluate\nthe shortcut learning on two different but common medical image segmentation\ntasks. In addition, we suggest strategies to mitigate the influence of shortcut\nlearning and improve the generalizability of the segmentation models. By\nuncovering the presence and implications of shortcuts in medical image\nsegmentation, we provide insights and methodologies for evaluating and\novercoming this pervasive challenge and call for attention in the community for\nshortcuts in segmentation. Our code is public at\nhttps://github.com/nina-weng/shortcut_skinseg .\n","authors":["Manxi Lin","Nina Weng","Kamil Mikolaj","Zahra Bashir","Morten Bo Søndergaard Svendsen","Martin Tolsgaard","Anders Nymark Christensen","Aasa Feragen"],"pdf_url":"https://arxiv.org/pdf/2403.06748v2.pdf","comment":"11 pages, 6 figures, accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.19255v1","updated":"2024-06-27T15:23:36Z","published":"2024-06-27T15:23:36Z","title":"Enhancing Video-Language Representations with Structural Spatio-Temporal\n  Alignment","summary":"  While pre-training large-scale video-language models (VLMs) has shown\nremarkable potential for various downstream video-language tasks, existing VLMs\ncan still suffer from certain commonly seen limitations, e.g., coarse-grained\ncross-modal aligning , under-modeling of temporal dynamics, detached\nvideo-language view. In this work, we target enhancing VLMs with a fine-grained\nstructural spatio-temporal alignment learning method (namely Finsta). First of\nall, we represent the input texts and videos with fine-grained scene graph (SG)\nstructures, both of which are further unified into a holistic SG (HSG) for\nbridging two modalities. Then, an SG-based framework is built, where the\ntextual SG (TSG) is encoded with a graph Transformer, while the video dynamic\nSG (DSG) and the HSG are modeled with a novel recurrent graph Transformer for\nspatial and temporal feature propagation. A spatial-temporal Gaussian\ndifferential graph Transformer is further devised to strengthen the sense of\nthe changes in objects across spatial and temporal dimensions. Next, based on\nthe fine-grained structural features of TSG and DSG, we perform object-centered\nspatial alignment and predicate-centered temporal alignment respectively,\nenhancing the video-language grounding in both the spatiality and temporality.\nWe design our method as a plug&play system, which can be integrated into\nexisting well-trained VLMs for further representation augmentation, without\ntraining from scratch or relying on SG annotations in downstream applications.\nOn 6 representative VL modeling tasks over 12 datasets in both standard and\nlong-form video scenarios, Finsta consistently improves the existing 13\nstrong-performing VLMs persistently, and refreshes the current state-of-the-art\nend task performance significantly in both the fine-tuning and zero-shot\nsettings.\n","authors":["Hao Fei","Shengqiong Wu","Meishan Zhang","Min Zhang","Tat-Seng Chua","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2406.19255v1.pdf","comment":"Accepted by IEEE TPAMI 2024"},{"id":"http://arxiv.org/abs/2406.19247v1","updated":"2024-06-27T15:14:23Z","published":"2024-06-27T15:14:23Z","title":"Local Manifold Learning for No-Reference Image Quality Assessment","summary":"  Contrastive learning has considerably advanced the field of Image Quality\nAssessment (IQA), emerging as a widely adopted technique. The core mechanism of\ncontrastive learning involves minimizing the distance between quality-similar\n(positive) examples while maximizing the distance between quality-dissimilar\n(negative) examples. Despite its successes, current contrastive learning\nmethods often neglect the importance of preserving the local manifold\nstructure. This oversight can result in a high degree of similarity among hard\nexamples within the feature space, thereby impeding effective differentiation\nand assessment. To address this issue, we propose an innovative framework that\nintegrates local manifold learning with contrastive learning for No-Reference\nImage Quality Assessment (NR-IQA). Our method begins by sampling multiple crops\nfrom a given image, identifying the most visually salient crop. This crop is\nthen used to cluster other crops from the same image as the positive class,\nwhile crops from different images are treated as negative classes to increase\ninter-class distance. Uniquely, our approach also considers non-saliency crops\nfrom the same image as intra-class negative classes to preserve their\ndistinctiveness. Additionally, we employ a mutual learning framework, which\nfurther enhances the model's ability to adaptively learn and identify visual\nsaliency regions. Our approach demonstrates a better performance compared to\nstate-of-the-art methods in 7 standard datasets, achieving PLCC values of 0.942\n(compared to 0.908 in TID2013) and 0.914 (compared to 0.894 in LIVEC).\n","authors":["Timin Gao","Wensheng Pan","Yan Zhang","Sicheng Zhao","Shengchuan Zhang","Xiawu Zheng","Ke Li","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2406.19247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01656v2","updated":"2024-06-27T15:07:39Z","published":"2024-05-02T18:26:15Z","title":"S4: Self-Supervised Sensing Across the Spectrum","summary":"  Satellite image time series (SITS) segmentation is crucial for many\napplications like environmental monitoring, land cover mapping and agricultural\ncrop type classification. However, training models for SITS segmentation\nremains a challenging task due to the lack of abundant training data, which\nrequires fine grained annotation. We propose S4 a new self-supervised\npre-training approach that significantly reduces the requirement for labeled\ntraining data by utilizing two new insights: (a) Satellites capture images in\ndifferent parts of the spectrum such as radio frequencies, and visible\nfrequencies. (b) Satellite imagery is geo-registered allowing for fine-grained\nspatial alignment. We use these insights to formulate pre-training tasks in S4.\nWe also curate m2s2-SITS, a large-scale dataset of unlabeled,\nspatially-aligned, multi-modal and geographic specific SITS that serves as\nrepresentative pre-training data for S4. Finally, we evaluate S4 on multiple\nSITS segmentation datasets and demonstrate its efficacy against competing\nbaselines while using limited labeled data.\n","authors":["Jayanth Shenoy","Xingjian Davis Zhang","Shlok Mehrotra","Bill Tao","Rem Yang","Han Zhao","Deepak Vasisht"],"pdf_url":"https://arxiv.org/pdf/2405.01656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19239v1","updated":"2024-06-27T15:02:04Z","published":"2024-06-27T15:02:04Z","title":"ALMA: a mathematics-driven approach for determining tuning parameters in\n  generalized LASSO problems, with applications to MRI","summary":"  Magnetic Resonance Imaging (MRI) is a powerful technique employed for\nnon-invasive in vivo visualization of internal structures. Sparsity is often\ndeployed to accelerate the signal acquisition or overcome the presence of\nmotion artifacts, improving the quality of image reconstruction. Image\nreconstruction algorithms use TV-regularized LASSO (Total Variation-regularized\nLASSO) to retrieve the missing information of undersampled signals, by cleaning\nthe data of noise and while optimizing sparsity. A tuning parameter moderates\nthe balance between these two aspects; its choice affecting the quality of the\nreconstructions. Currently, there is a lack of general deterministic techniques\nto choose these parameters, which are oftentimes manually selected and thus\nhinder the reliability of the reconstructions. Here, we present ALMA (Algorithm\nfor Lagrange Multipliers Approximation), an iterative mathematics-inspired\ntechnique that computes tuning parameters for generalized LASSO problems during\nMRI reconstruction. We analyze quantitatively the performance of these\nparameters for imaging reconstructions via TV-LASSO in an MRI context on\nphantoms. Although our study concentrates on TV-LASSO, the techniques developed\nhere hold significant promise for a wide array of applications. ALMA is not\nonly adaptable to more generalized LASSO problems but is also robust to\naccommodate other forms of regularization beyond total variation. Moreover, it\nextends effectively to handle non-Cartesian sampling trajectories, broadening\nits utility in complex data reconstruction scenarios. More generally, ALMA\nprovides a powerful tool for numerically solving constrained optimization\nproblems across various disciplines, offering a versatile and impactful\nsolution for advanced computational challenges.\n","authors":["Gianluca Giacchi","Isidoros Iakovidis","Bastien Milani","Matthias Stuber","Micah Murray","Benedetta Franceschiello"],"pdf_url":"https://arxiv.org/pdf/2406.19239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19236v1","updated":"2024-06-27T15:01:42Z","published":"2024-06-27T15:01:42Z","title":"Human-Aware Vision-and-Language Navigation: Bridging Simulation to\n  Reality with Dynamic Human Interactions","summary":"  Vision-and-Language Navigation (VLN) aims to develop embodied agents that\nnavigate based on human instructions. However, current VLN frameworks often\nrely on static environments and optimal expert supervision, limiting their\nreal-world applicability. To address this, we introduce Human-Aware\nVision-and-Language Navigation (HA-VLN), extending traditional VLN by\nincorporating dynamic human activities and relaxing key assumptions. We propose\nthe Human-Aware 3D (HA3D) simulator, which combines dynamic human activities\nwith the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R)\ndataset, extending R2R with human activity descriptions. To tackle HA-VLN\nchallenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and\nNon-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing\ncross-modal fusion and diverse training strategies for effective navigation in\ndynamic human environments. A comprehensive evaluation, including metrics\nconsidering human activities, and systematic analysis of HA-VLN's unique\nchallenges, underscores the need for further research to enhance HA-VLN agents'\nreal-world robustness and adaptability. Ultimately, this work provides\nbenchmarks and insights for future research on embodied AI and Sim2Real\ntransfer, paving the way for more realistic and applicable VLN systems in\nhuman-populated environments.\n","authors":["Minghan Li","Heng Li","Zhi-Qi Cheng","Yifei Dong","Yuxuan Zhou","Jun-Yan He","Qi Dai","Teruko Mitamura","Alexander G. Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2406.19236v1.pdf","comment":"30 pages, 18 figures, Project Page:\n  https://lpercc.github.io/HA3D_simulator/"},{"id":"http://arxiv.org/abs/2406.17382v2","updated":"2024-06-27T14:59:18Z","published":"2024-06-25T08:58:53Z","title":"Automatic infant 2D pose estimation from videos: comparing seven deep\n  neural network methods","summary":"  Automatic markerless estimation of infant posture and motion from ordinary\nvideos carries great potential for movement studies \"in the wild\", facilitating\nunderstanding of motor development and massively increasing the chances of\nearly diagnosis of disorders. There is rapid development of human pose\nestimation methods in computer vision thanks to advances in deep learning and\nmachine learning. However, these methods are trained on datasets featuring\nadults in different contexts. This work tests and compares seven popular\nmethods (AlphaPose, DeepLabCut/DeeperCut, Detectron2, HRNet,\nMediaPipe/BlazePose, OpenPose, and ViTPose) on videos of infants in supine\nposition. Surprisingly, all methods except DeepLabCut and MediaPipe have\ncompetitive performance without additional finetuning, with ViTPose performing\nbest. Next to standard performance metrics (object keypoint similarity, average\nprecision and recall), we introduce errors expressed in the neck-mid-hip ratio\nand additionally study missed and redundant detections and the reliability of\nthe internal confidence ratings of the different methods, which are relevant\nfor downstream tasks. Among the networks with competitive performance, only\nAlphaPose could run close to real time (27 fps) on our machine. We provide\ndocumented Docker containers or instructions for all the methods we used, our\nanalysis scripts, and processed data at https://hub.docker.com/u/humanoidsctu\nand https://osf.io/x465b/.\n","authors":["Filipe Gama","Matej Misar","Lukas Navara","Sergiu T. Popescu","Matej Hoffmann"],"pdf_url":"https://arxiv.org/pdf/2406.17382v2.pdf","comment":"21 pages, 3 figures, 14 tables"},{"id":"http://arxiv.org/abs/2406.05668v2","updated":"2024-06-27T14:55:41Z","published":"2024-06-09T06:53:39Z","title":"SRC-Net: Bi-Temporal Spatial Relationship Concerned Network for Change\n  Detection","summary":"  Change detection (CD) in remote sensing imagery is a crucial task with\napplications in environmental monitoring, urban development, and disaster\nmanagement. CD involves utilizing bi-temporal images to identify changes over\ntime. The bi-temporal spatial relationships between features at the same\nlocation at different times play a key role in this process. However, existing\nchange detection networks often do not fully leverage these spatial\nrelationships during bi-temporal feature extraction and fusion. In this work,\nwe propose SRC-Net: a bi-temporal spatial relationship concerned network for\nCD. The proposed SRC-Net includes a Perception and Interaction Module that\nincorporates spatial relationships and establishes a cross-branch perception\nmechanism to enhance the precision and robustness of feature extraction.\nAdditionally, a Patch-Mode joint Feature Fusion Module is introduced to address\ninformation loss in current methods. It considers different change modes and\nconcerns about spatial relationships, resulting in more expressive fusion\nfeatures. Furthermore, we construct a novel network using these two\nrelationship concerned modules and conducted experiments on the LEVIR-CD and\nWHU Building datasets. The experimental results demonstrate that our network\noutperforms state-of-the-art (SOTA) methods while maintaining a modest\nparameter count. We believe our approach sets a new paradigm for change\ndetection and will inspire further advancements in the field. The code and\nmodels are publicly available at https://github.com/Chnja/SRCNet.\n","authors":["Hongjia Chen","Xin Xu","Fangling Pu"],"pdf_url":"https://arxiv.org/pdf/2406.05668v2.pdf","comment":"13 pages, 12 figures, IEEE Journal of Selected Topics in Applied\n  Earth Observations and Remote Sensing (2024)"},{"id":"http://arxiv.org/abs/2406.19225v1","updated":"2024-06-27T14:50:50Z","published":"2024-06-27T14:50:50Z","title":"ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model\n  for Semantic Segmentation","summary":"  Domain adaptive semantic segmentation aims to generate accurate and dense\npredictions for an unlabeled target domain by leveraging a supervised model\ntrained on a labeled source domain. The prevalent self-training approach\ninvolves retraining the dense discriminative classifier of $p(class|pixel\nfeature)$ using the pseudo-labels from the target domain. While many methods\nfocus on mitigating the issue of noisy pseudo-labels, they often overlook the\nunderlying data distribution p(pixel feature|class) in both the source and\ntarget domains. To address this limitation, we propose the multi-prototype\nGaussian-Mixture-based (ProtoGMM) model, which incorporates the GMM into\ncontrastive losses to perform guided contrastive learning. Contrastive losses\nare commonly executed in the literature using memory banks, which can lead to\nclass biases due to underrepresented classes. Furthermore, memory banks often\nhave fixed capacities, potentially restricting the model's ability to capture\ndiverse representations of the target/source domains. An alternative approach\nis to use global class prototypes (i.e. averaged features per category).\nHowever, the global prototypes are based on the unimodal distribution\nassumption per class, disregarding within-class variation. To address these\nchallenges, we propose the ProtoGMM model. This novel approach involves\nestimating the underlying multi-prototype source distribution by utilizing the\nGMM on the feature space of the source samples. The components of the GMM model\nact as representative prototypes. To achieve increased intra-class semantic\nsimilarity, decreased inter-class similarity, and domain alignment between the\nsource and target domains, we employ multi-prototype contrastive learning\nbetween source distribution and target samples. The experiments show the\neffectiveness of our method on UDA benchmarks.\n","authors":["Nazanin Moradinasab","Laura S. Shankman","Rebecca A. Deaton","Gary K. Owens","Donald E. Brown"],"pdf_url":"https://arxiv.org/pdf/2406.19225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19217v1","updated":"2024-06-27T14:43:50Z","published":"2024-06-27T14:43:50Z","title":"Think Step by Step: Chain-of-Gesture Prompting for Error Detection in\n  Robotic Surgical Videos","summary":"  Despite significant advancements in robotic systems and surgical data\nscience, ensuring safe and optimal execution in robot-assisted minimally\ninvasive surgery (RMIS) remains a complex challenge. Current surgical error\ndetection methods involve two parts: identifying surgical gestures and then\ndetecting errors within each gesture clip. These methods seldom consider the\nrich contextual and semantic information inherent in surgical videos, limiting\ntheir performance due to reliance on accurate gesture identification. Motivated\nby the chain-of-thought prompting in natural language processing, this letter\npresents a novel and real-time end-to-end error detection framework,\nChain-of-Thought (COG) prompting, leveraging contextual information from\nsurgical videos. This encompasses two reasoning modules designed to mimic the\ndecision-making processes of expert surgeons. Concretely, we first design a\nGestural-Visual Reasoning module, which utilizes transformer and attention\narchitectures for gesture prompting, while the second, a Multi-Scale Temporal\nReasoning module, employs a multi-stage temporal convolutional network with\nboth slow and fast paths for temporal information extraction. We extensively\nvalidate our method on the public benchmark RMIS dataset JIGSAWS. Our method\nencapsulates the reasoning processes inherent to surgical activities enabling\nit to outperform the state-of-the-art by 4.6% in F1 score, 4.6% in Accuracy,\nand 5.9% in Jaccard index while processing each frame in 6.69 milliseconds on\naverage, demonstrating the great potential of our approach in enhancing the\nsafety and efficacy of RMIS procedures and surgical education. The code will be\navailable.\n","authors":["Zhimin Shao","Jialang Xu","Danail Stoyanov","Evangelos B. Mazomenos","Yueming Jin"],"pdf_url":"https://arxiv.org/pdf/2406.19217v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.06196v2","updated":"2024-06-27T14:19:56Z","published":"2024-05-10T02:23:56Z","title":"VLSM-Adapter: Finetuning Vision-Language Segmentation Efficiently with\n  Lightweight Blocks","summary":"  Foundation Vision-Language Models (VLMs) trained using large-scale\nopen-domain images and text pairs have recently been adapted to develop\nVision-Language Segmentation Models (VLSMs) that allow providing text prompts\nduring inference to guide image segmentation. If robust and powerful VLSMs can\nbe built for medical images, it could aid medical professionals in many\nclinical tasks where they must spend substantial time delineating the target\nstructure of interest. VLSMs for medical images resort to fine-tuning base VLM\nor VLSM pretrained on open-domain natural image datasets due to fewer annotated\nmedical image datasets; this fine-tuning is resource-consuming and expensive as\nit usually requires updating all or a significant fraction of the pretrained\nparameters. Recently, lightweight blocks called adapters have been proposed in\nVLMs that keep the pretrained model frozen and only train adapters during\nfine-tuning, substantially reducing the computing resources required. We\nintroduce a novel adapter, VLSM-Adapter, that can fine-tune pretrained\nvision-language segmentation models using transformer encoders. Our experiments\nin widely used CLIP-based segmentation models show that with only 3 million\ntrainable parameters, the VLSM-Adapter outperforms state-of-the-art and is\ncomparable to the upper bound end-to-end fine-tuning. The source code is\navailable at: https://github.com/naamiinepal/vlsm-adapter.\n","authors":["Manish Dhakal","Rabin Adhikari","Safal Thapaliya","Bishesh Khanal"],"pdf_url":"https://arxiv.org/pdf/2405.06196v2.pdf","comment":"Accepted at MICCAI 2024, the 27th International Conference on Medical\n  Image Computing and Computer Assisted Intervention"},{"id":"http://arxiv.org/abs/2406.19175v1","updated":"2024-06-27T13:51:53Z","published":"2024-06-27T13:51:53Z","title":"Towards Reducing Data Acquisition and Labeling for Defect Detection\n  using Simulated Data","summary":"  In many manufacturing settings, annotating data for machine learning and\ncomputer vision is costly, but synthetic data can be generated at significantly\nlower cost. Substituting the real-world data with synthetic data is therefore\nappealing for many machine learning applications that require large amounts of\ntraining data. However, relying solely on synthetic data is frequently\ninadequate for effectively training models that perform well on real-world\ndata, primarily due to domain shifts between the synthetic and real-world data.\nWe discuss approaches for dealing with such a domain shift when detecting\ndefects in X-ray scans of aluminium wheels. Using both simulated and real-world\nX-ray images, we train an object detection model with different strategies to\nidentify the training approach that generates the best detection results while\nminimising the demand for annotated real-world training samples. Our\npreliminary findings suggest that the sim-2-real domain adaptation approach is\nmore cost-efficient than a fully supervised oracle - if the total number of\navailable annotated samples is fixed. Given a certain number of labeled\nreal-world samples, training on a mix of synthetic and unlabeled real-world\ndata achieved comparable or even better detection results at significantly\nlower cost. We argue that future research into the cost-efficiency of different\ntraining strategies is important for a better understanding of how to allocate\nbudget in applied machine learning projects.\n","authors":["Lukas Malte Kemeter","Rasmus Hvingelby","Paulina Sierak","Tobias Schön","Bishwajit Gosswam"],"pdf_url":"https://arxiv.org/pdf/2406.19175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19162v1","updated":"2024-06-27T13:29:25Z","published":"2024-06-27T13:29:25Z","title":"Single Image Estimation of Cell Migration Direction by Deep Circular\n  Regression","summary":"  In this paper we study the problem of estimating the migration direction of\ncells based on a single image. To the best of our knowledge, there is only one\nrelated work that uses a classification CNN for four classes (quadrants). This\napproach does not allow detailed directional resolution. We solve the single\nimage estimation problem using deep circular regression with special attention\nto cycle-sensitive methods. On two databases we achieve an average accuracy of\n$\\sim$17 degrees, which is a significant improvement over the previous work.\n","authors":["Lennart Bruns","Lucas Lamparter","Milos Galic","Xiaoyi Jiang"],"pdf_url":"https://arxiv.org/pdf/2406.19162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14574v2","updated":"2024-06-27T13:29:07Z","published":"2023-12-22T10:10:50Z","title":"MMGPL: Multimodal Medical Data Analysis with Graph Prompt Learning","summary":"  Prompt learning has demonstrated impressive efficacy in the fine-tuning of\nmultimodal large models to a wide range of downstream tasks. Nonetheless,\napplying existing prompt learning methods for the diagnosis of neurological\ndisorder still suffers from two issues: (i) existing methods typically treat\nall patches equally, despite the fact that only a small number of patches in\nneuroimaging are relevant to the disease, and (ii) they ignore the structural\ninformation inherent in the brain connection network which is crucial for\nunderstanding and diagnosing neurological disorders. To tackle these issues, we\nintroduce a novel prompt learning model by learning graph prompts during the\nfine-tuning process of multimodal large models for diagnosing neurological\ndisorders. Specifically, we first leverage GPT-4 to obtain relevant disease\nconcepts and compute semantic similarity between these concepts and all\npatches. Secondly, we reduce the weight of irrelevant patches according to the\nsemantic similarity between each patch and disease-related concepts. Moreover,\nwe construct a graph among tokens based on these concepts and employ a graph\nconvolutional network layer to extract the structural information of the graph,\nwhich is used to prompt the pre-trained multimodal large models for diagnosing\nneurological disorders. Extensive experiments demonstrate that our method\nachieves superior performance for neurological disorder diagnosis compared with\nstate-of-the-art methods and validated by clinicians.\n","authors":["Liang Peng","Songyue Cai","Zongqian Wu","Huifang Shang","Xiaofeng Zhu","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2312.14574v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12223v3","updated":"2024-06-27T13:13:11Z","published":"2023-12-19T15:11:46Z","title":"Self-Supervised Detection of Perfect and Partial Input-Dependent\n  Symmetries","summary":"  Group equivariance can overly constrain models if the symmetries in the group\ndiffer from those observed in data. While common methods address this by\ndetermining the appropriate level of symmetry at the dataset level, they are\nlimited to supervised settings and ignore scenarios in which multiple levels of\nsymmetry co-exist in the same dataset. In this paper, we propose a method able\nto detect the level of symmetry of each input without the need for labels. Our\nframework is general enough to accommodate different families of both\ncontinuous and discrete symmetry distributions, such as arbitrary unimodal,\nsymmetric distributions and discrete groups. We validate the effectiveness of\nour approach on synthetic datasets with different per-class levels of\nsymmetries, and demonstrate practical applications such as the detection of\nout-of-distribution symmetries. Our code is publicly available at\nhttps://github.com/aurban0/ssl-sym.\n","authors":["Alonso Urbano","David W. Romero"],"pdf_url":"https://arxiv.org/pdf/2312.12223v3.pdf","comment":"19 pages, 8 figures, corrected typos, revised argument in Appendix\n  B.1, results unchanged"},{"id":"http://arxiv.org/abs/2406.19150v1","updated":"2024-06-27T13:08:35Z","published":"2024-06-27T13:08:35Z","title":"RAVEN: Multitask Retrieval Augmented Vision-Language Learning","summary":"  The scaling of large language models to encode all the world's knowledge in\nmodel parameters is unsustainable and has exacerbated resource barriers.\nRetrieval-Augmented Generation (RAG) presents a potential solution, yet its\napplication to vision-language models (VLMs) is under explored. Existing\nmethods focus on models designed for single tasks. Furthermore, they're limited\nby the need for resource intensive pre training, additional parameter\nrequirements, unaddressed modality prioritization and lack of clear benefit\nover non-retrieval baselines. This paper introduces RAVEN, a multitask\nretrieval augmented VLM framework that enhances base VLMs through efficient,\ntask specific fine-tuning. By integrating retrieval augmented samples without\nthe need for additional retrieval-specific parameters, we show that the model\nacquires retrieval properties that are effective across multiple tasks. Our\nresults and extensive ablations across retrieved modalities for the image\ncaptioning and VQA tasks indicate significant performance improvements compared\nto non retrieved baselines +1 CIDEr on MSCOCO, +4 CIDEr on NoCaps and nearly a\n+3\\% accuracy on specific VQA question types. This underscores the efficacy of\napplying RAG approaches to VLMs, marking a stride toward more efficient and\naccessible multimodal learning.\n","authors":["Varun Nagaraj Rao","Siddharth Choudhary","Aditya Deshpande","Ravi Kumar Satzoda","Srikar Appalaraju"],"pdf_url":"https://arxiv.org/pdf/2406.19150v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2406.19578v1","updated":"2024-06-27T23:43:36Z","published":"2024-06-27T23:43:36Z","title":"PathAlign: A vision-language model for whole slide images in\n  histopathology","summary":"  Microscopic interpretation of histopathology images underlies many important\ndiagnostic and treatment decisions. While advances in vision-language modeling\nraise new opportunities for analysis of such images, the gigapixel-scale size\nof whole slide images (WSIs) introduces unique challenges. Additionally,\npathology reports simultaneously highlight key findings from small regions\nwhile also aggregating interpretation across multiple slides, often making it\ndifficult to create robust image-text pairs. As such, pathology reports remain\na largely untapped source of supervision in computational pathology, with most\nefforts relying on region-of-interest annotations or self-supervision at the\npatch-level. In this work, we develop a vision-language model based on the\nBLIP-2 framework using WSIs paired with curated text from pathology reports.\nThis enables applications utilizing a shared image-text embedding space, such\nas text or image retrieval for finding cases of interest, as well as\nintegration of the WSI encoder with a frozen large language model (LLM) for\nWSI-based generative text capabilities such as report generation or\nAI-in-the-loop interactions. We utilize a de-identified dataset of over 350,000\nWSIs and diagnostic text pairs, spanning a wide range of diagnoses, procedure\ntypes, and tissue types. We present pathologist evaluation of text generation\nand text retrieval using WSI embeddings, as well as results for WSI\nclassification and workflow prioritization (slide-level triaging).\nModel-generated text for WSIs was rated by pathologists as accurate, without\nclinically significant error or omission, for 78% of WSIs on average. This work\ndemonstrates exciting potential capabilities for language-aligned WSI\nembeddings.\n","authors":["Faruk Ahmed","Andrew Sellergren","Lin Yang","Shawn Xu","Boris Babenko","Abbi Ward","Niels Olson","Arash Mohtashamian","Yossi Matias","Greg S. Corrado","Quang Duong","Dale R. Webster","Shravya Shetty","Daniel Golden","Yun Liu","David F. Steiner","Ellery Wulczyn"],"pdf_url":"https://arxiv.org/pdf/2406.19578v1.pdf","comment":"9 main pages and 19 pages of supplemental material; 3 main tables, 3\n  main figures and 11 supplemental tables, 7 supplemental figures"},{"id":"http://arxiv.org/abs/2212.12050v3","updated":"2024-06-27T23:43:30Z","published":"2022-12-22T22:00:58Z","title":"A Semantic Framework for Neural-Symbolic Computing","summary":"  Two approaches to AI, neural networks and symbolic systems, have been proven\nvery successful for an array of AI problems. However, neither has been able to\nachieve the general reasoning ability required for human-like intelligence. It\nhas been argued that this is due to inherent weaknesses in each approach.\nLuckily, these weaknesses appear to be complementary, with symbolic systems\nbeing adept at the kinds of things neural networks have trouble with and\nvice-versa. The field of neural-symbolic AI attempts to exploit this asymmetry\nby combining neural networks and symbolic AI into integrated systems. Often\nthis has been done by encoding symbolic knowledge into neural networks.\nUnfortunately, although many different methods for this have been proposed,\nthere is no common definition of an encoding to compare them. We seek to\nrectify this problem by introducing a semantic framework for neural-symbolic\nAI, which is then shown to be general enough to account for a large family of\nneural-symbolic systems. We provide a number of examples and proofs of the\napplication of the framework to the neural encoding of various forms of\nknowledge representation and neural network. These, at first sight disparate\napproaches, are all shown to fall within the framework's formal definition of\nwhat we call semantic encoding for neural-symbolic AI.\n","authors":["Simon Odense","Artur d'Avila Garcez"],"pdf_url":"https://arxiv.org/pdf/2212.12050v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02319v2","updated":"2024-06-27T23:22:14Z","published":"2024-04-02T21:35:54Z","title":"Symbolic Prompt Program Search: A Structure-Aware Approach to Efficient\n  Compile-Time Prompt Optimization","summary":"  In many modern LLM applications, such as retrieval augmented generation,\nprompts have become programs themselves. In these settings, prompt programs are\nrepeatedly called with different user queries or data instances. A big\npractical challenge is optimizing such prompt programs. Recent work has mostly\nfocused on either simple prompt programs or assumed that the general structure\nof a prompt program is fixed.\n  We introduce SAMMO, a framework to perform symbolic prompt program search for\ncompile-time optimizations of prompt programs. SAMMO represents prompt programs\non a symbolic level which allows for a rich set of transformations that can be\nsearched over during optimization. We show that SAMMO generalizes previous\nmethods and improves the performance of complex prompts on (1) instruction\ntuning, (2) RAG pipeline tuning, and (3) prompt compression, across several\ndifferent LLMs. We make all code available open-source at\nhttps://github.com/microsoft/sammo .\n","authors":["Tobias Schnabel","Jennifer Neville"],"pdf_url":"https://arxiv.org/pdf/2404.02319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19570v1","updated":"2024-06-27T23:15:45Z","published":"2024-06-27T23:15:45Z","title":"Synthetic Cancer -- Augmenting Worms with LLMs","summary":"  With increasingly sophisticated large language models (LLMs), the potential\nfor abuse rises drastically. As a submission to the Swiss AI Safety Prize, we\npresent a novel type of metamorphic malware leveraging LLMs for two key\nprocesses. First, LLMs are used for automatic code rewriting to evade\nsignature-based detection by antimalware programs. The malware then spreads its\ncopies via email by utilizing an LLM to socially engineer email replies to\nencourage recipients to execute the attached malware. Our submission includes a\nfunctional minimal prototype, highlighting the risks that LLMs pose for\ncybersecurity and underscoring the need for further research into intelligent\nmalware.\n","authors":["Benjamin Zimmerman","David Zollikofer"],"pdf_url":"https://arxiv.org/pdf/2406.19570v1.pdf","comment":"Won first place at the Swiss AI Safety Prize. Some technical details\n  omitted, contact authors for more information"},{"id":"http://arxiv.org/abs/2406.19568v1","updated":"2024-06-27T23:03:58Z","published":"2024-06-27T23:03:58Z","title":"What Matters in Detecting AI-Generated Videos like Sora?","summary":"  Recent advancements in diffusion-based video generation have showcased\nremarkable results, yet the gap between synthetic and real-world videos remains\nunder-explored. In this study, we examine this gap from three fundamental\nperspectives: appearance, motion, and geometry, comparing real-world videos\nwith those generated by a state-of-the-art AI model, Stable Video Diffusion. To\nachieve this, we train three classifiers using 3D convolutional networks, each\ntargeting distinct aspects: vision foundation model features for appearance,\noptical flow for motion, and monocular depth for geometry. Each classifier\nexhibits strong performance in fake video detection, both qualitatively and\nquantitatively. This indicates that AI-generated videos are still easily\ndetectable, and a significant gap between real and fake videos persists.\nFurthermore, utilizing the Grad-CAM, we pinpoint systematic failures of\nAI-generated videos in appearance, motion, and geometry. Finally, we propose an\nEnsemble-of-Experts model that integrates appearance, optical flow, and depth\ninformation for fake video detection, resulting in enhanced robustness and\ngeneralization ability. Our model is capable of detecting videos generated by\nSora with high accuracy, even without exposure to any Sora videos during\ntraining. This suggests that the gap between real and fake videos can be\ngeneralized across various video generative models. Project page:\nhttps://justin-crchang.github.io/3DCNNDetection.github.io/\n","authors":["Chirui Chang","Zhengzhe Liu","Xiaoyang Lyu","Xiaojuan Qi"],"pdf_url":"https://arxiv.org/pdf/2406.19568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13372v4","updated":"2024-06-27T22:44:48Z","published":"2024-03-20T08:08:54Z","title":"LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models","summary":"  Efficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It provides a\nsolution for flexibly customizing the fine-tuning of 100+ LLMs without the need\nfor coding through the built-in web UI LlamaBoard. We empirically validate the\nefficiency and effectiveness of our framework on language modeling and text\ngeneration tasks. It has been released at\nhttps://github.com/hiyouga/LLaMA-Factory and received over 25,000 stars and\n3,000 forks.\n","authors":["Yaowei Zheng","Richong Zhang","Junhao Zhang","Yanhan Ye","Zheyan Luo","Zhangchi Feng","Yongqiang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.13372v4.pdf","comment":"13 pages, accepted to ACL 2024 System Demonstration Track"},{"id":"http://arxiv.org/abs/2406.19561v1","updated":"2024-06-27T22:24:46Z","published":"2024-06-27T22:24:46Z","title":"Meta-Gradient Search Control: A Method for Improving the Efficiency of\n  Dyna-style Planning","summary":"  We study how a Reinforcement Learning (RL) system can remain sample-efficient\nwhen learning from an imperfect model of the environment. This is particularly\nchallenging when the learning system is resource-constrained and in continual\nsettings, where the environment dynamics change. To address these challenges,\nour paper introduces an online, meta-gradient algorithm that tunes a\nprobability with which states are queried during Dyna-style planning. Our study\ncompares the aggregate, empirical performance of this meta-gradient method to\nbaselines that employ conventional sampling strategies. Results indicate that\nour method improves efficiency of the planning process, which, as a\nconsequence, improves the sample-efficiency of the overall learning process. On\nthe whole, we observe that our meta-learned solutions avoid several pathologies\nof conventional planning approaches, such as sampling inaccurate transitions\nand those that stall credit assignment. We believe these findings could prove\nuseful, in future work, for designing model-based RL systems at scale.\n","authors":["Bradley Burega","John D. Martin","Luke Kapeluck","Michael Bowling"],"pdf_url":"https://arxiv.org/pdf/2406.19561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19552v1","updated":"2024-06-27T22:08:22Z","published":"2024-06-27T22:08:22Z","title":"Rethinking harmless refusals when fine-tuning foundation models","summary":"  In this paper, we investigate the degree to which fine-tuning in Large\nLanguage Models (LLMs) effectively mitigates versus merely conceals undesirable\nbehavior. Through the lens of semi-realistic role-playing exercises designed to\nelicit such behaviors, we explore the response dynamics of LLMs post\nfine-tuning interventions. Our methodology involves prompting models for\nChain-of-Thought (CoT) reasoning and analyzing the coherence between the\nreasoning traces and the resultant outputs. Notably, we identify a pervasive\nphenomenon we term \\emph{reason-based deception}, where models either stop\nproducing reasoning traces or produce seemingly ethical reasoning traces that\nbelie the unethical nature of their final outputs. We further examine the\nefficacy of response strategies (polite refusal versus explicit rebuttal) in\ncurbing the occurrence of undesired behavior in subsequent outputs of\nmulti-turn interactions. Our findings reveal that explicit rebuttals\nsignificantly outperform polite refusals in preventing the continuation of\nundesired outputs and nearly eliminate reason-based deception, challenging\ncurrent practices in model fine-tuning. Accordingly, the two key contributions\nof this paper are (1) defining and studying reason-based deception, a new type\nof hidden behavior, and (2) demonstrating that rebuttals provide a more robust\nresponse model to harmful requests than refusals, thereby highlighting the need\nto reconsider the response strategies in fine-tuning approaches.\n","authors":["Florin Pop","Judd Rosenblatt","Diogo Schwerz de Lucena","Michael Vaiana"],"pdf_url":"https://arxiv.org/pdf/2406.19552v1.pdf","comment":"ICLR 2024 AGI Workshop Poster"},{"id":"http://arxiv.org/abs/2406.09606v2","updated":"2024-06-27T22:06:19Z","published":"2024-06-13T22:34:58Z","title":"Cross-Modality Program Representation Learning for Electronic Design\n  Automation with High-Level Synthesis","summary":"  In recent years, domain-specific accelerators (DSAs) have gained popularity\nfor applications such as deep learning and autonomous driving. To facilitate\nDSA designs, programmers use high-level synthesis (HLS) to compile a high-level\ndescription written in C/C++ into a design with low-level hardware description\nlanguages that eventually synthesize DSAs on circuits. However, creating a\nhigh-quality HLS design still demands significant domain knowledge,\nparticularly in microarchitecture decisions expressed as \\textit{pragmas}.\nThus, it is desirable to automate such decisions with the help of machine\nlearning for predicting the quality of HLS designs, requiring a deeper\nunderstanding of the program that consists of original code and pragmas.\nNaturally, these programs can be considered as sequence data. In addition,\nthese programs can be compiled and converted into a control data flow graph\n(CDFG). But existing works either fail to leverage both modalities or combine\nthe two in shallow or coarse ways. We propose ProgSG, a model that allows\ninteraction between the source code sequence modality and the graph modality in\na deep and fine-grained way. To alleviate the scarcity of labeled designs, a\npre-training method is proposed based on a suite of compiler's data flow\nanalysis tasks. Experimental results show that ProgSG reduces the RMSE of\ndesign performance predictions by up to $22\\%$, and identifies designs with an\naverage of $1.10\\times$ and $1.26\\times$ (up to $8.17\\times$ and $13.31\\times$)\nperformance improvement in design space exploration (DSE) task compared to HARP\nand AutoDSE, respectively.\n","authors":["Zongyue Qin","Yunsheng Bai","Atefeh Sohrabizadeh","Zijian Ding","Ziniu Hu","Yizhou Sun","Jason Cong"],"pdf_url":"https://arxiv.org/pdf/2406.09606v2.pdf","comment":"14 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:2305.10838"},{"id":"http://arxiv.org/abs/2404.05891v2","updated":"2024-06-27T21:54:40Z","published":"2024-04-08T22:20:23Z","title":"Condition Monitoring with Incomplete Data: An Integrated Variational\n  Autoencoder and Distance Metric Framework","summary":"  Condition monitoring of industrial systems is crucial for ensuring safety and\nmaintenance planning, yet notable challenges arise in real-world settings due\nto the limited or non-existent availability of fault samples. This paper\nintroduces an innovative solution to this problem by proposing a new method for\nfault detection and condition monitoring for unseen data. Adopting an approach\ninspired by zero-shot learning, our method can identify faults and assign a\nrelative health index to various operational conditions. Typically, we have\nplenty of data on normal operations, some data on compromised conditions, and\nvery few (if any) samples of severe faults. We use a variational autoencoder to\ncapture the probabilistic distribution of previously seen and new unseen\nconditions. The health status is determined by comparing each sample's\ndeviation from a normal operation reference distribution in the latent space.\nFaults are detected by establishing a threshold for the health indexes,\nallowing the model to identify severe, unseen faults with high accuracy, even\namidst noise. We validate our approach using the run-to-failure IMS-bearing\ndataset and compare it with other methods. The health indexes generated by our\nmodel closely match the established descriptive model of bearing wear,\nattesting to the robustness and reliability of our method. These findings\nhighlight the potential of our methodology in augmenting fault detection\ncapabilities within industrial domains, thereby contributing to heightened\nsafety protocols and optimized maintenance practices.\n","authors":["Maryam Ahang","Mostafa Abbasi","Todd Charter","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2404.05891v2.pdf","comment":"Accepted in the 2024 IEEE 20th International Conference on Automation\n  Science and Engineering (CASE 2024)"},{"id":"http://arxiv.org/abs/2406.19545v1","updated":"2024-06-27T21:47:42Z","published":"2024-06-27T21:47:42Z","title":"Leveraging Machine-Generated Rationales to Facilitate Social Meaning\n  Detection in Conversations","summary":"  We present a generalizable classification approach that leverages Large\nLanguage Models (LLMs) to facilitate the detection of implicitly encoded social\nmeaning in conversations. We design a multi-faceted prompt to extract a textual\nexplanation of the reasoning that connects visible cues to underlying social\nmeanings. These extracted explanations or rationales serve as augmentations to\nthe conversational text to facilitate dialogue understanding and transfer. Our\nempirical results over 2,340 experimental settings demonstrate the significant\npositive impact of adding these rationales. Our findings hold true for\nin-domain classification, zero-shot, and few-shot domain transfer for two\ndifferent social meaning detection tasks, each spanning two different corpora.\n","authors":["Ritam Dutt","Zhen Wu","Kelly Shi","Divyanshu Sheth","Prakhar Gupta","Carolyn Penstein Rose"],"pdf_url":"https://arxiv.org/pdf/2406.19545v1.pdf","comment":"To appear at The Proceedings of the Association for Computational\n  Linguistics, 2024"},{"id":"http://arxiv.org/abs/2406.19537v1","updated":"2024-06-27T21:21:22Z","published":"2024-06-27T21:21:22Z","title":"Handling Ontology Gaps in Semantic Parsing","summary":"  The majority of Neural Semantic Parsing (NSP) models are developed with the\nassumption that there are no concepts outside the ones such models can\nrepresent with their target symbols (closed-world assumption). This assumption\nleads to generate hallucinated outputs rather than admitting their lack of\nknowledge. Hallucinations can lead to wrong or potentially offensive responses\nto users. Hence, a mechanism to prevent this behavior is crucial to build\ntrusted NSP-based Question Answering agents. To that end, we propose the\nHallucination Simulation Framework (HSF), a general setting for stimulating and\nanalyzing NSP model hallucinations. The framework can be applied to any NSP\ntask with a closed-ontology. Using the proposed framework and KQA Pro as the\nbenchmark dataset, we assess state-of-the-art techniques for hallucination\ndetection. We then present a novel hallucination detection strategy that\nexploits the computational graph of the NSP model to detect the NSP\nhallucinations in the presence of ontology gaps, out-of-domain utterances, and\nto recognize NSP errors, improving the F1-Score respectively by ~21, ~24% and\n~1%. This is the first work in closed-ontology NSP that addresses the problem\nof recognizing ontology gaps. We release our code and checkpoints at\nhttps://github.com/amazon-science/handling-ontology-gaps-in-semantic-parsing.\n","authors":["Andrea Bacciu","Marco Damonte","Marco Basaldella","Emilio Monti"],"pdf_url":"https://arxiv.org/pdf/2406.19537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19528v1","updated":"2024-06-27T21:03:56Z","published":"2024-06-27T21:03:56Z","title":"Using Large Language Models to Assist Video Content Analysis: An\n  Exploratory Study of Short Videos on Depression","summary":"  Despite the growing interest in leveraging Large Language Models (LLMs) for\ncontent analysis, current studies have primarily focused on text-based content.\nIn the present work, we explored the potential of LLMs in assisting video\ncontent analysis by conducting a case study that followed a new workflow of\nLLM-assisted multimodal content analysis. The workflow encompasses codebook\ndesign, prompt engineering, LLM processing, and human evaluation. We\nstrategically crafted annotation prompts to get LLM Annotations in structured\nform and explanation prompts to generate LLM Explanations for a better\nunderstanding of LLM reasoning and transparency. To test LLM's video annotation\ncapabilities, we analyzed 203 keyframes extracted from 25 YouTube short videos\nabout depression. We compared the LLM Annotations with those of two human\ncoders and found that LLM has higher accuracy in object and activity\nAnnotations than emotion and genre Annotations. Moreover, we identified the\npotential and limitations of LLM's capabilities in annotating videos. Based on\nthe findings, we explore opportunities and challenges for future research and\nimprovements to the workflow. We also discuss ethical concerns surrounding\nfuture studies based on LLM-assisted video analysis.\n","authors":["Jiaying Liu","Yunlong Wang","Yao Lyu","Yiheng Su","Shuo Niu","Xuhai \"Orson\" Xu","Yan Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.19528v1.pdf","comment":"6 pages, 2 figures, under review in CSCW 24"},{"id":"http://arxiv.org/abs/2406.19512v1","updated":"2024-06-27T20:18:18Z","published":"2024-06-27T20:18:18Z","title":"Captioning Visualizations with Large Language Models (CVLLM): A Tutorial","summary":"  Automatically captioning visualizations is not new, but recent advances in\nlarge language models(LLMs) open exciting new possibilities. In this tutorial,\nafter providing a brief review of Information Visualization (InfoVis)\nprinciples and past work in captioning, we introduce neural models and the\ntransformer architecture used in generic LLMs. We then discuss their recent\napplications in InfoVis, with a focus on captioning. Additionally, we explore\npromising future directions in this field.\n","authors":["Giuseppe Carenini","Jordon Johnson","Ali Salamatian"],"pdf_url":"https://arxiv.org/pdf/2406.19512v1.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2406.19507v1","updated":"2024-06-27T19:58:11Z","published":"2024-06-27T19:58:11Z","title":"Too Good to be True? Turn Any Model Differentially Private With\n  DP-Weights","summary":"  Imagine training a machine learning model with Differentially Private\nStochastic Gradient Descent (DP-SGD), only to discover post-training that the\nnoise level was either too high, crippling your model's utility, or too low,\ncompromising privacy. The dreaded realization hits: you must start the lengthy\ntraining process from scratch. But what if you could avoid this retraining\nnightmare? In this study, we introduce a groundbreaking approach (to our\nknowledge) that applies differential privacy noise to the model's weights after\ntraining. We offer a comprehensive mathematical proof for this novel approach's\nprivacy bounds, use formal methods to validate its privacy guarantees, and\nempirically evaluate its effectiveness using membership inference attacks and\nperformance evaluations. This method allows for a single training run, followed\nby post-hoc noise adjustments to achieve optimal privacy-utility trade-offs. We\ncompare this novel fine-tuned model (DP-Weights model) to a traditional DP-SGD\nmodel, demonstrating that our approach yields statistically similar performance\nand privacy guarantees. Our results validate the efficacy of post-training\nnoise application, promising significant time savings and flexibility in\nfine-tuning differential privacy parameters, making it a practical alternative\nfor deploying differentially private models in real-world scenarios.\n","authors":["David Zagardo"],"pdf_url":"https://arxiv.org/pdf/2406.19507v1.pdf","comment":"For code visit the following repository,\n  https://github.com/dzagardo/forgetnet/"},{"id":"http://arxiv.org/abs/2406.19502v1","updated":"2024-06-27T19:29:36Z","published":"2024-06-27T19:29:36Z","title":"Investigating How Large Language Models Leverage Internal Knowledge to\n  Perform Complex Reasoning","summary":"  Despite significant advancements, there is a limited understanding of how\nlarge language models (LLMs) utilize knowledge for reasoning. To address this,\nwe propose a method that deconstructs complex real-world questions into a\ngraph, representing each question as a node with parent nodes of background\nknowledge needed to solve the question. We develop the DepthQA dataset,\ndeconstructing questions into three depths: (i) recalling conceptual knowledge,\n(ii) applying procedural knowledge, and (iii) analyzing strategic knowledge.\nBased on a hierarchical graph, we quantify forward discrepancy, discrepancies\nin LLMs' performance on simpler sub-problems versus complex questions. We also\nmeasure backward discrepancy, where LLMs answer complex questions but struggle\nwith simpler ones. Our analysis shows that smaller models have more\ndiscrepancies than larger models. Additionally, guiding models from simpler to\ncomplex questions through multi-turn interactions improves performance across\nmodel sizes, highlighting the importance of structured intermediate steps in\nknowledge reasoning. This work enhances our understanding of LLM reasoning and\nsuggests ways to improve their problem-solving abilities.\n","authors":["Miyoung Ko","Sue Hyun Park","Joonsuk Park","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2406.19502v1.pdf","comment":"Work in progress; code is available at\n  https://github.com/kaistAI/knowledge-reasoning"},{"id":"http://arxiv.org/abs/2406.19500v1","updated":"2024-06-27T19:28:42Z","published":"2024-06-27T19:28:42Z","title":"Knowledge acquisition for dialogue agents using reinforcement learning\n  on graph representations","summary":"  We develop an artificial agent motivated to augment its knowledge base beyond\nits initial training. The agent actively participates in dialogues with other\nagents, strategically acquiring new information. The agent models its knowledge\nas an RDF knowledge graph, integrating new beliefs acquired through\nconversation. Responses in dialogue are generated by identifying graph patterns\naround these new integrated beliefs. We show that policies can be learned using\nreinforcement learning to select effective graph patterns during an\ninteraction, without relying on explicit user feedback. Within this context,\nour study is a proof of concept for leveraging users as effective sources of\ninformation.\n","authors":["Selene Baez Santamaria","Shihan Wang","Piek Vossen"],"pdf_url":"https://arxiv.org/pdf/2406.19500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19497v1","updated":"2024-06-27T19:26:11Z","published":"2024-06-27T19:26:11Z","title":"Inclusivity in Large Language Models: Personality Traits and Gender Bias\n  in Scientific Abstracts","summary":"  Large language models (LLMs) are increasingly utilized to assist in\nscientific and academic writing, helping authors enhance the coherence of their\narticles. Previous studies have highlighted stereotypes and biases present in\nLLM outputs, emphasizing the need to evaluate these models for their alignment\nwith human narrative styles and potential gender biases. In this study, we\nassess the alignment of three prominent LLMs - Claude 3 Opus, Mistral AI Large,\nand Gemini 1.5 Flash - by analyzing their performance on benchmark\ntext-generation tasks for scientific abstracts. We employ the Linguistic\nInquiry and Word Count (LIWC) framework to extract lexical, psychological, and\nsocial features from the generated texts. Our findings indicate that, while\nthese models generally produce text closely resembling human authored content,\nvariations in stylistic features suggest significant gender biases. This\nresearch highlights the importance of developing LLMs that maintain a diversity\nof writing styles to promote inclusivity in academic discourse.\n","authors":["Naseela Pervez","Alexander J. Titus"],"pdf_url":"https://arxiv.org/pdf/2406.19497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19493v1","updated":"2024-06-27T19:20:09Z","published":"2024-06-27T19:20:09Z","title":"Development and Evaluation of a Retrieval-Augmented Generation Tool for\n  Creating SAPPhIRE Models of Artificial Systems","summary":"  Representing systems using the SAPPhIRE causality model is found useful in\nsupporting design-by-analogy. However, creating a SAPPhIRE model of artificial\nor biological systems is an effort-intensive process that requires human\nexperts to source technical knowledge from multiple technical documents\nregarding how the system works. This research investigates how to leverage\nLarge Language Models (LLMs) in creating structured descriptions of systems\nusing the SAPPhIRE model of causality. This paper, the second part of the\ntwo-part research, presents a new Retrieval-Augmented Generation (RAG) tool for\ngenerating information related to SAPPhIRE constructs of artificial systems and\nreports the results from a preliminary evaluation of the tool's success -\nfocusing on the factual accuracy and reliability of outcomes.\n","authors":["Anubhab Majumder","Kausik Bhattacharya","Amaresh Chakrabarti"],"pdf_url":"https://arxiv.org/pdf/2406.19493v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02681v2","updated":"2024-06-27T19:06:32Z","published":"2024-02-05T02:35:11Z","title":"Equivariant Symmetry Breaking Sets","summary":"  Equivariant neural networks (ENNs) have been shown to be extremely effective\nin applications involving underlying symmetries. By construction ENNs cannot\nproduce lower symmetry outputs given a higher symmetry input. However, symmetry\nbreaking occurs in many physical systems and we may obtain a less symmetric\nstable state from an initial highly symmetric one. Hence, it is imperative that\nwe understand how to systematically break symmetry in ENNs. In this work, we\npropose a novel symmetry breaking framework that is fully equivariant and is\nthe first which fully addresses spontaneous symmetry breaking. We emphasize\nthat our approach is general and applicable to equivariance under any group. To\nachieve this, we introduce the idea of symmetry breaking sets (SBS). Rather\nthan redesign existing networks, we design sets of symmetry breaking objects\nwhich we feed into our network based on the symmetry of our inputs and outputs.\nWe show there is a natural way to define equivariance on these sets, which\ngives an additional constraint. Minimizing the size of these sets equates to\ndata efficiency. We prove that minimizing these sets translates to a well\nstudied group theory problem, and tabulate solutions to this problem for the\npoint groups. Finally, we provide some examples of symmetry breaking to\ndemonstrate how our approach works in practice.\n","authors":["YuQing Xie","Tess Smidt"],"pdf_url":"https://arxiv.org/pdf/2402.02681v2.pdf","comment":"43 pages, 18 figures"},{"id":"http://arxiv.org/abs/2406.19486v1","updated":"2024-06-27T19:02:41Z","published":"2024-06-27T19:02:41Z","title":"LoPT: Low-Rank Prompt Tuning for Parameter Efficient Language Models","summary":"  In prompt tuning, a prefix or suffix text is added to the prompt, and the\nembeddings (soft prompts) or token indices (hard prompts) of the prefix/suffix\nare optimized to gain more control over language models for specific tasks.\nThis approach eliminates the need for hand-crafted prompt engineering or\nexplicit model fine-tuning. Prompt tuning is significantly more\nparameter-efficient than model fine-tuning, as it involves optimizing partial\ninputs of language models to produce desired outputs.\n  In this work, we aim to further reduce the amount of trainable parameters\nrequired for a language model to perform well on specific tasks. We propose\nLow-rank Prompt Tuning (LoPT), a low-rank model for prompts that achieves\nefficient prompt optimization. The proposed method demonstrates similar\noutcomes to full parameter prompt tuning while reducing the number of trainable\nparameters by a factor of 5. It also provides promising results compared to the\nstate-of-the-art methods that would require 10 to 20 times more parameters.\n","authors":["Shouchang Guo","Sonam Damani","Keng-hao Chang"],"pdf_url":"https://arxiv.org/pdf/2406.19486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19478v1","updated":"2024-06-27T18:43:51Z","published":"2024-06-27T18:43:51Z","title":"Sparse Regression for Machine Translation","summary":"  We use transductive regression techniques to learn mappings between source\nand target features of given parallel corpora and use these mappings to\ngenerate machine translation outputs. We show the effectiveness of $L_1$\nregularized regression (\\textit{lasso}) to learn the mappings between sparsely\nobserved feature sets versus $L_2$ regularized regression. Proper selection of\ntraining instances plays an important role to learn correct feature mappings\nwithin limited computational resources and at expected accuracy levels. We\nintroduce \\textit{dice} instance selection method for proper selection of\ntraining instances, which plays an important role to learn correct feature\nmappings for improving the source and target coverage of the training set. We\nshow that $L_1$ regularized regression performs better than $L_2$ regularized\nregression both in regression measurements and in the translation experiments\nusing graph decoding. We present encouraging results when translating from\nGerman to English and Spanish to English. We also demonstrate results when the\nphrase table of a phrase-based decoder is replaced with the mappings we find\nwith the regression model.\n","authors":["Ergun Biçici"],"pdf_url":"https://arxiv.org/pdf/2406.19478v1.pdf","comment":"8 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2305.14752v2","updated":"2024-06-27T18:40:19Z","published":"2023-05-24T05:54:10Z","title":"A New Era in Software Security: Towards Self-Healing Software via Large\n  Language Models and Formal Verification","summary":"  This paper introduces an innovative approach that combines Large Language\nModels (LLMs) with Formal Verification strategies for automatic software\nvulnerability repair. Initially, we employ Bounded Model Checking (BMC) to\nidentify vulnerabilities and extract counterexamples. These counterexamples are\nsupported by mathematical proofs and the stack trace of the vulnerabilities.\nUsing a specially designed prompt, we combine the original source code with the\nidentified vulnerability, including its stack trace and counterexample that\nspecifies the line number and error type. This combined information is then fed\ninto an LLM, which is instructed to attempt to fix the code. The new code is\nsubsequently verified again using BMC to ensure the fix succeeded. We present\nthe ESBMC-AI framework as a proof of concept, leveraging the well-recognized\nand industry-adopted Efficient SMT-based Context-Bounded Model Checker (ESBMC)\nand a pre-trained transformer model to detect and fix errors in C programs,\nparticularly in critical software components. We evaluated our approach on\n50,000 C programs randomly selected from the FormAI dataset with their\nrespective vulnerability classifications. Our results demonstrate ESBMC-AI's\ncapability to automate the detection and repair of issues such as buffer\noverflow, arithmetic overflow, and pointer dereference failures with high\naccuracy. ESBMC-AI is a pioneering initiative, integrating LLMs with BMC\ntechniques, offering potential integration into the continuous integration and\ndeployment (CI/CD) process within the software development lifecycle.\n","authors":["Norbert Tihanyi","Ridhi Jain","Yiannis Charalambous","Mohamed Amine Ferrag","Youcheng Sun","Lucas C. Cordeiro"],"pdf_url":"https://arxiv.org/pdf/2305.14752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17073v2","updated":"2024-06-27T18:15:16Z","published":"2024-06-24T18:59:24Z","title":"Meta-GCN: A Dynamically Weighted Loss Minimization Method for Dealing\n  with the Data Imbalance in Graph Neural Networks","summary":"  Although many real-world applications, such as disease prediction, and fault\ndetection suffer from class imbalance, most existing graph-based classification\nmethods ignore the skewness of the distribution of classes; therefore, tend to\nbe biased towards the majority class(es). Conventional methods typically tackle\nthis problem through the assignment of weights to each one of the class samples\nbased on a function of their loss, which can lead to over-fitting on outliers.\nIn this paper, we propose a meta-learning algorithm, named Meta-GCN, for\nadaptively learning the example weights by simultaneously minimizing the\nunbiased meta-data set loss and optimizing the model weights through the use of\na small unbiased meta-data set. Through experiments, we have shown that\nMeta-GCN outperforms state-of-the-art frameworks and other baselines in terms\nof accuracy, the area under the receiver operating characteristic (AUC-ROC)\ncurve, and macro F1-Score for classification tasks on two different datasets.\n","authors":["Mahdi Mohammadizadeh","Arash Mozhdehi","Yani Ioannou","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.17073v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03832v2","updated":"2024-06-27T18:14:03Z","published":"2024-05-06T20:30:14Z","title":"Guylingo: The Republic of Guyana Creole Corpora","summary":"  While major languages often enjoy substantial attention and resources, the\nlinguistic diversity across the globe encompasses a multitude of smaller,\nindigenous, and regional languages that lack the same level of computational\nsupport. One such region is the Caribbean. While commonly labeled as \"English\nspeaking\", the ex-British Caribbean region consists of a myriad of Creole\nlanguages thriving alongside English. In this paper, we present Guylingo: a\ncomprehensive corpus designed for advancing NLP research in the domain of\nCreolese (Guyanese English-lexicon Creole), the most widely spoken language in\nthe culturally rich nation of Guyana. We first outline our framework for\ngathering and digitizing this diverse corpus, inclusive of colloquial\nexpressions, idioms, and regional variations in a low-resource language. We\nthen demonstrate the challenges of training and evaluating NLP models for\nmachine translation in Creole. Lastly, we discuss the unique opportunities\npresented by recent NLP advancements for accelerating the formal adoption of\nCreole languages as official languages in the Caribbean.\n","authors":["Christopher Clarke","Roland Daynauth","Charlene Wilkinson","Hubert Devonish","Jason Mars"],"pdf_url":"https://arxiv.org/pdf/2405.03832v2.pdf","comment":"Accepted to NAACL 2024 Main Conference Special Theme Track: Languages\n  of Latin America and The Caribbean"},{"id":"http://arxiv.org/abs/2406.19464v1","updated":"2024-06-27T18:06:38Z","published":"2024-06-27T18:06:38Z","title":"ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data","summary":"  Audio signals provide rich information for the robot interaction and object\nproperties through contact. These information can surprisingly ease the\nlearning of contact-rich robot manipulation skills, especially when the visual\ninformation alone is ambiguous or incomplete. However, the usage of audio data\nin robot manipulation has been constrained to teleoperated demonstrations\ncollected by either attaching a microphone to the robot or object, which\nsignificantly limits its usage in robot learning pipelines. In this work, we\nintroduce ManiWAV: an 'ear-in-hand' data collection device to collect\nin-the-wild human demonstrations with synchronous audio and visual feedback,\nand a corresponding policy interface to learn robot manipulation policy\ndirectly from the demonstrations. We demonstrate the capabilities of our system\nthrough four contact-rich manipulation tasks that require either passively\nsensing the contact events and modes, or actively sensing the object surface\nmaterials and states. In addition, we show that our system can generalize to\nunseen in-the-wild environments, by learning from diverse in-the-wild human\ndemonstrations. Project website: https://mani-wav.github.io/\n","authors":["Zeyi Liu","Cheng Chi","Eric Cousineau","Naveen Kuppuswamy","Benjamin Burchfiel","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2406.19464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14572v2","updated":"2024-06-27T18:00:31Z","published":"2024-06-13T17:53:29Z","title":"Bioptic -- A Target-Agnostic Efficacy-Based Small Molecules Search\n  Engine","summary":"  Recent successes in virtual screening have been made possible by large models\nand extensive chemical libraries. However, combining these elements is\nchallenging: the larger the model, the more expensive it is to run, making\nultra-large libraries unfeasible. To address this, we developed a\ntarget-agnostic, efficacy-based molecule search model, which allows us to find\nstructurally dissimilar molecules with similar biological activities. We used\nthe best practices to design fast retrieval system, based on\nprocessor-optimized SIMD instructions, enabling us to screen the ultra-large\n40B Enamine REAL library with 100\\% recall rate. We extensively benchmarked our\nmodel and several state-of-the-art models for both speed performance and\nretrieval quality of novel molecules.\n","authors":["Vlad Vinogradov","Ivan Izmailov","Simon Steshin","Kong T. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2406.14572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19434v1","updated":"2024-06-27T17:59:05Z","published":"2024-06-27T17:59:05Z","title":"Lightweight Predictive 3D Gaussian Splats","summary":"  Recent approaches representing 3D objects and scenes using Gaussian splats\nshow increased rendering speed across a variety of platforms and devices. While\nrendering such representations is indeed extremely efficient, storing and\ntransmitting them is often prohibitively expensive. To represent large-scale\nscenes, one often needs to store millions of 3D Gaussians, occupying gigabytes\nof disk space. This poses a very practical limitation, prohibiting widespread\nadoption.Several solutions have been proposed to strike a balance between disk\nsize and rendering quality, noticeably reducing the visual quality. In this\nwork, we propose a new representation that dramatically reduces the hard drive\nfootprint while featuring similar or improved quality when compared to the\nstandard 3D Gaussian splats. When compared to other compact solutions, ours\noffers higher quality renderings with significantly reduced storage, being able\nto efficiently run on a mobile device in real-time. Our key observation is that\nnearby points in the scene can share similar representations. Hence, only a\nsmall ratio of 3D points needs to be stored. We introduce an approach to\nidentify such points which are called parent points. The discarded points\ncalled children points along with attributes can be efficiently predicted by\ntiny MLPs.\n","authors":["Junli Cao","Vidit Goel","Chaoyang Wang","Anil Kag","Ju Hu","Sergei Korolev","Chenfanfu Jiang","Sergey Tulyakov","Jian Ren"],"pdf_url":"https://arxiv.org/pdf/2406.19434v1.pdf","comment":"Project Page: https://plumpuddings.github.io/LPGS//"},{"id":"http://arxiv.org/abs/2406.19384v1","updated":"2024-06-27T17:57:03Z","published":"2024-06-27T17:57:03Z","title":"The Remarkable Robustness of LLMs: Stages of Inference?","summary":"  We demonstrate and investigate the remarkable robustness of Large Language\nModels by deleting and swapping adjacent layers. We find that deleting and\nswapping interventions retain 72-95\\% of the original model's prediction\naccuracy without fine-tuning, whereas models with more layers exhibit more\nrobustness. Based on the results of the layer-wise intervention and further\nexperiments, we hypothesize the existence of four universal stages of inference\nacross eight different models: detokenization, feature engineering, prediction\nensembling, and residual sharpening. The first stage integrates local\ninformation, lifting raw token representations into higher-level contextual\nrepresentations. Next is the iterative refinement of task and entity-specific\nfeatures. Then, the second half of the model begins with a phase transition,\nwhere hidden representations align more with the vocabulary space due to\nspecialized model components. Finally, the last layer sharpens the following\ntoken distribution by eliminating obsolete features that add noise to the\nprediction.\n","authors":["Vedang Lad","Wes Gurnee","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2406.19384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19370v1","updated":"2024-06-27T17:50:05Z","published":"2024-06-27T17:50:05Z","title":"Emergence of Hidden Capabilities: Exploring Learning Dynamics in Concept\n  Space","summary":"  Modern generative models demonstrate impressive capabilities, likely stemming\nfrom an ability to identify and manipulate abstract concepts underlying their\ntraining data. However, fundamental questions remain: what determines the\nconcepts a model learns, the order in which it learns them, and its ability to\nmanipulate those concepts? To address these questions, we propose analyzing a\nmodel's learning dynamics via a framework we call the concept space, where each\naxis represents an independent concept underlying the data generating process.\nBy characterizing learning dynamics in this space, we identify how the speed at\nwhich a concept is learned, and hence the order of concept learning, is\ncontrolled by properties of the data we term concept signal. Further, we\nobserve moments of sudden turns in the direction of a model's learning dynamics\nin concept space. Surprisingly, these points precisely correspond to the\nemergence of hidden capabilities, i.e., where latent interventions show the\nmodel possesses the capability to manipulate a concept, but these capabilities\ncannot yet be elicited via naive input prompting. While our results focus on\nsynthetically defined toy datasets, we hypothesize a general claim on emergence\nof hidden capabilities may hold: generative models possess latent capabilities\nthat emerge suddenly and consistently during training, though a model might not\nexhibit these capabilities under naive input prompting.\n","authors":["Core Francisco Park","Maya Okawa","Andrew Lee","Ekdeep Singh Lubana","Hidenori Tanaka"],"pdf_url":"https://arxiv.org/pdf/2406.19370v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.19354v1","updated":"2024-06-27T17:33:03Z","published":"2024-06-27T17:33:03Z","title":"Fundamental Problems With Model Editing: How Should Rational Belief\n  Revision Work in LLMs?","summary":"  The model editing problem concerns how language models should learn new facts\nabout the world over time. While empirical research on model editing has drawn\nwidespread attention, the conceptual foundations of model editing remain shaky\n-- perhaps unsurprisingly, since model editing is essentially belief revision,\na storied problem in philosophy that has eluded succinct solutions for decades.\nModel editing nonetheless demands a solution, since we need to be able to\ncontrol the knowledge within language models. With this goal in mind, this\npaper critiques the standard formulation of the model editing problem and\nproposes a formal testbed for model editing research. We first describe 12 open\nproblems with model editing, based on challenges with (1) defining the problem,\n(2) developing benchmarks, and (3) assuming LLMs have editable beliefs in the\nfirst place. Many of these challenges are extremely difficult to address, e.g.\ndetermining far-reaching consequences of edits, labeling probabilistic\nentailments between facts, and updating beliefs of agent simulators. Next, we\nintroduce a semi-synthetic dataset for model editing based on Wikidata, where\nwe can evaluate edits against labels given by an idealized Bayesian agent. This\nenables us to say exactly how belief revision in language models falls short of\na desirable epistemic standard. We encourage further research exploring\nsettings where such a gold standard can be compared against. Our code is\npublicly available at: https://github.com/peterbhase/LLM-belief-revision\n","authors":["Peter Hase","Thomas Hofweber","Xiang Zhou","Elias Stengel-Eskin","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2406.19354v1.pdf","comment":"23 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.13040v2","updated":"2024-06-27T17:27:13Z","published":"2024-03-19T17:35:17Z","title":"Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping","summary":"  Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify\ncolor Doppler in cardiac imaging. In this study, we propose novel alternatives\nto the traditional iVFM optimization scheme by utilizing physics-informed\nneural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.\nWhen evaluated on simulated color Doppler images derived from a\npatient-specific computational fluid dynamics model and in vivo Doppler\nacquisitions, both approaches demonstrate comparable reconstruction performance\nto the original iVFM algorithm. The efficiency of PINNs is boosted through\ndual-stage optimization and pre-optimized weights. On the other hand, the\nnnU-Net method excels in generalizability and real-time capabilities. Notably,\nnnU-Net shows superior robustness on sparse and truncated Doppler data while\nmaintaining independence from explicit boundary conditions. Overall, our\nresults highlight the effectiveness of these methods in reconstructing\nintraventricular vector blood flow. The study also suggests potential\napplications of PINNs in ultrafast color Doppler imaging and the incorporation\nof fluid dynamics equations to derive biomarkers for cardiovascular diseases\nbased on blood flow.\n","authors":["Hang Jung Ling","Salomé Bru","Julia Puig","Florian Vixège","Simon Mendez","Franck Nicoud","Pierre-Yves Courand","Olivier Bernard","Damien Garcia"],"pdf_url":"https://arxiv.org/pdf/2403.13040v2.pdf","comment":"12 pages, accepted for publication in IEEE TUFFC; camera ready\n  corrections, corrected acknowledgments"},{"id":"http://arxiv.org/abs/2406.19349v1","updated":"2024-06-27T17:26:38Z","published":"2024-06-27T17:26:38Z","title":"IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and\n  Toxicity Types for Indonesian Language","summary":"  Hate speech poses a significant threat to social harmony. Over the past two\nyears, Indonesia has seen a ten-fold increase in the online hate speech ratio,\nunderscoring the urgent need for effective detection mechanisms. However,\nprogress is hindered by the limited availability of labeled data for Indonesian\ntexts. The condition is even worse for marginalized minorities, such as Shia,\nLGBTQ, and other ethnic minorities because hate speech is underreported and\nless understood by detection tools. Furthermore, the lack of accommodation for\nsubjectivity in current datasets compounds this issue. To address this, we\nintroduce IndoToxic2024, a comprehensive Indonesian hate speech and toxicity\nclassification dataset. Comprising 43,692 entries annotated by 19 diverse\nindividuals, the dataset focuses on texts targeting vulnerable groups in\nIndonesia, specifically during the hottest political event in the country: the\npresidential election. We establish baselines for seven binary classification\ntasks, achieving a macro-F1 score of 0.78 with a BERT model (IndoBERTweet)\nfine-tuned for hate speech classification. Furthermore, we demonstrate how\nincorporating demographic information can enhance the zero-shot performance of\nthe large language model, gpt-3.5-turbo. However, we also caution that an\noveremphasis on demographic information can negatively impact the fine-tuned\nmodel performance due to data fragmentation.\n","authors":["Lucky Susanto","Musa Izzanardi Wijanarko","Prasetia Anugrah Pratama","Traci Hong","Ika Idris","Alham Fikri Aji","Derry Wijaya"],"pdf_url":"https://arxiv.org/pdf/2406.19349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05162v2","updated":"2024-06-27T17:23:58Z","published":"2024-02-07T18:34:38Z","title":"Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank\n  Modifications","summary":"  Large language models (LLMs) show inherent brittleness in their safety\nmechanisms, as evidenced by their susceptibility to jailbreaking and even\nnon-malicious fine-tuning. This study explores this brittleness of safety\nalignment by leveraging pruning and low-rank modifications. We develop methods\nto identify critical regions that are vital for safety guardrails, and that are\ndisentangled from utility-relevant regions at both the neuron and rank levels.\nSurprisingly, the isolated regions we find are sparse, comprising about $3\\%$\nat the parameter level and $2.5\\%$ at the rank level. Removing these regions\ncompromises safety without significantly impacting utility, corroborating the\ninherent brittleness of the model's safety mechanisms. Moreover, we show that\nLLMs remain vulnerable to low-cost fine-tuning attacks even when modifications\nto the safety-critical regions are restricted. These findings underscore the\nurgent need for more robust safety strategies in LLMs.\n","authors":["Boyi Wei","Kaixuan Huang","Yangsibo Huang","Tinghao Xie","Xiangyu Qi","Mengzhou Xia","Prateek Mittal","Mengdi Wang","Peter Henderson"],"pdf_url":"https://arxiv.org/pdf/2402.05162v2.pdf","comment":"22 pages, 9 figures. Project page is available at\n  https://boyiwei.com/alignment-attribution/"},{"id":"http://arxiv.org/abs/2307.08564v2","updated":"2024-06-27T17:18:10Z","published":"2023-07-17T15:31:58Z","title":"Shaping New Norms for AI","summary":"  As Artificial Intelligence (AI) becomes increasingly integrated into our\nlives, the need for new norms is urgent. However, AI evolves at a much faster\npace than the characteristic time of norm formation, posing an unprecedented\nchallenge to our societies. This paper examines possible criticalities of the\nprocesses of norm formation surrounding AI. Thus, it focuses on how new norms\ncan be established, rather than on what these norms should be. It distinguishes\ndifferent scenarios based on the centralisation or decentralisation of the norm\nformation process, analysing the cases where new norms are shaped by formal\nauthorities, informal institutions, or emerge spontaneously in a bottom-up\nfashion. On the latter point, the paper reports a conversation with ChatGPT in\nwhich the LLM discusses some of the emerging norms it has observed. Far from\nseeking exhaustiveness, this article aims to offer readers interpretive tools\nto understand society's response to the growing pervasiveness of AI. An outlook\non how AI could influence the formation of future social norms emphasises the\nimportance for open societies to anchor their formal deliberation process in an\nopen, inclusive, and transparent public discourse.\n","authors":["Andrea Baronchelli"],"pdf_url":"https://arxiv.org/pdf/2307.08564v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16755v2","updated":"2024-06-27T17:13:32Z","published":"2024-05-27T01:54:16Z","title":"CHESS: Contextual Harnessing for Efficient SQL Synthesis","summary":"  Utilizing large language models (LLMs) for transforming natural language\nquestions into SQL queries (text-to-SQL) is a promising yet challenging\napproach, particularly when applied to real-world databases with complex and\nextensive schemas. In particular, effectively incorporating data catalogs and\ndatabase values for SQL generation remains an obstacle, leading to suboptimal\nsolutions. We address this problem by proposing a new pipeline that effectively\nretrieves relevant data and context, selects an efficient schema, and\nsynthesizes correct and efficient SQL queries. To increase retrieval precision,\nour pipeline introduces a hierarchical retrieval method leveraging\nmodel-generated keywords, locality-sensitive hashing indexing, and vector\ndatabases. Additionally, we have developed an adaptive schema pruning technique\nthat adjusts based on the complexity of the problem and the model's context\nsize. Our approach generalizes to both frontier proprietary models like GPT-4\nand open-source models such as Llama-3-70B. Through a series of ablation\nstudies, we demonstrate the effectiveness of each component of our pipeline and\nits impact on the end-to-end performance. Our method achieves new\nstate-of-the-art performance on the cross-domain challenging BIRD dataset.\n","authors":["Shayan Talaei","Mohammadreza Pourreza","Yu-Chen Chang","Azalia Mirhoseini","Amin Saberi"],"pdf_url":"https://arxiv.org/pdf/2405.16755v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12373v2","updated":"2024-06-27T16:56:13Z","published":"2024-06-18T07:58:33Z","title":"WebCanvas: Benchmarking Web Agents in Online Environments","summary":"  For web agents to be practically useful, they must adapt to the continuously\nevolving web environment characterized by frequent updates to user interfaces\nand content. However, most existing benchmarks only capture the static aspects\nof the web. To bridge this gap, we introduce WebCanvas, an innovative online\nevaluation framework for web agents that effectively addresses the dynamic\nnature of web interactions. WebCanvas contains three main components to\nfacilitate realistic assessments: (1) A novel evaluation metric which reliably\ncapture critical intermediate actions or states necessary for task completions\nwhile disregarding noise caused by insignificant events or changed\nweb-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version\nof original Mind2Web static dataset containing 542 tasks with 2439 intermediate\nevaluation states; (3) Lightweight and generalizable annotation tools and\ntesting pipelines that enables the community to collect and maintain the\nhigh-quality, up-to-date dataset. Building on WebCanvas, we open-source an\nagent framework with extensible modules for reasoning, providing a foundation\nfor the community to conduct online inference and evaluations. Our\nbest-performing agent achieves a task success rate of 23.1% and a task\ncompletion rate of 48.8% on the Mind2Web-Live test set. Additionally, we\nanalyze the performance discrepancies across various websites, domains, and\nexperimental environments. We encourage the community to contribute further\ninsights on online agent evaluation, thereby advancing this field of research.\n","authors":["Yichen Pan","Dehan Kong","Sida Zhou","Cheng Cui","Yifei Leng","Bing Jiang","Hangyu Liu","Yanyi Shang","Shuyan Zhou","Tongshuang Wu","Zhengyang Wu"],"pdf_url":"https://arxiv.org/pdf/2406.12373v2.pdf","comment":"Our platform, tool and dataset are publically available at\n  https://www.imean.ai/web-canvas/ and\n  https://huggingface.co/datasets/iMeanAI/Mind2Web-Live/"},{"id":"http://arxiv.org/abs/2406.19320v1","updated":"2024-06-27T16:54:12Z","published":"2024-06-27T16:54:12Z","title":"Efficient World Models with Context-Aware Tokenization","summary":"  Scaling up deep Reinforcement Learning (RL) methods presents a significant\nchallenge. Following developments in generative modelling, model-based RL\npositions itself as a strong contender. Recent advances in sequence modelling\nhave led to effective transformer-based world models, albeit at the price of\nheavy computations due to the long sequences of tokens required to accurately\nsimulate environments. In this work, we propose $\\Delta$-IRIS, a new agent with\na world model architecture composed of a discrete autoencoder that encodes\nstochastic deltas between time steps and an autoregressive transformer that\npredicts future deltas by summarizing the current state of the world with\ncontinuous tokens. In the Crafter benchmark, $\\Delta$-IRIS sets a new state of\nthe art at multiple frame budgets, while being an order of magnitude faster to\ntrain than previous attention-based approaches. We release our code and models\nat https://github.com/vmicheli/delta-iris.\n","authors":["Vincent Micheli","Eloi Alonso","François Fleuret"],"pdf_url":"https://arxiv.org/pdf/2406.19320v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2406.19317v1","updated":"2024-06-27T16:52:19Z","published":"2024-06-27T16:52:19Z","title":"Jump Starting Bandits with LLM-Generated Prior Knowledge","summary":"  We present substantial evidence demonstrating the benefits of integrating\nLarge Language Models (LLMs) with a Contextual Multi-Armed Bandit framework.\nContextual bandits have been widely used in recommendation systems to generate\npersonalized suggestions based on user-specific contexts. We show that LLMs,\npre-trained on extensive corpora rich in human knowledge and preferences, can\nsimulate human behaviours well enough to jump-start contextual multi-armed\nbandits to reduce online learning regret. We propose an initialization\nalgorithm for contextual bandits by prompting LLMs to produce a pre-training\ndataset of approximate human preferences for the bandit. This significantly\nreduces online learning regret and data-gathering costs for training such\nmodels. Our approach is validated empirically through two sets of experiments\nwith different bandit setups: one which utilizes LLMs to serve as an oracle and\na real-world experiment utilizing data from a conjoint survey experiment.\n","authors":["Parand A. Alamdari","Yanshuai Cao","Kevin H. Wilson"],"pdf_url":"https://arxiv.org/pdf/2406.19317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19314v1","updated":"2024-06-27T16:47:42Z","published":"2024-06-27T16:47:42Z","title":"LiveBench: A Challenging, Contamination-Free LLM Benchmark","summary":"  Test set contamination, wherein test data from a benchmark ends up in a newer\nmodel's training set, is a well-documented obstacle for fair LLM evaluation and\ncan quickly render benchmarks obsolete. To mitigate this, many recent\nbenchmarks crowdsource new prompts and evaluations from human or LLM judges;\nhowever, these can introduce significant biases, and break down when scoring\nhard questions. In this work, we introduce a new benchmark for LLMs designed to\nbe immune to both test set contamination and the pitfalls of LLM judging and\nhuman crowdsourcing. We release LiveBench, the first benchmark that (1)\ncontains frequently-updated questions from recent information sources, (2)\nscores answers automatically according to objective ground-truth values, and\n(3) contains a wide variety of challenging tasks, spanning math, coding,\nreasoning, language, instruction following, and data analysis. To achieve this,\nLiveBench contains questions that are based on recently-released math\ncompetitions, arXiv papers, news articles, and datasets, and it contains\nharder, contamination-free versions of tasks from previous benchmarks such as\nBig-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source\nmodels, as well as dozens of open-source models ranging from 0.5B to 110B in\nsize. LiveBench is difficult, with top models achieving below 65% accuracy. We\nrelease all questions, code, and model answers. Questions will be added and\nupdated on a monthly basis, and we will release new tasks and harder versions\nof tasks over time so that LiveBench can distinguish between the capabilities\nof LLMs as they improve in the future. We welcome community engagement and\ncollaboration for expanding the benchmark tasks and models.\n","authors":["Colin White","Samuel Dooley","Manley Roberts","Arka Pal","Ben Feuer","Siddhartha Jain","Ravid Shwartz-Ziv","Neel Jain","Khalid Saifullah","Siddartha Naidu","Chinmay Hegde","Yann LeCun","Tom Goldstein","Willie Neiswanger","Micah Goldblum"],"pdf_url":"https://arxiv.org/pdf/2406.19314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07610v3","updated":"2024-06-27T16:38:35Z","published":"2024-02-12T12:30:42Z","title":"Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping","summary":"  Self-alignment is an effective way to reduce the cost of human annotation\nwhile ensuring promising model capability. However, most current methods\ncomplete the data collection and training steps in a single round, which may\noverlook the continuously improving ability of self-aligned models. This gives\nrise to a key query: What if we do multi-time bootstrapping self-alignment?\nDoes this strategy enhance model performance or lead to rapid degradation? In\nthis paper, our pioneering exploration delves into the impact of bootstrapping\nself-alignment on large language models. Our findings reveal that bootstrapping\nself-alignment markedly surpasses the single-round approach, by guaranteeing\ndata diversity from in-context learning. To further exploit the capabilities of\nbootstrapping, we investigate and adjust the training order of data, which\nyields improved performance of the model. Drawing on these findings, we propose\nStep-On-Feet Tuning (SOFT) which leverages model's continuously enhanced\nfew-shot ability to boost zero or one-shot performance. Based on easy-to-hard\ntraining recipe, we propose SOFT+ which further boost self-alignment's\nperformance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across\nvarious classification and generation tasks, highlighting the potential of\nbootstrapping self-alignment on continually enhancing model alignment\nperformance.\n","authors":["Haoyu Wang","Guozheng Ma","Ziqiao Meng","Zeyu Qin","Li Shen","Zhong Zhang","Bingzhe Wu","Liu Liu","Yatao Bian","Tingyang Xu","Xueqian Wang","Peilin Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.07610v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19292v1","updated":"2024-06-27T16:05:13Z","published":"2024-06-27T16:05:13Z","title":"From Artificial Needles to Real Haystacks: Improving Retrieval\n  Capabilities in LLMs by Finetuning on Synthetic Data","summary":"  Recent studies have shown that Large Language Models (LLMs) struggle to\naccurately retrieve information and maintain reasoning capabilities when\nprocessing long-context inputs. To address these limitations, we propose a\nfinetuning approach utilizing a carefully designed synthetic dataset comprising\nnumerical key-value retrieval tasks. Our experiments on models like GPT-3.5\nTurbo and Mistral 7B demonstrate that finetuning LLMs on this dataset\nsignificantly improves LLMs' information retrieval and reasoning capabilities\nin longer-context settings. We present an analysis of the finetuned models,\nillustrating the transfer of skills from synthetic to real task evaluations\n(e.g., $10.5\\%$ improvement on $20$ documents MDQA at position $10$ for GPT-3.5\nTurbo). We also find that finetuned LLMs' performance on general benchmarks\nremains almost constant while LLMs finetuned on other baseline long-context\naugmentation data can encourage hallucination (e.g., on TriviaQA, Mistral 7B\nfinetuned on our synthetic data cause no performance drop while other baseline\ndata can cause a drop that ranges from $2.33\\%$ to $6.19\\%$). Our study\nhighlights the potential of finetuning on synthetic data for improving the\nperformance of LLMs on longer-context tasks.\n","authors":["Zheyang Xiong","Vasilis Papageorgiou","Kangwook Lee","Dimitris Papailiopoulos"],"pdf_url":"https://arxiv.org/pdf/2406.19292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.10253v4","updated":"2024-06-27T16:01:27Z","published":"2023-09-19T02:19:48Z","title":"GPTFUZZER: Red Teaming Large Language Models with Auto-Generated\n  Jailbreak Prompts","summary":"  Large language models (LLMs) have recently experienced tremendous popularity\nand are widely used from casual conversations to AI-driven programming.\nHowever, despite their considerable success, LLMs are not entirely reliable and\ncan give detailed guidance on how to conduct harmful or illegal activities.\nWhile safety measures can reduce the risk of such outputs, adversarial\njailbreak attacks can still exploit LLMs to produce harmful content. These\njailbreak templates are typically manually crafted, making large-scale testing\nchallenging.\n  In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing\nframework inspired by the AFL fuzzing framework. Instead of manual engineering,\nGPTFuzz automates the generation of jailbreak templates for red-teaming LLMs.\nAt its core, GPTFuzz starts with human-written templates as initial seeds, then\nmutates them to produce new templates. We detail three key components of\nGPTFuzz: a seed selection strategy for balancing efficiency and variability,\nmutate operators for creating semantically equivalent or similar sentences, and\na judgment model to assess the success of a jailbreak attack.\n  We evaluate GPTFuzz against various commercial and open-source LLMs,\nincluding ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our\nresults indicate that GPTFuzz consistently produces jailbreak templates with a\nhigh success rate, surpassing human-crafted templates. Remarkably, GPTFuzz\nachieves over 90% attack success rates against ChatGPT and Llama-2 models, even\nwith suboptimal initial seed templates. We anticipate that GPTFuzz will be\ninstrumental for researchers and practitioners in examining LLM robustness and\nwill encourage further exploration into enhancing LLM safety.\n","authors":["Jiahao Yu","Xingwei Lin","Zheng Yu","Xinyu Xing"],"pdf_url":"https://arxiv.org/pdf/2309.10253v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07683v3","updated":"2024-06-27T15:54:58Z","published":"2023-09-14T12:58:30Z","title":"Assessing the nature of large language models: A caution against\n  anthropocentrism","summary":"  Generative AI models garnered a large amount of public attention and\nspeculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion\ncamps exist: one excited about possibilities these models offer for fundamental\nchanges to human tasks, and another highly concerned about power these models\nseem to have. To address these concerns, we assessed several LLMs, primarily\nGPT 3.5, using standard, normed, and validated cognitive and personality\nmeasures. For this seedling project, we developed a battery of tests that\nallowed us to estimate the boundaries of some of these models capabilities, how\nstable those capabilities are over a short period of time, and how they compare\nto humans. Our results indicate that LLMs are unlikely to have developed\nsentience, although its ability to respond to personality inventories is\ninteresting. GPT3.5 did display large variability in both cognitive and\npersonality measures over repeated observations, which is not expected if it\nhad a human-like personality. Variability notwithstanding, LLMs display what in\na human would be considered poor mental health, including low self-esteem,\nmarked dissociation from reality, and in some cases narcissism and psychopathy,\ndespite upbeat and helpful responses.\n","authors":["Ann Speed"],"pdf_url":"https://arxiv.org/pdf/2309.07683v3.pdf","comment":"31 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.19280v1","updated":"2024-06-27T15:50:41Z","published":"2024-06-27T15:50:41Z","title":"HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale","summary":"  The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.\n","authors":["Junying Chen","Ruyi Ouyang","Anningzhe Gao","Shunian Chen","Guiming Hardy Chen","Xidong Wang","Ruifei Zhang","Zhenyang Cai","Ke Ji","Guangjun Yu","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.06530v3","updated":"2024-06-27T15:39:12Z","published":"2024-02-09T16:41:50Z","title":"Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite\n  Kernel Strategy in One-Class Classification","summary":"  Early detection of myocardial infarction (MI), a critical condition arising\nfrom coronary artery disease (CAD), is vital to prevent further myocardial\ndamage. This study introduces a novel method for early MI detection using a\none-class classification (OCC) algorithm in echocardiography. Our study\novercomes the challenge of limited echocardiography data availability by\nadopting a novel approach based on Multi-modal Subspace Support Vector Data\nDescription. The proposed technique involves a specialized MI detection\nframework employing multi-view echocardiography incorporating a composite\nkernel in the non-linear projection trick, fusing Gaussian and Laplacian\nsigmoid functions. Additionally, we enhance the update strategy of the\nprojection matrices by adapting maximization for both or one of the modalities\nin the optimization process. Our method boosts MI detection capability by\nefficiently transforming features extracted from echocardiography data into an\noptimized lower-dimensional subspace. The OCC model trained specifically on\ntarget class instances from the comprehensive HMC-QU dataset that includes\nmultiple echocardiography views indicates a marked improvement in MI detection\naccuracy. Our findings reveal that our proposed multi-view approach achieves a\ngeometric mean of 71.24%, signifying a substantial advancement in\nechocardiography-based MI diagnosis and offering more precise and efficient\ndiagnostic tools.\n","authors":["Muhammad Uzair Zahid","Aysen Degerli","Fahad Sohrab","Serkan Kiranyaz","Tahir Hamid","Rashid Mazhar","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2402.06530v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15847v3","updated":"2024-06-27T15:38:17Z","published":"2024-01-29T02:43:40Z","title":"Muffin or Chihuahua? Challenging Multimodal Large Language Models with\n  Multipanel VQA","summary":"  Multipanel images, commonly seen as web screenshots, posters, etc., pervade\nour daily lives. These images, characterized by their composition of multiple\nsubfigures in distinct layouts, effectively convey information to people.\nToward building advanced multimodal AI applications, such as agents that\nunderstand complex scenes and navigate through webpages, the skill of\nmultipanel visual reasoning is essential, and a comprehensive evaluation of\nmodels in this regard is important. Therefore, we introduce Multipanel Visual\nQuestion Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets\nof questions, answers, and multipanel images that specifically challenge models\nin comprehending multipanel images. Our evaluation shows that questions in the\nMultipanelVQA benchmark pose significant challenges to the state-of-the-art\nMultimodal Large Language Models (MLLMs) tested, even though humans can attain\napproximately 99% accuracy on these questions. Distinctively, the MultipanelVQA\nbenchmark features synthetically generated multipanel images specifically\ncrafted to isolate and assess the impact of various factors, such as the\nlayout, on MLLMs' multipanel image comprehension abilities. As a result, in\naddition to benchmarking the capabilities of MLLMs in understanding multipanel\nimages, we analyze various factors of the multipanel image that affect MLLMs'\nperformance with synthetic data and offer insights for enhancement. Code and\ndata are released at https://sites.google.com/view/multipanelvqa/home.\n","authors":["Yue Fan","Jing Gu","Kaiwen Zhou","Qianqi Yan","Shan Jiang","Ching-Chen Kuo","Xinze Guan","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2401.15847v3.pdf","comment":"ACL 2024"}]},"2024-06-26T00:00:00Z":{"Image and Video Processing":[{"id":"http://arxiv.org/abs/2201.12348v4","updated":"2024-06-26T23:10:02Z","published":"2022-01-28T01:31:49Z","title":"End-to-End Optimization of Metasurfaces for Imaging with Compressed\n  Sensing","summary":"  We present a framework for the end-to-end optimization of metasurface imaging\nsystems that reconstruct targets using compressed sensing, a technique for\nsolving underdetermined imaging problems when the target object exhibits\nsparsity (i.e. the object can be described by a small number of non-zero\nvalues, but the positions of these values are unknown). We nest an iterative,\nunapproximated compressed sensing reconstruction algorithm into our end-to-end\noptimization pipeline, resulting in an interpretable, data-efficient method for\nmaximally leveraging metaoptics to exploit object sparsity. We apply our\nframework to super-resolution imaging and high-resolution depth imaging with a\nphase-change material. In both situations, our end-to-end framework\ncomputationally discovers optimal metasurface structures for compressed sensing\nrecovery, automatically balancing a number of complicated design considerations\nto select an imaging measurement matrix from a complex, physically constrained\nmanifold with millions ofdimensions. The optimized metasurface imaging systems\nare robust to noise, significantly improving over random scattering surfaces\nand approaching the ideal compressed sensing performance of a Gaussian matrix,\nshowing how a physical metasurface system can demonstrably approach the\nmathematical limits of compressed sensing.\n","authors":["Gaurav Arya","William F. Li","Charles Roques-Carmes","Marin Soljačić","Steven G. Johnson","Zin Lin"],"pdf_url":"https://arxiv.org/pdf/2201.12348v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17173v2","updated":"2024-06-26T20:54:45Z","published":"2024-06-24T23:23:18Z","title":"Diff3Dformer: Leveraging Slice Sequence Diffusion for Enhanced 3D CT\n  Classification with Transformer Networks","summary":"  The manifestation of symptoms associated with lung diseases can vary in\ndifferent depths for individual patients, highlighting the significance of 3D\ninformation in CT scans for medical image classification. While Vision\nTransformer has shown superior performance over convolutional neural networks\nin image classification tasks, their effectiveness is often demonstrated on\nsufficiently large 2D datasets and they easily encounter overfitting issues on\nsmall medical image datasets. To address this limitation, we propose a\nDiffusion-based 3D Vision Transformer (Diff3Dformer), which utilizes the latent\nspace of the Diffusion model to form the slice sequence for 3D analysis and\nincorporates clustering attention into ViT to aggregate repetitive information\nwithin 3D CT scans, thereby harnessing the power of the advanced transformer in\n3D classification tasks on small datasets. Our method exhibits improved\nperformance on two different scales of small datasets of 3D lung CT scans,\nsurpassing the state of the art 3D methods and other transformer-based\napproaches that emerged during the COVID-19 pandemic, demonstrating its robust\nand superior performance across different scales of data. Experimental results\nunderscore the superiority of our proposed method, indicating its potential for\nenhancing medical image classification tasks in real-world scenarios.\n","authors":["Zihao Jin","Yingying Fang","Jiahao Huang","Caiwen Xu","Simon Walsh","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2406.17173v2.pdf","comment":"conference"},{"id":"http://arxiv.org/abs/2406.18508v1","updated":"2024-06-26T17:29:15Z","published":"2024-06-26T17:29:15Z","title":"Assessment of Clonal Hematopoiesis of Indeterminate Potential from\n  Cardiac Magnetic Resonance Imaging using Deep Learning in a Cardio-oncology\n  Population","summary":"  Background: We propose a novel method to identify who may likely have clonal\nhematopoiesis of indeterminate potential (CHIP), a condition characterized by\nthe presence of somatic mutations in hematopoietic stem cells without\ndetectable hematologic malignancy, using deep learning techniques. Methods: We\ndeveloped a convolutional neural network (CNN) to predict CHIP status using 4\ndifferent views from standard delayed gadolinium-enhanced cardiac magnetic\nresonance imaging (CMR). We used 5-fold cross validation on 82 cardio-oncology\npatients to assess the performance of our model. Different algorithms were\ncompared to find the optimal patient-level prediction method using the\nimage-level CNN predictions. Results: We found that the best model had an area\nunder the receiver operating characteristic curve of 0.85 and an accuracy of\n82%. Conclusions: We conclude that a deep learning-based diagnostic approach\nfor CHIP using CMR is promising.\n","authors":["Sangeon Ryu","Shawn Ahn","Jeacy Espinoza","Alokkumar Jha","Stephanie Halene","James S. Duncan","Jennifer M Kwan","Nicha C. Dvornek"],"pdf_url":"https://arxiv.org/pdf/2406.18508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18628v1","updated":"2024-06-26T16:58:15Z","published":"2024-06-26T16:58:15Z","title":"IDA-UIE: An Iterative Framework for Deep Network-based Degradation Aware\n  Underwater Image Enhancement","summary":"  Underwater image quality is affected by fluorescence, low illumination,\nabsorption, and scattering. Recent works in underwater image enhancement have\nproposed different deep network architectures to handle these problems. Most of\nthese works have proposed a single network to handle all the challenges. We\nbelieve that deep networks trained for specific conditions deliver better\nperformance than a single network learned from all degradation cases.\nAccordingly, the first contribution of this work lies in the proposal of an\niterative framework where a single dominant degradation condition is identified\nand resolved. This proposal considers the following eight degradation\nconditions -- low illumination, low contrast, haziness, blurred image, presence\nof noise and color imbalance in three different channels. A deep network is\ndesigned to identify the dominant degradation condition. Accordingly, an\nappropriate deep network is selected for degradation condition-specific\nenhancement. The second contribution of this work is the construction of\ndegradation condition specific datasets from good quality images of two\nstandard datasets (UIEB and EUVP). This dataset is used to learn the condition\nspecific enhancement networks. The proposed approach is found to outperform\nnine baseline methods on UIEB and EUVP datasets.\n","authors":["Pranjali Singh","Prithwijit Guha"],"pdf_url":"https://arxiv.org/pdf/2406.18628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.15613v3","updated":"2024-06-26T15:47:02Z","published":"2024-01-28T10:00:45Z","title":"Towards Arbitrary-Scale Histopathology Image Super-resolution: An\n  Efficient Dual-branch Framework via Implicit Self-texture Enhancement","summary":"  High-quality whole-slide scanners are expensive, complex, and time-consuming,\nthus limiting the acquisition and utilization of high-resolution pathology\nwhole-slide images in daily clinical work. Deep learning-based single-image\nsuper-resolution techniques are an effective way to solve this problem by\nsynthesizing high-resolution images from low-resolution ones. However, the\nexisting super-resolution models applied in pathology images can only work in\nfixed integer magnifications, significantly decreasing their applicability.\nThough methods based on implicit neural representation have shown promising\nresults in arbitrary-scale super-resolution of natural images, applying them\ndirectly to pathology images is inadequate because they have unique\nfine-grained image textures different from natural images. Thus, we propose an\nImplicit Self-Texture Enhancement-based dual-branch framework (ISTE) for\narbitrary-scale super-resolution of pathology images to address this challenge.\nISTE contains a pixel learning branch and a texture learning branch, which\nfirst learn pixel features and texture features, respectively. Then, we design\na two-stage texture enhancement strategy to fuse the features from the two\nbranches to obtain the super-resolution results, where the first stage is\nfeature-based texture enhancement, and the second stage is spatial-domain-based\ntexture enhancement. Extensive experiments on three public datasets show that\nISTE outperforms existing fixed-scale and arbitrary-scale algorithms at\nmultiple magnifications and helps to improve downstream task performance. To\nthe best of our knowledge, this is the first work to achieve arbitrary-scale\nsuper-resolution in pathology images. Codes will be available.\n","authors":["Minghong Duan","Linhao Qu","Zhiwei Yang","Manning Wang","Chenxi Zhang","Zhijian Song"],"pdf_url":"https://arxiv.org/pdf/2401.15613v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.05622v2","updated":"2024-06-26T15:34:47Z","published":"2022-11-10T14:54:31Z","title":"InstantGroup: Instant Template Generation for Scalable Group of Brain\n  MRI Registration","summary":"  Template generation is a critical step in groupwise image registration, which\ninvolves aligning a group of subjects into a common space. While existing\nmethods can generate high-quality template images, they often incur substantial\ntime costs or are limited by fixed group scales. In this paper, we present\nInstantGroup, an efficient groupwise template generation framework based on\nvariational autoencoder (VAE) models that leverage latent representations'\narithmetic properties, enabling scalability to groups of any size. InstantGroup\nfeatures a Dual VAEs backbone with shared-weight twin networks to handle pairs\nof inputs and incorporates a Displacement Inversion Module (DIM) to maintain\ntemplate unbiasedness and a Subject-Template Alignment Module (STAM) to improve\ntemplate quality and registration accuracy. Experiments on 3D brain MRI scans\nfrom the OASIS and ADNI datasets reveal that InstantGroup dramatically reduces\nruntime, generating templates within seconds for various group sizes while\nmaintaining superior performance compared to state-of-the-art baselines on\nquantitative metrics, including unbiasedness and registration accuracy.\n","authors":["Ziyi He","Albert C. S. Chung"],"pdf_url":"https://arxiv.org/pdf/2211.05622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18422v1","updated":"2024-06-26T15:18:20Z","published":"2024-06-26T15:18:20Z","title":"Repeat and Concatenate: 2D to 3D Image Translation with 3D to 3D\n  Generative Modeling","summary":"  This paper investigates a 2D to 3D image translation method with a\nstraightforward technique, enabling correlated 2D X-ray to 3D CT-like\nreconstruction. We observe that existing approaches, which integrate\ninformation across multiple 2D views in the latent space, lose valuable signal\ninformation during latent encoding. Instead, we simply repeat and concatenate\nthe 2D views into higher-channel 3D volumes and approach the 3D reconstruction\nchallenge as a straightforward 3D to 3D generative modeling problem,\nsidestepping several complex modeling issues. This method enables the\nreconstructed 3D volume to retain valuable information from the 2D inputs,\nwhich are passed between channel states in a Swin UNETR backbone. Our approach\napplies neural optimal transport, which is fast and stable to train,\neffectively integrating signal information across multiple views without the\nrequirement for precise alignment; it produces non-collapsed reconstructions\nthat are highly faithful to the 2D views, even after limited training. We\ndemonstrate correlated results, both qualitatively and quantitatively, having\ntrained our model on a single dataset and evaluated its generalization ability\nacross six datasets, including out-of-distribution samples.\n","authors":["Abril Corona-Figueroa","Hubert P. H. Shum","Chris G. Willcocks"],"pdf_url":"https://arxiv.org/pdf/2406.18422v1.pdf","comment":"CVPRW 2024 - DCA in MI; Best Paper Award"},{"id":"http://arxiv.org/abs/2212.13459v2","updated":"2024-06-26T13:59:56Z","published":"2022-12-27T12:03:38Z","title":"Scaling Painting Style Transfer","summary":"  Neural style transfer (NST) is a deep learning technique that produces an\nunprecedentedly rich style transfer from a style image to a content image. It\nis particularly impressive when it comes to transferring style from a painting\nto an image. NST was originally achieved by solving an optimization problem to\nmatch the global statistics of the style image while preserving the local\ngeometric features of the content image. The two main drawbacks of this\noriginal approach is that it is computationally expensive and that the\nresolution of the output images is limited by high GPU memory requirements.\nMany solutions have been proposed to both accelerate NST and produce images\nwith larger size. However, our investigation shows that these accelerated\nmethods all compromise the quality of the produced images in the context of\npainting style transfer. Indeed, transferring the style of a painting is a\ncomplex task involving features at different scales, from the color palette and\ncompositional style to the fine brushstrokes and texture of the canvas. This\npaper provides a solution to solve the original global optimization for\nultra-high resolution (UHR) images, enabling multiscale NST at unprecedented\nimage sizes. This is achieved by spatially localizing the computation of each\nforward and backward passes through the VGG network. Extensive qualitative and\nquantitative comparisons, as well as a \\textcolor{coverletter}{perceptual\nstudy}, show that our method produces style transfer of unmatched quality for\nsuch high-resolution painting styles. By a careful comparison, we show that\nstate-of-the-art fast methods are still prone to artifacts, thus suggesting\nthat fast painting style transfer remains an open problem. Source code is\navailable at https://github.com/bgalerne/scaling_painting_style_transfer.\n","authors":["Bruno Galerne","Lara Raad","José Lezama","Jean-Michel Morel"],"pdf_url":"https://arxiv.org/pdf/2212.13459v2.pdf","comment":"14 pages, 9 figures, 4 tables, accepted at EGSR 2024"},{"id":"http://arxiv.org/abs/2406.18350v1","updated":"2024-06-26T13:51:57Z","published":"2024-06-26T13:51:57Z","title":"On Reducing Activity with Distillation and Regularization for Energy\n  Efficient Spiking Neural Networks","summary":"  Interest in spiking neural networks (SNNs) has been growing steadily,\npromising an energy-efficient alternative to formal neural networks (FNNs),\ncommonly known as artificial neural networks (ANNs). Despite increasing\ninterest, especially for Edge applications, these event-driven neural networks\nsuffered from their difficulty to be trained compared to FNNs. To alleviate\nthis problem, a number of innovative methods have been developed to provide\nperformance more or less equivalent to that of FNNs. However, the spiking\nactivity of a network during inference is usually not considered. While SNNs\nmay usually have performance comparable to that of FNNs, it is often at the\ncost of an increase of the network's activity, thus limiting the benefit of\nusing them as a more energy-efficient solution.\n  In this paper, we propose to leverage Knowledge Distillation (KD) for SNNs\ntraining with surrogate gradient descent in order to optimize the trade-off\nbetween performance and spiking activity. Then, after understanding why KD led\nto an increase in sparsity, we also explored Activations regularization and\nproposed a novel method with Logits Regularization. These approaches, validated\non several datasets, clearly show a reduction in network spiking activity\n(-26.73% on GSC and -14.32% on CIFAR-10) while preserving accuracy.\n","authors":["Thomas Louis","Benoit Miramond","Alain Pegatoquet","Adrien Girard"],"pdf_url":"https://arxiv.org/pdf/2406.18350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18327v1","updated":"2024-06-26T13:14:24Z","published":"2024-06-26T13:14:24Z","title":"Multi-modal Evidential Fusion Network for Trusted PET/CT Tumor\n  Segmentation","summary":"  Accurate segmentation of tumors in PET/CT images is important in\ncomputer-aided diagnosis and treatment of cancer. The key issue of such a\nsegmentation problem lies in the effective integration of complementary\ninformation from PET and CT images. However, the quality of PET and CT images\nvaries widely in clinical settings, which leads to uncertainty in the modality\ninformation extracted by networks. To take the uncertainty into account in\nmulti-modal information fusion, this paper proposes a novel Multi-modal\nEvidential Fusion Network (MEFN) comprising a Cross-Modal Feature Learning\n(CFL) module and a Multi-modal Trusted Fusion (MTF) module. The CFL module\nreduces the domain gap upon modality conversion and highlights common tumor\nfeatures, thereby alleviating the needs of the segmentation module to handle\nmodality specificity. The MTF module utilizes mutual attention mechanisms and\nan uncertainty calibrator to fuse modality features based on modality\nuncertainty and then fuse the segmentation results under the guidance of\nDempster-Shafer Theory. Besides, a new uncertainty perceptual loss is\nintroduced to force the model focusing on uncertain features and hence improve\nits ability to extract trusted modality information. Extensive comparative\nexperiments are conducted on two publicly available PET/CT datasets to evaluate\nthe performance of our proposed method whose results demonstrate that our MEFN\nsignificantly outperforms state-of-the-art methods with improvements of 2.15%\nand 3.23% in DSC scores on the AutoPET dataset and the Hecktor dataset,\nrespectively. More importantly, our model can provide radiologists with\ncredible uncertainty of the segmentation results for their decision in\naccepting or rejecting the automatic segmentation results, which is\nparticularly important for clinical applications. Our code will be available at\nhttps://github.com/QPaws/MEFN.\n","authors":["Yuxuan Qi","Li Lin","Jiajun Wang","Jingya Zhang","Bin Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.18327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18310v1","updated":"2024-06-26T12:50:10Z","published":"2024-06-26T12:50:10Z","title":"Spatial-temporal Hierarchical Reinforcement Learning for Interpretable\n  Pathology Image Super-Resolution","summary":"  Pathology image are essential for accurately interpreting lesion cells in\ncytopathology screening, but acquiring high-resolution digital slides requires\nspecialized equipment and long scanning times. Though super-resolution (SR)\ntechniques can alleviate this problem, existing deep learning models recover\npathology image in a black-box manner, which can lead to untruthful biological\ndetails and misdiagnosis. Additionally, current methods allocate the same\ncomputational resources to recover each pixel of pathology image, leading to\nthe sub-optimal recovery issue due to the large variation of pathology image.\nIn this paper, we propose the first hierarchical reinforcement learning\nframework named Spatial-Temporal hierARchical Reinforcement Learning (STAR-RL),\nmainly for addressing the aforementioned issues in pathology image\nsuper-resolution problem. We reformulate the SR problem as a Markov decision\nprocess of interpretable operations and adopt the hierarchical recovery\nmechanism in patch level, to avoid sub-optimal recovery. Specifically, the\nhigher-level spatial manager is proposed to pick out the most corrupted patch\nfor the lower-level patch worker. Moreover, the higher-level temporal manager\nis advanced to evaluate the selected patch and determine whether the\noptimization should be stopped earlier, thereby avoiding the over-processed\nproblem. Under the guidance of spatial-temporal managers, the lower-level patch\nworker processes the selected patch with pixel-wise interpretable actions at\neach time step. Experimental results on medical images degraded by different\nkernels show the effectiveness of STAR-RL. Furthermore, STAR-RL validates the\npromotion in tumor diagnosis with a large margin and shows generalizability\nunder various degradations. The source code is available at\nhttps://github.com/CUHK-AIM-Group/STAR-RL.\n","authors":["Wenting Chen","Jie Liu","Tommy W. S. Chow","Yixuan Yuan"],"pdf_url":"https://arxiv.org/pdf/2406.18310v1.pdf","comment":"Accepted to IEEE TRANSACTIONS ON MEDICAL IMAGING (TMI)"},{"id":"http://arxiv.org/abs/2406.18278v1","updated":"2024-06-26T12:04:09Z","published":"2024-06-26T12:04:09Z","title":"Generalized Deepfake Attribution","summary":"  The landscape of fake media creation changed with the introduction of\nGenerative Adversarial Networks (GAN s). Fake media creation has been on the\nrise with the rapid advances in generation technology, leading to new\nchallenges in Detecting fake media. A fundamental characteristic of GAN s is\ntheir sensitivity to parameter initialization, known as seeds. Each distinct\nseed utilized during training leads to the creation of unique model instances,\nresulting in divergent image outputs despite employing the same architecture.\nThis means that even if we have one GAN architecture, it can produce countless\nvariations of GAN models depending on the seed used. Existing methods for\nattributing deepfakes work well only if they have seen the specific GAN model\nduring training. If the GAN architectures are retrained with a different seed,\nthese methods struggle to attribute the fakes. This seed dependency issue made\nit difficult to attribute deepfakes with existing methods. We proposed a\ngeneralized deepfake attribution network (GDA-N et) to attribute fake images to\ntheir respective GAN architectures, even if they are generated from a retrained\nversion of the GAN architecture with a different seed (cross-seed) or from the\nfine-tuned version of the existing GAN model. Extensive experiments on\ncross-seed and fine-tuned data of GAN models show that our method is highly\neffective compared to existing methods. We have provided the source code to\nvalidate our results.\n","authors":["Sowdagar Mahammad Shahid","Sudev Kumar Padhi","Umesh Kashyap","Sk. Subidh Ali"],"pdf_url":"https://arxiv.org/pdf/2406.18278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18247v1","updated":"2024-06-26T10:49:26Z","published":"2024-06-26T10:49:26Z","title":"Generative artificial intelligence in ophthalmology: multimodal retinal\n  images for the diagnosis of Alzheimer's disease with convolutional neural\n  networks","summary":"  Background/Aim. This study aims to predict Amyloid Positron Emission\nTomography (AmyloidPET) status with multimodal retinal imaging and\nconvolutional neural networks (CNNs) and to improve the performance through\npretraining with synthetic data. Methods. Fundus autofluorescence, optical\ncoherence tomography (OCT), and OCT angiography images from 328 eyes of 59\nAmyloidPET positive subjects and 108 AmyloidPET negative subjects were used for\nclassification. Denoising Diffusion Probabilistic Models (DDPMs) were trained\nto generate synthetic images and unimodal CNNs were pretrained on synthetic\ndata and finetuned on real data or trained solely on real data. Multimodal\nclassifiers were developed to combine predictions of the four unimodal CNNs\nwith patient metadata. Class activation maps of the unimodal classifiers\nprovided insight into the network's attention to inputs. Results. DDPMs\ngenerated diverse, realistic images without memorization. Pretraining unimodal\nCNNs with synthetic data improved AUPR at most from 0.350 to 0.579. Integration\nof metadata in multimodal CNNs improved AUPR from 0.486 to 0.634, which was the\nbest overall best classifier. Class activation maps highlighted relevant\nretinal regions which correlated with AD. Conclusion. Our method for generating\nand leveraging synthetic data has the potential to improve AmyloidPET\nprediction from multimodal retinal imaging. A DDPM can generate realistic and\nunique multimodal synthetic retinal images. Our best performing unimodal and\nmultimodal classifiers were not pretrained on synthetic data, however\npretraining with synthetic data slightly improved classification performance\nfor two out of the four modalities.\n","authors":["I. R. Slootweg","M. Thach","K. R. Curro-Tafili","F. D. Verbraak","F. H. Bouwman","Y. A. L. Pijnenburg","J. F. Boer","J. H. P. de Kwisthout","L. Bagheriye","P. J. González"],"pdf_url":"https://arxiv.org/pdf/2406.18247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18242v1","updated":"2024-06-26T10:46:44Z","published":"2024-06-26T10:46:44Z","title":"ConStyle v2: A Strong Prompter for All-in-One Image Restoration","summary":"  This paper introduces ConStyle v2, a strong plug-and-play prompter designed\nto output clean visual prompts and assist U-Net Image Restoration models in\nhandling multiple degradations. The joint training process of IRConStyle, an\nImage Restoration framework consisting of ConStyle and a general restoration\nnetwork, is divided into two stages: first, pre-training ConStyle alone, and\nthen freezing its weights to guide the training of the general restoration\nnetwork. Three improvements are proposed in the pre-training stage to train\nConStyle: unsupervised pre-training, adding a pretext task (i.e.\nclassification), and adopting knowledge distillation. Without bells and\nwhistles, we can get ConStyle v2, a strong prompter for all-in-one Image\nRestoration, in less than two GPU days and doesn't require any fine-tuning.\nExtensive experiments on Restormer (transformer-based), NAFNet (CNN-based),\nMAXIM-1S (MLP-based), and a vanilla CNN network demonstrate that ConStyle v2\ncan enhance any U-Net style Image Restoration models to all-in-one Image\nRestoration models. Furthermore, models guided by the well-trained ConStyle v2\nexhibit superior performance in some specific degradation compared to ConStyle.\n","authors":["Dongqi Fan","Junhao Zhang","Liang Chang"],"pdf_url":"https://arxiv.org/pdf/2406.18242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03425v5","updated":"2024-06-26T10:38:29Z","published":"2024-04-04T13:06:25Z","title":"ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State\n  Space Model","summary":"  Convolutional neural networks (CNN) and Transformers have made impressive\nprogress in the field of remote sensing change detection (CD). However, both\narchitectures have inherent shortcomings: CNN are constrained by a limited\nreceptive field that may hinder their ability to capture broader spatial\ncontexts, while Transformers are computationally intensive, making them costly\nto train and deploy on large datasets. Recently, the Mamba architecture, based\non state space models, has shown remarkable performance in a series of natural\nlanguage processing tasks, which can effectively compensate for the\nshortcomings of the above two architectures. In this paper, we explore for the\nfirst time the potential of the Mamba architecture for remote sensing CD tasks.\nWe tailor the corresponding frameworks, called MambaBCD, MambaSCD, and\nMambaBDA, for binary change detection (BCD), semantic change detection (SCD),\nand building damage assessment (BDA), respectively. All three frameworks adopt\nthe cutting-edge Visual Mamba architecture as the encoder, which allows full\nlearning of global spatial contextual information from the input images. For\nthe change decoder, which is available in all three architectures, we propose\nthree spatio-temporal relationship modeling mechanisms, which can be naturally\ncombined with the Mamba architecture and fully utilize its attribute to achieve\nspatio-temporal interaction of multi-temporal features, thereby obtaining\naccurate change information. On five benchmark datasets, our proposed\nframeworks outperform current CNN- and Transformer-based approaches without\nusing any complex training strategies or tricks, fully demonstrating the\npotential of the Mamba architecture in CD tasks. Further experiments show that\nour architecture is quite robust to degraded data. The source code will be\navailable in https://github.com/ChenHongruixuan/MambaCD\n","authors":["Hongruixuan Chen","Jian Song","Chengxi Han","Junshi Xia","Naoto Yokoya"],"pdf_url":"https://arxiv.org/pdf/2404.03425v5.pdf","comment":"Accepted by IEEE TGRS"},{"id":"http://arxiv.org/abs/2208.12880v4","updated":"2024-06-26T10:16:08Z","published":"2022-08-26T22:17:52Z","title":"Neuromorphic Visual Scene Understanding with Resonator Networks","summary":"  Analyzing a visual scene by inferring the configuration of a generative model\nis widely considered the most flexible and generalizable approach to scene\nunderstanding. Yet, one major problem is the computational challenge of the\ninference procedure, involving a combinatorial search across object identities\nand poses. Here we propose a neuromorphic solution exploiting three key\nconcepts: (1) a computational framework based on Vector Symbolic Architectures\n(VSA) with complex-valued vectors; (2) the design of Hierarchical Resonator\nNetworks (HRN) to factorize the non-commutative transforms translation and\nrotation in visual scenes; (3) the design of a multi-compartment spiking phasor\nneuron model for implementing complex-valued resonator networks on neuromorphic\nhardware. The VSA framework uses vector binding operations to form a generative\nimage model in which binding acts as the equivariant operation for geometric\ntransformations. A scene can, therefore, be described as a sum of vector\nproducts, which can then be efficiently factorized by a resonator network to\ninfer objects and their poses. The HRN features a partitioned architecture in\nwhich vector binding is equivariant for horizontal and vertical translation\nwithin one partition and for rotation and scaling within the other partition.\nThe spiking neuron model allows mapping the resonator network onto efficient\nand low-power neuromorphic hardware. Our approach is demonstrated on synthetic\nscenes composed of simple 2D shapes undergoing rigid geometric transformations\nand color changes. A companion paper demonstrates the same approach in\nreal-world application scenarios for machine vision and robotics.\n","authors":["Alpha Renner","Lazar Supic","Andreea Danielescu","Giacomo Indiveri","Bruno A. Olshausen","Yulia Sandamirskaya","Friedrich T. Sommer","E. Paxon Frady"],"pdf_url":"https://arxiv.org/pdf/2208.12880v4.pdf","comment":"23 pages, 8 figures, minor revisions and extended supplementary\n  material"},{"id":"http://arxiv.org/abs/2406.18212v1","updated":"2024-06-26T09:56:29Z","published":"2024-06-26T09:56:29Z","title":"Joint Stream: Malignant Region Learning for Breast Cancer Diagnosis","summary":"  Early diagnosis of breast cancer (BC) significantly contributes to reducing\nthe mortality rate worldwide. The detection of different factors and biomarkers\nsuch as Estrogen receptor (ER), Progesterone receptor (PR), Human epidermal\ngrowth factor receptor 2 (HER2) gene, Histological grade (HG), Auxiliary lymph\nnode (ALN) status, and Molecular subtype (MS) can play a significant role in\nimproved BC diagnosis. However, the existing methods predict only a single\nfactor which makes them less suitable to use in diagnosis and designing a\nstrategy for treatment. In this paper, we propose to classify the six essential\nindicating factors (ER, PR, HER2, ALN, HG, MS) for early BC diagnosis using\nH\\&E stained WSI's. To precisely capture local neighboring relationships, we\nuse spatial and frequency domain information from the large patch size of WSI's\nmalignant regions. Furthermore, to cater the variable number of regions of\ninterest sizes and give due attention to each region, we propose a malignant\nregion learning attention network. Our experimental results demonstrate that\ncombining spatial and frequency information using the malignant region learning\nmodule significantly improves multi-factor and single-factor classification\nperformance on publicly available datasets.\n","authors":["Abdul Rehman","Sarfaraz Hussein","Waqas Sultani"],"pdf_url":"https://arxiv.org/pdf/2406.18212v1.pdf","comment":"Under Review (Biomedical Signal Processing and Control)"},{"id":"http://arxiv.org/abs/2406.17536v2","updated":"2024-06-26T09:52:47Z","published":"2024-06-25T13:20:39Z","title":"MedMNIST-C: Comprehensive benchmark and improved classifier robustness\n  by simulating realistic image corruptions","summary":"  The integration of neural-network-based systems into clinical practice is\nlimited by challenges related to domain generalization and robustness. The\ncomputer vision community established benchmarks such as ImageNet-C as a\nfundamental prerequisite to measure progress towards those challenges. Similar\ndatasets are largely absent in the medical imaging community which lacks a\ncomprehensive benchmark that spans across imaging modalities and applications.\nTo address this gap, we create and open-source MedMNIST-C, a benchmark dataset\nbased on the MedMNIST+ collection covering 12 datasets and 9 imaging\nmodalities. We simulate task and modality-specific image corruptions of varying\nseverity to comprehensively evaluate the robustness of established algorithms\nagainst real-world artifacts and distribution shifts. We further provide\nquantitative evidence that our simple-to-use artificial corruptions allow for\nhighly performant, lightweight data augmentation to enhance model robustness.\nUnlike traditional, generic augmentation strategies, our approach leverages\ndomain knowledge, exhibiting significantly higher robustness when compared to\nwidely adopted methods. By introducing MedMNIST-C and open-sourcing the\ncorresponding library allowing for targeted data augmentations, we contribute\nto the development of increasingly robust methods tailored to the challenges of\nmedical imaging. The code is available at\nhttps://github.com/francescodisalvo05/medmnistc-api}{github.com/francescodisalvo05/medmnistc-api .\n","authors":["Francesco Di Salvo","Sebastian Doerrich","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2406.17536v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18201v1","updated":"2024-06-26T09:33:51Z","published":"2024-06-26T09:33:51Z","title":"EFCNet: Every Feature Counts for Small Medical Object Segmentation","summary":"  This paper explores the segmentation of very small medical objects with\nsignificant clinical value. While Convolutional Neural Networks (CNNs),\nparticularly UNet-like models, and recent Transformers have shown substantial\nprogress in image segmentation, our empirical findings reveal their poor\nperformance in segmenting the small medical objects and lesions concerned in\nthis paper. This limitation may be attributed to information loss during their\nencoding and decoding process. In response to this challenge, we propose a\nnovel model named EFCNet for small object segmentation in medical images. Our\nmodel incorporates two modules: the Cross-Stage Axial Attention Module (CSAA)\nand the Multi-Precision Supervision Module (MPS). These modules address\ninformation loss during encoding and decoding procedures, respectively.\nSpecifically, CSAA integrates features from all stages of the encoder to\nadaptively learn suitable information needed in different decoding stages,\nthereby reducing information loss in the encoder. On the other hand, MPS\nintroduces a novel multi-precision supervision mechanism to the decoder. This\nmechanism prioritizes attention to low-resolution features in the initial\nstages of the decoder, mitigating information loss caused by subsequent\nconvolution and sampling processes and enhancing the model's global perception.\nWe evaluate our model on two benchmark medical image datasets. The results\ndemonstrate that EFCNet significantly outperforms previous segmentation methods\ndesigned for both medical and normal images.\n","authors":["Lingjie Kong","Qiaoling Wei","Chengming Xu","Han Chen","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2406.18201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05404v3","updated":"2024-06-26T08:10:43Z","published":"2023-08-10T07:53:06Z","title":"Enhancing Low-light Light Field Images with A Deep Compensation\n  Unfolding Network","summary":"  This paper presents a novel and interpretable end-to-end learning framework,\ncalled the deep compensation unfolding network (DCUNet), for restoring light\nfield (LF) images captured under low-light conditions. DCUNet is designed with\na multi-stage architecture that mimics the optimization process of solving an\ninverse imaging problem in a data-driven fashion. The framework uses the\nintermediate enhanced result to estimate the illumination map, which is then\nemployed in the unfolding process to produce a new enhanced result.\nAdditionally, DCUNet includes a content-associated deep compensation module at\neach optimization stage to suppress noise and illumination map estimation\nerrors. To properly mine and leverage the unique characteristics of LF images,\nthis paper proposes a pseudo-explicit feature interaction module that\ncomprehensively exploits redundant information in LF images. The experimental\nresults on both simulated and real datasets demonstrate the superiority of our\nDCUNet over state-of-the-art methods, both qualitatively and quantitatively.\nMoreover, DCUNet preserves the essential geometric structure of enhanced LF\nimages much better. The code will be publicly available at\nhttps://github.com/lyuxianqiang/LFLL-DCU.\n","authors":["Xianqiang Lyu","Junhui Hou"],"pdf_url":"https://arxiv.org/pdf/2308.05404v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18102v1","updated":"2024-06-26T06:39:11Z","published":"2024-06-26T06:39:11Z","title":"A Lung Nodule Dataset with Histopathology-based Cancer Type Annotation","summary":"  Recently, Computer-Aided Diagnosis (CAD) systems have emerged as\nindispensable tools in clinical diagnostic workflows, significantly alleviating\nthe burden on radiologists. Nevertheless, despite their integration into\nclinical settings, CAD systems encounter limitations. Specifically, while CAD\nsystems can achieve high performance in the detection of lung nodules, they\nface challenges in accurately predicting multiple cancer types. This limitation\ncan be attributed to the scarcity of publicly available datasets annotated with\nexpert-level cancer type information. This research aims to bridge this gap by\nproviding publicly accessible datasets and reliable tools for medical\ndiagnosis, facilitating a finer categorization of different types of lung\ndiseases so as to offer precise treatment recommendations. To achieve this\nobjective, we curated a diverse dataset of lung Computed Tomography (CT)\nimages, comprising 330 annotated nodules (nodules are labeled as bounding\nboxes) from 95 distinct patients. The quality of the dataset was evaluated\nusing a variety of classical classification and detection models, and these\npromising results demonstrate that the dataset has a feasible application and\nfurther facilitate intelligent auxiliary diagnosis.\n","authors":["Muwei Jian","Hongyu Chen","Zaiyong Zhang","Nan Yang","Haorang Zhang","Lifu Ma","Wenjing Xu","Huixiang Zhi"],"pdf_url":"https://arxiv.org/pdf/2406.18102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18079v1","updated":"2024-06-26T05:31:36Z","published":"2024-06-26T05:31:36Z","title":"MFDNet: Multi-Frequency Deflare Network for Efficient Nighttime Flare\n  Removal","summary":"  When light is scattered or reflected accidentally in the lens, flare\nartifacts may appear in the captured photos, affecting the photos' visual\nquality. The main challenge in flare removal is to eliminate various flare\nartifacts while preserving the original content of the image. To address this\nchallenge, we propose a lightweight Multi-Frequency Deflare Network (MFDNet)\nbased on the Laplacian Pyramid. Our network decomposes the flare-corrupted\nimage into low and high-frequency bands, effectively separating the\nillumination and content information in the image. The low-frequency part\ntypically contains illumination information, while the high-frequency part\ncontains detailed content information. So our MFDNet consists of two main\nmodules: the Low-Frequency Flare Perception Module (LFFPM) to remove flare in\nthe low-frequency part and the Hierarchical Fusion Reconstruction Module (HFRM)\nto reconstruct the flare-free image. Specifically, to perceive flare from a\nglobal perspective while retaining detailed information for image restoration,\nLFFPM utilizes Transformer to extract global information while utilizing a\nconvolutional neural network to capture detailed local features. Then HFRM\ngradually fuses the outputs of LFFPM with the high-frequency component of the\nimage through feature aggregation. Moreover, our MFDNet can reduce the\ncomputational cost by processing in multiple frequency bands instead of\ndirectly removing the flare on the input image. Experimental results\ndemonstrate that our approach outperforms state-of-the-art methods in removing\nnighttime flare on real-world and synthetic images from the Flare7K dataset.\nFurthermore, the computational complexity of our model is remarkably low.\n","authors":["Yiguo Jiang","Xuhang Chen","Chi-Man Pun","Shuqiang Wang","Wei Feng"],"pdf_url":"https://arxiv.org/pdf/2406.18079v1.pdf","comment":"Accepted by The Visual Computer journal"},{"id":"http://arxiv.org/abs/2406.18063v1","updated":"2024-06-26T04:49:34Z","published":"2024-06-26T04:49:34Z","title":"Data-driven imaging geometric recovery of ultrahigh resolution robotic\n  micro-CT for in-vivo and other applications","summary":"  We introduce an ultrahigh-resolution (50\\mu m\\) robotic micro-CT design for\nlocalized imaging of carotid plaques using robotic arms, cutting-edge detector,\nand machine learning technologies. To combat geometric error-induced artifacts\nin interior CT scans, we propose a data-driven geometry estimation method that\nmaximizes the consistency between projection data and the reprojection\ncounterparts of a reconstructed volume. Particularly, we use a normalized cross\ncorrelation metric to overcome the projection truncation effect. Our approach\nis validated on a robotic CT scan of a sacrificed mouse and a micro-CT phantom\nscan, both producing sharper images with finer details than that prior\ncorrection.\n","authors":["Mengzhou Li","Guibin Zan","Wenbin Yun","Josef Uher","John Wen","Ge Wang"],"pdf_url":"https://arxiv.org/pdf/2406.18063v1.pdf","comment":"4-page paper for 8th International Conference on Computational and\n  Mathematical Biomedical Engineering"},{"id":"http://arxiv.org/abs/2406.18054v1","updated":"2024-06-26T04:12:34Z","published":"2024-06-26T04:12:34Z","title":"Leveraging Pre-trained Models for FF-to-FFPE Histopathological Image\n  Translation","summary":"  The two primary types of Hematoxylin and Eosin (H&E) slides in histopathology\nare Formalin-Fixed Paraffin-Embedded (FFPE) and Fresh Frozen (FF). FFPE slides\noffer high quality histopathological images but require a labor-intensive\nacquisition process. In contrast, FF slides can be prepared quickly, but the\nimage quality is relatively poor. Our task is to translate FF images into FFPE\nstyle, thereby improving the image quality for diagnostic purposes. In this\npaper, we propose Diffusion-FFPE, a method for FF-to-FFPE histopathological\nimage translation using a pre-trained diffusion model. Specifically, we employ\na one-step diffusion model as the generator and fine-tune it with LoRA adapters\nusing adversarial learning objectives. To ensure that the model effectively\ncaptures both global structural information and local details, we propose a\nmulti-scale feature fusion (MFF) module. This module utilizes two VAE encoders\nto extract features of varying image sizes and performs feature fusion before\nfeeding them into the UNet. Furthermore, we utilize a pre-trained\nvision-language model for histopathology as the backbone for the discriminator\nto further improve performance We conducted FF-to-FFPE translation experiments\non the TCGA-NSCLC datasets, and our method achieved better performance compared\nto other methods. The code and models are released at\nhttps://github.com/QilaiZhang/Diffusion-FFPE.\n","authors":["Qilai Zhang","Jiawen Li","Peiran Liao","Jiali Hu","Tian Guan","Anjia Han","Yonghong He"],"pdf_url":"https://arxiv.org/pdf/2406.18054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.18018v1","updated":"2024-06-26T02:12:42Z","published":"2024-06-26T02:12:42Z","title":"A Cross Spatio-Temporal Pathology-based Lung Nodule Dataset","summary":"  Recently, intelligent analysis of lung nodules with the assistant of computer\naided detection (CAD) techniques can improve the accuracy rate of lung cancer\ndiagnosis. However, existing CAD systems and pulmonary datasets mainly focus on\nComputed Tomography (CT) images from one single period, while ignoring the\ncross spatio-temporal features associated with the progression of nodules\ncontained in imaging data from various captured periods of lung cancer. If the\nevolution patterns of nodules across various periods in the patients' CT\nsequences can be explored, it will play a crucial role in guiding the precise\nscreening identification of lung cancer. Therefore, a cross spatio-temporal\nlung nodule dataset with pathological information for nodule identification and\ndiagnosis is constructed, which contains 328 CT sequences and 362 annotated\nnodules from 109 patients. This comprehensive database is intended to drive\nresearch in the field of CAD towards more practical and robust methods, and\nalso contribute to the further exploration of precision medicine related field.\nTo ensure patient confidentiality, we have removed sensitive information from\nthe dataset.\n","authors":["Muwei Jian","Haoran Zhang","Mingju Shao","Hongyu Chen","Huihui Huang","Yanjie Zhong","Changlei Zhang","Bin Wang","Penghui Gao"],"pdf_url":"https://arxiv.org/pdf/2406.18018v1.pdf","comment":null}]},"2024-06-25T00:00:00Z":{"Image and Video Processing":[{"id":"http://arxiv.org/abs/2406.17970v1","updated":"2024-06-25T23:03:48Z","published":"2024-06-25T23:03:48Z","title":"Highly Constrained Coded Aperture Imaging Systems Design Via a Knowledge\n  Distillation Approach","summary":"  Computational optical imaging (COI) systems have enabled the acquisition of\nhigh-dimensional signals through optical coding elements (OCEs). OCEs encode\nthe high-dimensional signal in one or more snapshots, which are subsequently\ndecoded using computational algorithms. Currently, COI systems are optimized\nthrough an end-to-end (E2E) approach, where the OCEs are modeled as a layer of\na neural network and the remaining layers perform a specific imaging task.\nHowever, the performance of COI systems optimized through E2E is limited by the\nphysical constraints imposed by these systems. This paper proposes a knowledge\ndistillation (KD) framework for the design of highly physically constrained COI\nsystems. This approach employs the KD methodology, which consists of a\nteacher-student relationship, where a high-performance, unconstrained COI\nsystem (the teacher), guides the optimization of a physically constrained\nsystem (the student) characterized by a limited number of snapshots. We\nvalidate the proposed approach, using a binary coded apertures single pixel\ncamera for monochromatic and multispectral image reconstruction. Simulation\nresults demonstrate the superiority of the KD scheme over traditional E2E\noptimization for the designing of highly physically constrained COI systems.\n","authors":["Leon Suarez-Rodriguez","Roman Jacome","Henry Arguello"],"pdf_url":"https://arxiv.org/pdf/2406.17970v1.pdf","comment":"7 pages, 3 figures. Accepted at ICIP 2024"},{"id":"http://arxiv.org/abs/2406.17936v1","updated":"2024-06-25T20:56:41Z","published":"2024-06-25T20:56:41Z","title":"Hot-Distance: Combining One-Hot and Signed Distance Embeddings for\n  Segmentation","summary":"  Machine learning models are only as good as the data to which they are fit.\nAs such, it is always preferable to use as much data as possible in training\nmodels. What data can be used for fitting a model depends a lot on the\nformulation of the task. We introduce Hot-Distance, a novel segmentation target\nthat incorporates the strength of signed boundary distance prediction with the\nflexibility of one-hot encoding, to increase the amount of usable training data\nfor segmentation of subcellular structures in focused ion beam scanning\nelectron microscopy (FIB-SEM).\n","authors":["Marwan Zouinkhi","Jeff L. Rhoades","Aubrey V. Weigel"],"pdf_url":"https://arxiv.org/pdf/2406.17936v1.pdf","comment":"3 pages, 1 figure, in progress"},{"id":"http://arxiv.org/abs/2406.17928v1","updated":"2024-06-25T20:40:42Z","published":"2024-06-25T20:40:42Z","title":"Total Variation Regularization for Tomographic Reconstruction of\n  Cylindrically Symmetric Objects","summary":"  Flash X-ray computed tomography (CT) is an important imaging modality for\ncharacterization of high-speed dynamic events, such as Kolsky bar impact\nexperiments for the study of mechanical properties of materials subjected to\nimpulsive forces. Due to experimental constraints, the number of X-ray views\nthat can be obtained is typically very sparse in both space and time, requiring\nstrong priors in order to enable a CT reconstruction. In this paper, we propose\nan effective method for exploiting the cylindrical symmetry inherent in the\nexperiment via a variant of total variation (TV) regularization that operates\nin cylindrical coordinates, and demonstrate that it outperforms competing\napproaches.\n","authors":["Maliha Hossain","Charles A. Bouman","Brendt Wohlberg"],"pdf_url":"https://arxiv.org/pdf/2406.17928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16343v2","updated":"2024-06-25T19:35:10Z","published":"2024-05-25T20:00:27Z","title":"Learning Point Spread Function Invertibility Assessment for Image\n  Deconvolution","summary":"  Deep-learning (DL)-based image deconvolution (ID) has exhibited remarkable\nrecovery performance, surpassing traditional linear methods. However, unlike\ntraditional ID approaches that rely on analytical properties of the point\nspread function (PSF) to achieve high recovery performance - such as specific\nspectrum properties or small conditional numbers in the convolution matrix - DL\ntechniques lack quantifiable metrics for evaluating PSF suitability for\nDL-assisted recovery. Aiming to enhance deconvolution quality, we propose a\nmetric that employs a non-linear approach to learn the invertibility of an\narbitrary PSF using a neural network by mapping it to a unit impulse. A lower\ndiscrepancy between the mapped PSF and a unit impulse indicates a higher\nlikelihood of successful inversion by a DL network. Our findings reveal that\nthis metric correlates with high recovery performance in DL and traditional\nmethods, thereby serving as an effective regularizer in deconvolution tasks.\nThis approach reduces the computational complexity over conventional condition\nnumber assessments and is a differentiable process. These useful properties\nallow its application in designing diffractive optical elements through\nend-to-end (E2E) optimization, achieving invertible PSFs, and outperforming the\nE2E baseline framework.\n","authors":["Romario Gualdrón-Hurtado","Roman Jacome","Sergio Urrea","Henry Arguello","Luis Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2405.16343v2.pdf","comment":"Accepted at EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2406.17902v1","updated":"2024-06-25T19:26:39Z","published":"2024-06-25T19:26:39Z","title":"Domain Adaptation of Echocardiography Segmentation Via Reinforcement\n  Learning","summary":"  Performance of deep learning segmentation models is significantly challenged\nin its transferability across different medical imaging domains, particularly\nwhen aiming to adapt these models to a target domain with insufficient\nannotated data for effective fine-tuning. While existing domain adaptation (DA)\nmethods propose strategies to alleviate this problem, these methods do not\nexplicitly incorporate human-verified segmentation priors, compromising the\npotential of a model to produce anatomically plausible segmentations. We\nintroduce RL4Seg, an innovative reinforcement learning framework that reduces\nthe need to otherwise incorporate large expertly annotated datasets in the\ntarget domain, and eliminates the need for lengthy manual human review. Using a\ntarget dataset of 10,000 unannotated 2D echocardiographic images, RL4Seg not\nonly outperforms existing state-of-the-art DA methods in accuracy but also\nachieves 99% anatomical validity on a subset of 220 expert-validated subjects\nfrom the target domain. Furthermore, our framework's reward network offers\nuncertainty estimates comparable with dedicated state-of-the-art uncertainty\nmethods, demonstrating the utility and effectiveness of RL4Seg in overcoming\ndomain adaptation challenges in medical image segmentation.\n","authors":["Arnaud Judge","Thierry Judge","Nicolas Duchateau","Roman A. Sandler","Joseph Z. Sokol","Olivier Bernard","Pierre-Marc Jodoin"],"pdf_url":"https://arxiv.org/pdf/2406.17902v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.17897v1","updated":"2024-06-25T19:12:46Z","published":"2024-06-25T19:12:46Z","title":"Pixel-weighted Multi-pose Fusion for Metal Artifact Reduction in X-ray\n  Computed Tomography","summary":"  X-ray computed tomography (CT) reconstructs the internal morphology of a\nthree dimensional object from a collection of projection images, most commonly\nusing a single rotation axis. However, for objects containing dense materials\nlike metal, the use of a single rotation axis may leave some regions of the\nobject obscured by the metal, even though projections from other rotation axes\n(or poses) might contain complementary information that would better resolve\nthese obscured regions.\n  In this paper, we propose pixel-weighted Multi-pose Fusion to reduce metal\nartifacts by fusing the information from complementary measurement poses into a\nsingle reconstruction. Our method uses Multi-Agent Consensus Equilibrium\n(MACE), an extension of Plug-and-Play, as a framework for integrating\nprojection data from different poses. A primary novelty of the proposed method\nis that the output of different MACE agents are fused in a pixel-weighted\nmanner to minimize the effects of metal throughout the reconstruction. Using\nreal CT data on an object with and without metal inserts, we demonstrate that\nthe proposed pixel-weighted Multi-pose Fusion method significantly reduces\nmetal artifacts relative to single-pose reconstructions.\n","authors":["Diyu Yang","Craig A. J. Kemp","Soumendu Majee","Gregery T. Buzzard","Charles A. Bouman"],"pdf_url":"https://arxiv.org/pdf/2406.17897v1.pdf","comment":"Submitted to IEEE MMSP 2024. arXiv admin note: substantial text\n  overlap with arXiv:2209.07561"},{"id":"http://arxiv.org/abs/2402.18451v3","updated":"2024-06-25T19:04:56Z","published":"2024-02-28T16:24:08Z","title":"MambaMIR: An Arbitrary-Masked Mamba for Joint Medical Image\n  Reconstruction and Uncertainty Estimation","summary":"  The recent Mamba model has shown remarkable adaptability for visual\nrepresentation learning, including in medical imaging tasks. This study\nintroduces MambaMIR, a Mamba-based model for medical image reconstruction, as\nwell as its Generative Adversarial Network-based variant, MambaMIR-GAN. Our\nproposed MambaMIR inherits several advantages, such as linear complexity,\nglobal receptive fields, and dynamic weights, from the original Mamba model.\nThe innovated arbitrary-mask mechanism effectively adapt Mamba to our image\nreconstruction task, providing randomness for subsequent Monte Carlo-based\nuncertainty estimation. Experiments conducted on various medical image\nreconstruction tasks, including fast MRI and SVCT, which cover anatomical\nregions such as the knee, chest, and abdomen, have demonstrated that MambaMIR\nand MambaMIR-GAN achieve comparable or superior reconstruction results relative\nto state-of-the-art methods. Additionally, the estimated uncertainty maps offer\nfurther insights into the reliability of the reconstruction quality. The code\nis publicly available at https://github.com/ayanglab/MambaMIR.\n","authors":["Jiahao Huang","Liutao Yang","Fanwen Wang","Yang Nan","Angelica I. Aviles-Rivero","Carola-Bibiane Schönlieb","Daoqiang Zhang","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2402.18451v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17659v2","updated":"2024-06-25T19:01:09Z","published":"2024-05-27T21:04:43Z","title":"Enhancing Global Sensitivity and Uncertainty Quantification in Medical\n  Image Reconstruction with Monte Carlo Arbitrary-Masked Mamba","summary":"  Deep learning has been extensively applied in medical image reconstruction,\nwhere Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs)\nrepresent the predominant paradigms, each possessing distinct advantages and\ninherent limitations: CNNs exhibit linear complexity with local sensitivity,\nwhereas ViTs demonstrate quadratic complexity with global sensitivity. The\nemerging Mamba has shown superiority in learning visual representation, which\ncombines the advantages of linear scalability and global sensitivity. In this\nstudy, we introduce MambaMIR, an Arbitrary-Masked Mamba-based model with\nwavelet decomposition for joint medical image reconstruction and uncertainty\nestimation. A novel Arbitrary Scan Masking (ASM) mechanism \"masks out\"\nredundant information to introduce randomness for further uncertainty\nestimation. Compared to the commonly used Monte Carlo (MC) dropout, our\nproposed MC-ASM provides an uncertainty map without the need for hyperparameter\ntuning and mitigates the performance drop typically observed when applying\ndropout to low-level tasks. For further texture preservation and better\nperceptual quality, we employ the wavelet transformation into MambaMIR and\nexplore its variant based on the Generative Adversarial Network, namely\nMambaMIR-GAN. Comprehensive experiments have been conducted for multiple\nrepresentative medical image reconstruction tasks, demonstrating that the\nproposed MambaMIR and MambaMIR-GAN outperform other baseline and\nstate-of-the-art methods in different reconstruction tasks, where MambaMIR\nachieves the best reconstruction fidelity and MambaMIR-GAN has the best\nperceptual quality. In addition, our MC-ASM provides uncertainty maps as an\nadditional tool for clinicians, while mitigating the typical performance drop\ncaused by the commonly used dropout.\n","authors":["Jiahao Huang","Liutao Yang","Fanwen Wang","Yang Nan","Weiwen Wu","Chengyan Wang","Kuangyu Shi","Angelica I. Aviles-Rivero","Carola-Bibiane Schönlieb","Daoqiang Zhang","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2405.17659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17709v1","updated":"2024-06-25T16:48:18Z","published":"2024-06-25T16:48:18Z","title":"Mask-Guided Attention U-Net for Enhanced Neonatal Brain Extraction and\n  Image Preprocessing","summary":"  In this study, we introduce MGA-Net, a novel mask-guided attention neural\nnetwork, which extends the U-net model for precision neonatal brain imaging.\nMGA-Net is designed to extract the brain from other structures and reconstruct\nhigh-quality brain images. The network employs a common encoder and two\ndecoders: one for brain mask extraction and the other for brain region\nreconstruction. A key feature of MGA-Net is its high-level mask-guided\nattention module, which leverages features from the brain mask decoder to\nenhance image reconstruction. To enable the same encoder and decoder to process\nboth MRI and ultrasound (US) images, MGA-Net integrates sinusoidal positional\nencoding. This encoding assigns distinct positional values to MRI and US\nimages, allowing the model to effectively learn from both modalities.\nConsequently, features learned from a single modality can aid in learning a\nmodality with less available data, such as US. We extensively validated the\nproposed MGA-Net on diverse datasets from varied clinical settings and neonatal\nage groups. The metrics used for assessment included the DICE similarity\ncoefficient, recall, and accuracy for image segmentation; structural similarity\nfor image reconstruction; and root mean squared error for total brain volume\nestimation from 3D ultrasound images. Our results demonstrate that MGA-Net\nsignificantly outperforms traditional methods, offering superior performance in\nbrain extraction and segmentation while achieving high precision in image\nreconstruction and volumetric analysis. Thus, MGA-Net represents a robust and\neffective preprocessing tool for MRI and 3D ultrasound images, marking a\nsignificant advance in neuroimaging that enhances both research and clinical\ndiagnostics in the neonatal period and beyond.\n","authors":["Bahram Jafrasteh","Simon Pedro Lubian-Lopez","Emiliano Trimarco","Macarena Roman Ruiz","Carmen Rodriguez Barrios","Yolanda Marin Almagro","Isabel Benavente-Fernandez"],"pdf_url":"https://arxiv.org/pdf/2406.17709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17670v1","updated":"2024-06-25T15:58:56Z","published":"2024-06-25T15:58:56Z","title":"Brain Tumor Classification using Vision Transformer with Selective\n  Cross-Attention Mechanism and Feature Calibration","summary":"  Brain tumor classification is a challenging task in medical image analysis.\nIn this paper, we propose a novel approach to brain tumor classification using\na vision transformer with a novel cross-attention mechanism. Our approach\nleverages the strengths of transformers in modeling long-range dependencies and\nmulti-scale feature fusion. We introduce two new mechanisms to improve the\nperformance of the cross-attention fusion module: Feature Calibration Mechanism\n(FCM) and Selective Cross-Attention (SCA). FCM calibrates the features from\ndifferent branches to make them more compatible, while SCA selectively attends\nto the most informative features. Our experiments demonstrate that the proposed\napproach outperforms other state-of-the-art methods in brain tumor\nclassification, achieving improved accuracy and efficiency. The proposed FCM\nand SCA mechanisms can be easily integrated into other vision transformer\narchitectures, making them a promising direction for future research in medical\nimage analysis. Experimental results confirm that our approach surpasses\nexisting methods, achieving state-of-the-art performance in brain tumor\nclassification tasks.\n","authors":["Mohammad Ali Labbaf Khaniki","Alireza Golkarieh","Mohammad Manthouri"],"pdf_url":"https://arxiv.org/pdf/2406.17670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17666v1","updated":"2024-06-25T15:54:49Z","published":"2024-06-25T15:54:49Z","title":"Transformer-based segmentation of adnexal lesions and ovarian implants\n  in CT images","summary":"  Two self-supervised pretrained transformer-based segmentation models (SMIT\nand Swin UNETR) fine-tuned on a dataset of ovarian cancer CT images provided\nreasonably accurate delineations of the tumors in an independent test dataset.\nTumors in the adnexa were segmented more accurately by both transformers (SMIT\nand Swin UNETR) than the omental implants. AI-assisted labeling performed on 72\nout of 245 omental implants resulted in smaller manual editing effort of 39.55\nmm compared to full manual correction of partial labels of 106.49 mm and\nresulted in overall improved accuracy performance. Both SMIT and Swin UNETR did\nnot generate any false detection of omental metastases in the urinary bladder\nand relatively few false detections in the small bowel, with 2.16 cc on average\nfor SMIT and 7.37 cc for Swin UNETR respectively.\n","authors":["Aneesh Rangnekar","Kevin M. Boehm","Emily A. Aherne","Ines Nikolovski","Natalie Gangai","Ying Liu","Dimitry Zamarin","Kara L. Roche","Sohrab P. Shah","Yulia Lakhman","Harini Veeraraghavan"],"pdf_url":"https://arxiv.org/pdf/2406.17666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17617v1","updated":"2024-06-25T15:02:01Z","published":"2024-06-25T15:02:01Z","title":"Embedded event based object detection with spiking neural network","summary":"  The complexity of event-based object detection (OD) poses considerable\nchallenges. Spiking Neural Networks (SNNs) show promising results and pave the\nway for efficient event-based OD. Despite this success, the path to efficient\nSNNs on embedded devices remains a challenge. This is due to the size of the\nnetworks required to accomplish the task and the ability of devices to take\nadvantage of SNNs benefits. Even when \"edge\" devices are considered, they\ntypically use embedded GPUs that consume tens of watts. In response to these\nchallenges, our research introduces an embedded neuromorphic testbench that\nutilizes the SPiking Low-power Event-based ArchiTecture (SPLEAT) accelerator.\nUsing an extended version of the Qualia framework, we can train, evaluate,\nquantize, and deploy spiking neural networks on an FPGA implementation of\nSPLEAT. We used this testbench to load a state-of-the-art SNN solution,\nestimate the performance loss associated with deploying the network on\ndedicated hardware, and run real-world event-based OD on neuromorphic hardware\nspecifically designed for low-power spiking neural networks. Remarkably, our\nembedded spiking solution, which includes a model with 1.08 million parameters,\noperates efficiently with 490 mJ per prediction.\n","authors":["Jonathan Courtois","Pierre-Emmanuel Novac","Edgar Lemaire","Alain Pegatoquet","Benoit Miramond"],"pdf_url":"https://arxiv.org/pdf/2406.17617v1.pdf","comment":"Result link: https://youtu.be/TsolUDaMY7Y"},{"id":"http://arxiv.org/abs/2406.17578v1","updated":"2024-06-25T14:21:55Z","published":"2024-06-25T14:21:55Z","title":"Sparse-view Signal-domain Photoacoustic Tomography Reconstruction Method\n  Based on Neural Representation","summary":"  Photoacoustic tomography is a hybrid biomedical technology, which combines\nthe advantages of acoustic and optical imaging. However, for the conventional\nimage reconstruction method, the image quality is affected obviously by\nartifacts under the condition of sparse sampling. in this paper, a novel\nmodel-based sparse reconstruction method via implicit neural representation was\nproposed for improving the image quality reconstructed from sparse data.\nSpecially, the initial acoustic pressure distribution was modeled as a\ncontinuous function of spatial coordinates, and parameterized by a multi-layer\nperceptron. The weights of multi-layer perceptron were determined by training\nthe network in self-supervised manner. And the total variation regularization\nterm was used to offer the prior knowledge. We compared our result with some\nablation studies, and the results show that out method outperforms existing\nmethods on simulation and experimental data. Under the sparse sampling\ncondition, our method can suppress the artifacts and avoid the ill-posed\nproblem effectively, which reconstruct images with higher signal-to-noise ratio\nand contrast-to-noise ratio than traditional methods. The high-quality results\nfor sparse data make the proposed method hold the potential for further\ndecreasing the hardware cost of photoacoustic tomography system.\n","authors":["Bowei Yao","Yi Zeng","Haizhao Dai","Qing Wu","Youshen Xiao","Fei Gao","Yuyao Zhang","Jingyi Yu","Xiran Cai"],"pdf_url":"https://arxiv.org/pdf/2406.17578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17577v1","updated":"2024-06-25T14:18:42Z","published":"2024-06-25T14:18:42Z","title":"Advancing Cell Detection in Anterior Segment Optical Coherence\n  Tomography Images","summary":"  Anterior uveitis, a common form of eye inflammation, can lead to permanent\nvision loss if not promptly diagnosed. Monitoring this condition involves\nquantifying inflammatory cells in the anterior chamber (AC) of the eye, which\ncan be captured using Anterior Segment Optical Coherence Tomography (AS-OCT).\nHowever, manually identifying cells in AS-OCT images is time-consuming and\nsubjective. Moreover, existing automated approaches may have limitations in\nboth the effectiveness of detecting cells and the reliability of their\ndetection results. To address these challenges, we propose an automated\nframework to detect cells in the AS-OCT images. This framework consists of a\nzero-shot chamber segmentation module and a cell detection module. The first\nmodule segments the AC area in the image without requiring human-annotated\ntraining data. Subsequently, the second module identifies individual cells\nwithin the segmented AC region. Through experiments, our framework demonstrates\nsuperior performance compared to current state-of-the-art methods for both AC\nsegmentation and cell detection tasks. Notably, we find that previous cell\ndetection approaches could suffer from low recall, potentially overlooking a\nsignificant number of cells. In contrast, our framework offers an improved\nsolution, which could benefit the diagnosis and study of anterior uveitis. Our\ncode for cell detection is publicly available at:\nhttps://github.com/joeybyc/cell_detection.\n","authors":["Boyu Chen","Ameenat L. Solebo","Paul Taylor"],"pdf_url":"https://arxiv.org/pdf/2406.17577v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09335v2","updated":"2024-06-25T13:47:06Z","published":"2024-06-13T17:17:31Z","title":"Instance-level quantitative saliency in multiple sclerosis lesion\n  segmentation","summary":"  In recent years, explainable methods for artificial intelligence (XAI) have\ntried to reveal and describe models' decision mechanisms in the case of\nclassification tasks. However, XAI for semantic segmentation and in particular\nfor single instances has been little studied to date. Understanding the process\nunderlying automatic segmentation of single instances is crucial to reveal what\ninformation was used to detect and segment a given object of interest. In this\nstudy, we proposed two instance-level explanation maps for semantic\nsegmentation based on SmoothGrad and Grad-CAM++ methods. Then, we investigated\ntheir relevance for the detection and segmentation of white matter lesions\n(WML), a magnetic resonance imaging (MRI) biomarker in multiple sclerosis (MS).\n687 patients diagnosed with MS for a total of 4043 FLAIR and MPRAGE MRI scans\nwere collected at the University Hospital of Basel, Switzerland. Data were\nrandomly split into training, validation and test sets to train a 3D U-Net for\nMS lesion segmentation. We observed 3050 true positive (TP), 1818 false\npositive (FP), and 789 false negative (FN) cases. We generated instance-level\nexplanation maps for semantic segmentation, by developing two XAI methods based\non SmoothGrad and Grad-CAM++. We investigated: 1) the distribution of gradients\nin saliency maps with respect to both input MRI sequences; 2) the model's\nresponse in the case of synthetic lesions; 3) the amount of perilesional tissue\nneeded by the model to segment a lesion. Saliency maps (based on SmoothGrad) in\nFLAIR showed positive values inside a lesion and negative in its neighborhood.\nPeak values of saliency maps generated for these four groups of volumes\npresented distributions that differ significantly from one another, suggesting\na quantitative nature of the proposed saliency. Contextual information of 7mm\naround the lesion border was required for their segmentation.\n","authors":["Federico Spagnolo","Nataliia Molchanova","Roger Schaer","Meritxell Bach Cuadra","Mario Ocampo Pineda","Lester Melie-Garcia","Cristina Granziera","Vincent Andrearczyk","Adrien Depeursinge"],"pdf_url":"https://arxiv.org/pdf/2406.09335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10343v2","updated":"2024-06-25T13:05:30Z","published":"2024-04-16T07:26:20Z","title":"The Ninth NTIRE 2024 Efficient Super-Resolution Challenge Report","summary":"  This paper provides a comprehensive review of the NTIRE 2024 challenge,\nfocusing on efficient single-image super-resolution (ESR) solutions and their\noutcomes. The task of this challenge is to super-resolve an input image with a\nmagnification factor of x4 based on pairs of low and corresponding\nhigh-resolution images. The primary objective is to develop networks that\noptimize various aspects such as runtime, parameters, and FLOPs, while still\nmaintaining a peak signal-to-noise ratio (PSNR) of approximately 26.90 dB on\nthe DIV2K_LSDIR_valid dataset and 26.99 dB on the DIV2K_LSDIR_test dataset. In\naddition, this challenge has 4 tracks including the main track (overall\nperformance), sub-track 1 (runtime), sub-track 2 (FLOPs), and sub-track 3\n(parameters). In the main track, all three metrics (ie runtime, FLOPs, and\nparameter count) were considered. The ranking of the main track is calculated\nbased on a weighted sum-up of the scores of all other sub-tracks. In sub-track\n1, the practical runtime performance of the submissions was evaluated, and the\ncorresponding score was used to determine the ranking. In sub-track 2, the\nnumber of FLOPs was considered. The score calculated based on the corresponding\nFLOPs was used to determine the ranking. In sub-track 3, the number of\nparameters was considered. The score calculated based on the corresponding\nparameters was used to determine the ranking. RLFN is set as the baseline for\nefficiency measurement. The challenge had 262 registered participants, and 34\nteams made valid submissions. They gauge the state-of-the-art in efficient\nsingle-image super-resolution. To facilitate the reproducibility of the\nchallenge and enable other researchers to build upon these findings, the code\nand the pre-trained model of validated solutions are made publicly available at\nhttps://github.com/Amazingren/NTIRE2024_ESR/.\n","authors":["Bin Ren","Yawei Li","Nancy Mehta","Radu Timofte","Hongyuan Yu","Cheng Wan","Yuxin Hong","Bingnan Han","Zhuoyuan Wu","Yajun Zou","Yuqing Liu","Jizhe Li","Keji He","Chao Fan","Heng Zhang","Xiaolin Zhang","Xuanwu Yin","Kunlong Zuo","Bohao Liao","Peizhe Xia","Long Peng","Zhibo Du","Xin Di","Wangkai Li","Yang Wang","Wei Zhai","Renjing Pei","Jiaming Guo","Songcen Xu","Yang Cao","Zhengjun Zha","Yan Wang","Yi Liu","Qing Wang","Gang Zhang","Liou Zhang","Shijie Zhao","Long Sun","Jinshan Pan","Jiangxin Dong","Jinhui Tang","Xin Liu","Min Yan","Qian Wang","Menghan Zhou","Yiqiang Yan","Yixuan Liu","Wensong Chan","Dehua Tang","Dong Zhou","Li Wang","Lu Tian","Barsoum Emad","Bohan Jia","Junbo Qiao","Yunshuai Zhou","Yun Zhang","Wei Li","Shaohui Lin","Shenglong Zhou","Binbin Chen","Jincheng Liao","Suiyi Zhao","Zhao Zhang","Bo Wang","Yan Luo","Yanyan Wei","Feng Li","Mingshen Wang","Yawei Li","Jinhan Guan","Dehua Hu","Jiawei Yu","Qisheng Xu","Tao Sun","Long Lan","Kele Xu","Xin Lin","Jingtong Yue","Lehan Yang","Shiyi Du","Lu Qi","Chao Ren","Zeyu Han","Yuhan Wang","Chaolin Chen","Haobo Li","Mingjun Zheng","Zhongbao Yang","Lianhong Song","Xingzhuo Yan","Minghan Fu","Jingyi Zhang","Baiang Li","Qi Zhu","Xiaogang Xu","Dan Guo","Chunle Guo","Jiadi Chen","Huanhuan Long","Chunjiang Duanmu","Xiaoyan Lei","Jie Liu","Weilin Jia","Weifeng Cao","Wenlong Zhang","Yanyu Mao","Ruilong Guo","Nihao Zhang","Qian Wang","Manoj Pandey","Maksym Chernozhukov","Giang Le","Shuli Cheng","Hongyuan Wang","Ziyan Wei","Qingting Tang","Liejun Wang","Yongming Li","Yanhui Guo","Hao Xu","Akram Khatami-Rizi","Ahmad Mahmoudi-Aznaveh","Chih-Chung Hsu","Chia-Ming Lee","Yi-Shiuan Chou","Amogh Joshi","Nikhil Akalwadi","Sampada Malagi","Palani Yashaswini","Chaitra Desai","Ramesh Ashok Tabib","Ujwala Patil","Uma Mudenagudi"],"pdf_url":"https://arxiv.org/pdf/2404.10343v2.pdf","comment":"The report paper of NTIRE2024 Efficient Super-resolution, accepted by\n  CVPRW2024"},{"id":"http://arxiv.org/abs/2403.00612v2","updated":"2024-06-25T12:49:54Z","published":"2024-03-01T15:35:48Z","title":"Advancing dermatological diagnosis: Development of a hyperspectral\n  dermatoscope for enhanced skin imaging","summary":"  Clinical dermatology necessitates precision and innovation for efficient\ndiagnosis and treatment of various skin conditions. This paper introduces the\ndevelopment of a cutting-edge hyperspectral dermatoscope (the Hyperscope)\ntailored for human skin analysis. We detail the requirements to such a device\nand the design considerations, from optical configurations to sensor selection,\nnecessary to capture a wide spectral range with high fidelity. Preliminary\nresults from 15 individuals and 160 recorded skin images demonstrate the\npotential of the Hyperscope in identifying and characterizing various skin\nconditions, offering a promising avenue for non-invasive skin evaluation and a\nplatform for future research in dermatology-related hyperspectral imaging.\n","authors":["Martin J. Hetz","Carina Nogueira Garcia","Sarah Haggenmüller","Titus J. Brinker"],"pdf_url":"https://arxiv.org/pdf/2403.00612v2.pdf","comment":"12 pages, 11 Figures"},{"id":"http://arxiv.org/abs/2406.17483v1","updated":"2024-06-25T12:04:51Z","published":"2024-06-25T12:04:51Z","title":"TRIP: Trainable Region-of-Interest Prediction for Hardware-Efficient\n  Neuromorphic Processing on Event-based Vision","summary":"  Neuromorphic processors are well-suited for efficiently handling sparse\nevents from event-based cameras. However, they face significant challenges in\nthe growth of computing demand and hardware costs as the input resolution\nincreases. This paper proposes the Trainable Region-of-Interest Prediction\n(TRIP), the first hardware-efficient hard attention framework for event-based\nvision processing on a neuromorphic processor. Our TRIP framework actively\nproduces low-resolution Region-of-Interest (ROIs) for efficient and accurate\nclassification. The framework exploits sparse events' inherent low information\ndensity to reduce the overhead of ROI prediction. We introduced extensive\nhardware-aware optimizations for TRIP and implemented the hardware-optimized\nalgorithm on the SENECA neuromorphic processor. We utilized multiple\nevent-based classification datasets for evaluation. Our approach achieves\nstate-of-the-art accuracies in all datasets and produces reasonable ROIs with\nvarying locations and sizes. On the DvsGesture dataset, our solution requires\n46x less computation than the state-of-the-art while achieving higher accuracy.\nFurthermore, TRIP enables more than 2x latency and energy improvements on the\nSENECA neuromorphic processor compared to the conventional solution.\n","authors":["Cina Arjmand","Yingfu Xu","Kevin Shidqi","Alexandra F. Dobrita","Kanishkan Vadivel","Paul Detterer","Manolis Sifalakis","Amirreza Yousefzadeh","Guangzhi Tang"],"pdf_url":"https://arxiv.org/pdf/2406.17483v1.pdf","comment":"Accepted in ICONS 2024"},{"id":"http://arxiv.org/abs/2406.17472v1","updated":"2024-06-25T11:30:31Z","published":"2024-06-25T11:30:31Z","title":"UHD-IQA Benchmark Database: Pushing the Boundaries of Blind Photo\n  Quality Assessment","summary":"  We introduce a novel Image Quality Assessment (IQA) dataset comprising 6073\nUHD-1 (4K) images, annotated at a fixed width of 3840 pixels. Contrary to\nexisting No-Reference (NR) IQA datasets, ours focuses on highly aesthetic\nphotos of high technical quality, filling a gap in the literature. The images,\ncarefully curated to exclude synthetic content, are sufficiently diverse to\ntrain general NR-IQA models. The dataset is annotated with perceptual quality\nratings obtained through a crowdsourcing study. Ten expert raters, comprising\nphotographers and graphics artists, assessed each image at least twice in\nmultiple sessions spanning several days, resulting in highly reliable labels.\nAnnotators were rigorously selected based on several metrics, including\nself-consistency, to ensure their reliability. The dataset includes rich\nmetadata with user and machine-generated tags from over 5,000 categories and\npopularity indicators such as favorites, likes, downloads, and views. With its\nunique characteristics, such as its focus on high-quality images, reliable\ncrowdsourced annotations, and high annotation resolution, our dataset opens up\nnew opportunities for advancing perceptual image quality assessment research\nand developing practical NR-IQA models that apply to modern photos. Our dataset\nis available at https://database.mmsp-kn.de/uhd-iqa-benchmark-database.html\n","authors":["Vlad Hosu","Lorenzo Agnolucci","Oliver Wiedemann","Daisuke Iso"],"pdf_url":"https://arxiv.org/pdf/2406.17472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17471v1","updated":"2024-06-25T11:15:56Z","published":"2024-06-25T11:15:56Z","title":"Medical Image Segmentation Using Directional Window Attention","summary":"  Accurate segmentation of medical images is crucial for diagnostic purposes,\nincluding cell segmentation, tumor identification, and organ localization.\nTraditional convolutional neural network (CNN)-based approaches struggled to\nachieve precise segmentation results due to their limited receptive fields,\nparticularly in cases involving multi-organ segmentation with varying shapes\nand sizes. The transformer-based approaches address this limitation by\nleveraging the global receptive field, but they often face challenges in\ncapturing local information required for pixel-precise segmentation. In this\nwork, we introduce DwinFormer, a hierarchical encoder-decoder architecture for\nmedical image segmentation comprising a directional window (Dwin) attention and\nglobal self-attention (GSA) for feature encoding. The focus of our design is\nthe introduction of Dwin block within DwinFormer that effectively captures\nlocal and global information along the horizontal, vertical, and depthwise\ndirections of the input feature map by separately performing attention in each\nof these directional volumes. To this end, our Dwin block introduces a nested\nDwin attention (NDA) that progressively increases the receptive field in\nhorizontal, vertical, and depthwise directions and a convolutional Dwin\nattention (CDA) that captures local contextual information for the attention\ncomputation. While the proposed Dwin block captures local and global\ndependencies at the first two high-resolution stages of DwinFormer, the GSA\nblock encodes global dependencies at the last two lower-resolution stages.\nExperiments over the challenging 3D Synapse Multi-organ dataset and Cell HMS\ndataset demonstrate the benefits of our DwinFormer over the state-of-the-art\napproaches. Our source code will be publicly available at\n\\url{https://github.com/Daniyanaj/DWINFORMER}.\n","authors":["Daniya Najiha Abdul Kareem","Mustansar Fiaz","Noa Novershtern","Hisham Cholakkal"],"pdf_url":"https://arxiv.org/pdf/2406.17471v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2406.17423v1","updated":"2024-06-25T09:56:30Z","published":"2024-06-25T09:56:30Z","title":"Deep learning-based brain segmentation model performance validation with\n  clinical radiotherapy CT","summary":"  Manual segmentation of medical images is labor intensive and especially\nchallenging for images with poor contrast or resolution. The presence of\ndisease exacerbates this further, increasing the need for an automated\nsolution. To this extent, SynthSeg is a robust deep learning model designed for\nautomatic brain segmentation across various contrasts and resolutions. This\nstudy validates the SynthSeg robust brain segmentation model on computed\ntomography (CT), using a multi-center dataset. An open access dataset of 260\npaired CT and magnetic resonance imaging (MRI) from radiotherapy patients\ntreated in 5 centers was collected. Brain segmentations from CT and MRI were\nobtained with SynthSeg model, a component of the Freesurfer imaging suite.\nThese segmentations were compared and evaluated using Dice scores and Hausdorff\n95 distance (HD95), treating MRI-based segmentations as the ground truth. Brain\nregions that failed to meet performance criteria were excluded based on\nautomated quality control (QC) scores. Dice scores indicate a median overlap of\n0.76 (IQR: 0.65-0.83). The median HD95 is 2.95 mm (IQR: 1.73-5.39). QC score\nbased thresholding improves median dice by 0.1 and median HD95 by 0.05mm.\nMorphological differences related to sex and age, as detected by MRI, were also\nreplicated with CT, with an approximate 17% difference between the CT and MRI\nresults for sex and 10% difference between the results for age. SynthSeg can be\nutilized for CT-based automatic brain segmentation, but only in applications\nwhere precision is not essential. CT performance is lower than MRI based on the\nintegrated QC scores, but low-quality segmentations can be excluded with\nQC-based thresholding. Additionally, performing CT-based neuroanatomical\nstudies is encouraged, as the results show correlations in sex- and age-based\nanalyses similar to those found with MRI.\n","authors":["Selena Huisman","Matteo Maspero","Marielle Philippens","Joost Verhoeff","Szabolcs David"],"pdf_url":"https://arxiv.org/pdf/2406.17423v1.pdf","comment":"15 pages, 9 figures, 3 supplementary data csv's, 1 supplementary file\n  with 1 figure"},{"id":"http://arxiv.org/abs/2406.16658v2","updated":"2024-06-25T09:36:21Z","published":"2024-06-24T14:08:27Z","title":"Sampling Strategies in Bayesian Inversion: A Study of RTO and Langevin\n  Methods","summary":"  This paper studies two classes of sampling methods for the solution of\ninverse problems, namely Randomize-Then-Optimize (RTO), which is rooted in\nsensitivity analysis, and Langevin methods, which are rooted in the Bayesian\nframework. The two classes of methods correspond to different assumptions and\nyield samples from different target distributions. We highlight the main\nconceptual and theoretical differences between the two approaches and compare\nthem from a practical point of view by tackling two classical inverse problems\nin imaging: deblurring and inpainting. We show that the choice of the sampling\nmethod has a significant impact on the quality of the reconstruction and that\nthe RTO method is more robust to the choice of the parameters.\n","authors":["Remi Laumont","Yiqiu Dong","Martin Skovgaard Andersen"],"pdf_url":"https://arxiv.org/pdf/2406.16658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17338v1","updated":"2024-06-25T07:50:09Z","published":"2024-06-25T07:50:09Z","title":"Robustly Optimized Deep Feature Decoupling Network for Fatty Liver\n  Diseases Detection","summary":"  Current medical image classification efforts mainly aim for higher average\nperformance, often neglecting the balance between different classes. This can\nlead to significant differences in recognition accuracy between classes and\nobvious recognition weaknesses. Without the support of massive data, deep\nlearning faces challenges in fine-grained classification of fatty liver. In\nthis paper, we propose an innovative deep learning framework that combines\nfeature decoupling and adaptive adversarial training. Firstly, we employ two\niteratively compressed decouplers to supervised decouple common features and\nspecific features related to fatty liver in abdominal ultrasound images.\nSubsequently, the decoupled features are concatenated with the original image\nafter transforming the color space and are fed into the classifier. During\nadversarial training, we adaptively adjust the perturbation and balance the\nadversarial strength by the accuracy of each class. The model will eliminate\nrecognition weaknesses by correctly classifying adversarial samples, thus\nimproving recognition robustness. Finally, the accuracy of our method improved\nby 4.16%, achieving 82.95%. As demonstrated by extensive experiments, our\nmethod is a generalized learning framework that can be directly used to\neliminate the recognition weaknesses of any classifier while improving its\naverage performance. Code is available at https://github.com/HP-ML/MICCAI2024.\n","authors":["Peng Huang","Shu Hu","Bo Peng","Jiashu Zhang","Xi Wu","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2406.17338v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.15770v2","updated":"2024-06-25T07:45:53Z","published":"2024-03-23T08:57:46Z","title":"Graph Image Prior for Unsupervised Dynamic Cardiac Cine MRI\n  Reconstruction","summary":"  The inductive bias of the convolutional neural network (CNN) can be a strong\nprior for image restoration, which is known as the Deep Image Prior (DIP).\nRecently, DIP is utilized in unsupervised dynamic MRI reconstruction, which\nadopts a generative model from the latent space to the image space. However,\nexisting methods usually use a pyramid-shaped CNN generator shared by all\nframes, embedding the temporal modeling within the latent space, which may\nhamper the model expression capability. In this work, we propose a novel scheme\nfor dynamic MRI representation, named ``Graph Image Prior'' (GIP). GIP adopts a\ntwo-stage generative network in a new modeling methodology, which first employs\nindependent CNNs to recover the image structure for each frame, and then\nexploits the spatio-temporal correlations within the feature space\nparameterized by a graph model. A graph convolutional network is utilized for\nfeature fusion and dynamic image generation. In addition, we devise an ADMM\nalgorithm to alternately optimize the images and the network parameters to\nimprove the reconstruction performance. Experiments were conducted on cardiac\ncine MRI reconstruction, which demonstrate that GIP outperforms compressed\nsensing methods and other DIP-based unsupervised methods, significantly\nreducing the performance gap with state-of-the-art supervised algorithms.\nMoreover, GIP displays superior generalization ability when transferred to a\ndifferent reconstruction setting, without the need for any additional data.\n","authors":["Zhongsen Li","Wenxuan Chen","Shuai Wang","Chuyu Liu","Qing Zou","Rui Li"],"pdf_url":"https://arxiv.org/pdf/2403.15770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16109v2","updated":"2024-06-25T06:47:07Z","published":"2024-06-23T13:53:35Z","title":"X-ray2CTPA: Generating 3D CTPA scans from 2D X-ray conditioning","summary":"  Chest X-rays or chest radiography (CXR), commonly used for medical\ndiagnostics, typically enables limited imaging compared to computed tomography\n(CT) scans, which offer more detailed and accurate three-dimensional data,\nparticularly contrast-enhanced scans like CT Pulmonary Angiography (CTPA).\nHowever, CT scans entail higher costs, greater radiation exposure, and are less\naccessible than CXRs. In this work we explore cross-modal translation from a 2D\nlow contrast-resolution X-ray input to a 3D high contrast and\nspatial-resolution CTPA scan. Driven by recent advances in generative AI, we\nintroduce a novel diffusion-based approach to this task. We evaluate the models\nperformance using both quantitative metrics and qualitative feedback from\nradiologists, ensuring diagnostic relevance of the generated images.\nFurthermore, we employ the synthesized 3D images in a classification framework\nand show improved AUC in a PE categorization task, using the initial CXR input.\nThe proposed method is generalizable and capable of performing additional\ncross-modality translations in medical imaging. It may pave the way for more\naccessible and cost-effective advanced diagnostic tools. The code for this\nproject is available: https://github.com/NoaCahan/X-ray2CTPA .\n","authors":["Noa Cahan","Eyal Klang","Galit Aviram","Yiftach Barash","Eli Konen","Raja Giryes","Hayit Greenspan"],"pdf_url":"https://arxiv.org/pdf/2406.16109v2.pdf","comment":"preprint, project code: https://github.com/NoaCahan/X-ray2CTPA"},{"id":"http://arxiv.org/abs/2406.17302v1","updated":"2024-06-25T06:16:05Z","published":"2024-06-25T06:16:05Z","title":"HD snapshot diffractive spectral imaging and inferencing","summary":"  We present a novel high-definition (HD) snapshot diffractive spectral imaging\nsystem utilizing a diffractive filter array (DFA) to capture a single image\nthat encodes both spatial and spectral information. This single diffractogram\ncan be computationally reconstructed into a spectral image cube, providing a\nhigh-resolution representation of the scene across 25 spectral channels in the\n440-800 nm range at 1304x744 spatial pixels (~1 MP). This unique approach\noffers numerous advantages including snapshot capture, a form of optical\ncompression, flexible offline reconstruction, the ability to select the\nspectral basis after capture, and high light throughput due to the absence of\nlossy filters. We demonstrate a 30-50 nm spectral resolution and compared our\nreconstructed spectra against ground truth obtained by conventional\nspectrometers. Proof-of-concept experiments in diverse applications including\nbiological tissue classification, food quality assessment, and simulated\nstellar photometry validate our system's capability to perform robust and\naccurate inference. These results establish the DFA-based imaging system as a\nversatile and powerful tool for advancing scientific and industrial imaging\napplications.\n","authors":["Apratim Majumder","Monjurul Meem","Fernando Gonzalez del Cueto","Fernando Guevara-Vasquez","Syed N. Qadri","Freddie Santiago","Rajesh Menon"],"pdf_url":"https://arxiv.org/pdf/2406.17302v1.pdf","comment":"33 pages, 16 figures"},{"id":"http://arxiv.org/abs/2406.16026v2","updated":"2024-06-25T04:28:09Z","published":"2024-06-23T06:23:12Z","title":"CEST-KAN: Kolmogorov-Arnold Networks for CEST MRI Data Analysis","summary":"  Purpose: This study aims to propose and investigate the feasibility of using\nKolmogorov-Arnold Network (KAN) for CEST MRI data analysis (CEST-KAN). Methods:\nCEST MRI data were acquired from twelve healthy volunteers at 3T. Data from ten\nsubjects were used for training, while the remaining two were reserved for\ntesting. The performance of multi-layer perceptron (MLP) and KAN models with\nthe same network settings were evaluated and compared to the conventional\nmulti-pool Lorentzian fitting (MPLF) method in generating water and multiple\nCEST contrasts, including amide, relayed nuclear Overhauser effect (rNOE), and\nmagnetization transfer (MT). Results: The water and CEST maps generated by both\nMLP and KAN were visually comparable to the MPLF results. However, the KAN\nmodel demonstrated higher accuracy in extrapolating the CEST fitting metrics,\nas evidenced by the smaller validation loss during training and smaller\nabsolute error during testing. Voxel-wise correlation analysis showed that all\nfour CEST fitting metrics generated by KAN consistently exhibited higher\nPearson coefficients than the MLP results, indicating superior performance.\nMoreover, the KAN models consistently outperformed the MLP models in varying\nhidden layer numbers despite longer training time. Conclusion: In this study,\nwe demonstrated for the first time the feasibility of utilizing KAN for CEST\nMRI data analysis, highlighting its superiority over MLP in this task. The\nfindings suggest that CEST-KAN has the potential to be a robust and reliable\npost-analysis tool for CEST MRI in clinical settings.\n","authors":["Jiawen Wang","Pei Cai","Ziyan Wang","Huabin Zhang","Jianpan Huang"],"pdf_url":"https://arxiv.org/pdf/2406.16026v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17250v1","updated":"2024-06-25T03:34:54Z","published":"2024-06-25T03:34:54Z","title":"A benchmark for 2D foetal brain ultrasound analysis","summary":"  Brain development involves a sequence of structural changes from early stages\nof the embryo until several months after birth. Currently, ultrasound is the\nestablished technique for screening due to its ability to acquire dynamic\nimages in real-time without radiation and to its cost-efficiency. However,\nidentifying abnormalities remains challenging due to the difficulty in\ninterpreting foetal brain images. In this work we present a set of 104 2D\nfoetal brain ultrasound images acquired during the 20th week of gestation that\nhave been co-registered to a common space from a rough skull segmentation. The\nimages are provided both on the original space and template space centred on\nthe ellipses of all the subjects. Furthermore, the images have been annotated\nto highlight landmark points from structures of interest to analyse brain\ndevelopment. Both the final atlas template with probabilistic maps and the\noriginal images can be used to develop new segmentation techniques, test\nregistration approaches for foetal brain ultrasound, extend our work to\nlongitudinal datasets and to detect anomalies in new images.\n","authors":["Mariano Cabezas","Yago Diez","Clara Martinez-Diago","Anna Maroto"],"pdf_url":"https://arxiv.org/pdf/2406.17250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15222v2","updated":"2024-06-25T03:17:22Z","published":"2024-06-14T02:15:09Z","title":"Rapid and Accurate Diagnosis of Acute Aortic Syndrome using Non-contrast\n  CT: A Large-scale, Retrospective, Multi-center and AI-based Study","summary":"  Chest pain symptoms are highly prevalent in emergency departments (EDs),\nwhere acute aortic syndrome (AAS) is a catastrophic cardiovascular emergency\nwith a high fatality rate, especially when timely and accurate treatment is not\nadministered. However, current triage practices in the ED can cause up to\napproximately half of patients with AAS to have an initially missed diagnosis\nor be misdiagnosed as having other acute chest pain conditions. Subsequently,\nthese AAS patients will undergo clinically inaccurate or suboptimal\ndifferential diagnosis. Fortunately, even under these suboptimal protocols,\nnearly all these patients underwent non-contrast CT covering the aorta anatomy\nat the early stage of differential diagnosis. In this study, we developed an\nartificial intelligence model (DeepAAS) using non-contrast CT, which is highly\naccurate for identifying AAS and provides interpretable results to assist in\nclinical decision-making. Performance was assessed in two major phases: a\nmulti-center retrospective study (n = 20,750) and an exploration in real-world\nemergency scenarios (n = 137,525). In the multi-center cohort, DeepAAS achieved\na mean area under the receiver operating characteristic curve of 0.958 (95% CI\n0.950-0.967). In the real-world cohort, DeepAAS detected 109 AAS patients with\nmisguided initial suspicion, achieving 92.6% (95% CI 76.2%-97.5%) in mean\nsensitivity and 99.2% (95% CI 99.1%-99.3%) in mean specificity. Our AI model\nperformed well on non-contrast CT at all applicable early stages of\ndifferential diagnosis workflows, effectively reduced the overall missed\ndiagnosis and misdiagnosis rate from 48.8% to 4.8% and shortened the diagnosis\ntime for patients with misguided initial suspicion from an average of 681.8\n(74-11,820) mins to 68.5 (23-195) mins. DeepAAS could effectively fill the gap\nin the current clinical workflow without requiring additional tests.\n","authors":["Yujian Hu","Yilang Xiang","Yan-Jie Zhou","Yangyan He","Shifeng Yang","Xiaolong Du","Chunlan Den","Youyao Xu","Gaofeng Wang","Zhengyao Ding","Jingyong Huang","Wenjun Zhao","Xuejun Wu","Donglin Li","Qianqian Zhu","Zhenjiang Li","Chenyang Qiu","Ziheng Wu","Yunjun He","Chen Tian","Yihui Qiu","Zuodong Lin","Xiaolong Zhang","Yuan He","Zhenpeng Yuan","Xiaoxiang Zhou","Rong Fan","Ruihan Chen","Wenchao Guo","Jianpeng Zhang","Tony C. W. Mok","Zi Li","Le Lu","Dehai Lang","Xiaoqiang Li","Guofu Wang","Wei Lu","Zhengxing Huang","Minfeng Xu","Hongkun Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.15222v2.pdf","comment":"under peer review"},{"id":"http://arxiv.org/abs/2406.17238v1","updated":"2024-06-25T02:59:02Z","published":"2024-06-25T02:59:02Z","title":"Expansive Synthesis: Generating Large-Scale Datasets from Minimal\n  Samples","summary":"  The challenge of limited availability of data for training in machine\nlearning arises in many applications and the impact on performance and\ngeneralization is serious. Traditional data augmentation methods aim to enhance\ntraining with a moderately sufficient data set. Generative models like\nGenerative Adversarial Networks (GANs) often face problematic convergence when\ngenerating significant and diverse data samples. Diffusion models, though\neffective, still struggle with high computational cost and long training times.\nThis paper introduces an innovative Expansive Synthesis model that generates\nlarge-scale, high-fidelity datasets from minimal samples. The proposed approach\nexploits expander graph mappings and feature interpolation to synthesize\nexpanded datasets while preserving the intrinsic data distribution and feature\nstructural relationships. The rationale of the model is rooted in the\nnon-linear property of neural networks' latent space and in its capture by a\nKoopman operator to yield a linear space of features to facilitate the\nconstruction of larger and enriched consistent datasets starting with a much\nsmaller dataset. This process is optimized by an autoencoder architecture\nenhanced with self-attention layers and further refined for distributional\nconsistency by optimal transport. We validate our Expansive Synthesis by\ntraining classifiers on the generated datasets and comparing their performance\nto classifiers trained on larger, original datasets. Experimental results\ndemonstrate that classifiers trained on synthesized data achieve performance\nmetrics on par with those trained on full-scale datasets, showcasing the\nmodel's potential to effectively augment training data. This work represents a\nsignificant advancement in data generation, offering a robust solution to data\nscarcity and paving the way for enhanced data availability in machine learning\napplications.\n","authors":["Vahid Jebraeeli","Bo Jiang","Hamid Krim","Derya Cansever"],"pdf_url":"https://arxiv.org/pdf/2406.17238v1.pdf","comment":"14 pages. arXiv admin note: text overlap with arXiv:2405.13866"},{"id":"http://arxiv.org/abs/2406.17225v1","updated":"2024-06-25T02:18:35Z","published":"2024-06-25T02:18:35Z","title":"Multimodal Cross-Task Interaction for Survival Analysis in Whole Slide\n  Pathological Images","summary":"  Survival prediction, utilizing pathological images and genomic profiles, is\nincreasingly important in cancer analysis and prognosis. Despite significant\nprogress, precise survival analysis still faces two main challenges: (1) The\nmassive pixels contained in whole slide images (WSIs) complicate the process of\npathological images, making it difficult to generate an effective\nrepresentation of the tumor microenvironment (TME). (2) Existing multimodal\nmethods often rely on alignment strategies to integrate complementary\ninformation, which may lead to information loss due to the inherent\nheterogeneity between pathology and genes. In this paper, we propose a\nMultimodal Cross-Task Interaction (MCTI) framework to explore the intrinsic\ncorrelations between subtype classification and survival analysis tasks.\nSpecifically, to capture TME-related features in WSIs, we leverage the subtype\nclassification task to mine tumor regions. Simultaneously, multi-head attention\nmechanisms are applied in genomic feature extraction, adaptively performing\ngenes grouping to obtain task-related genomic embedding. With the joint\nrepresentation of pathological images and genomic data, we further introduce a\nTransport-Guided Attention (TGA) module that uses optimal transport theory to\nmodel the correlation between subtype classification and survival analysis\ntasks, effectively transferring potential information. Extensive experiments\ndemonstrate the superiority of our approaches, with MCTI outperforming\nstate-of-the-art frameworks on three public benchmarks.\n\\href{https://github.com/jsh0792/MCTI}{https://github.com/jsh0792/MCTI}.\n","authors":["Songhan Jiang","Zhengyu Gan","Linghan Cai","Yifeng Wang","Yongbing Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.17225v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15093v2","updated":"2024-06-25T01:07:15Z","published":"2024-06-21T12:14:24Z","title":"ECLIPSE: Expunging Clean-label Indiscriminate Poisons via Sparse\n  Diffusion Purification","summary":"  Clean-label indiscriminate poisoning attacks add invisible perturbations to\ncorrectly labeled training images, thus dramatically reducing the\ngeneralization capability of the victim models. Recently, some defense\nmechanisms have been proposed such as adversarial training, image\ntransformation techniques, and image purification. However, these schemes are\neither susceptible to adaptive attacks, built on unrealistic assumptions, or\nonly effective against specific poison types, limiting their universal\napplicability. In this research, we propose a more universally effective,\npractical, and robust defense scheme called ECLIPSE. We first investigate the\nimpact of Gaussian noise on the poisons and theoretically prove that any kind\nof poison will be largely assimilated when imposing sufficient random noise. In\nlight of this, we assume the victim has access to an extremely limited number\nof clean images (a more practical scene) and subsequently enlarge this sparse\nset for training a denoising probabilistic model (a universal denoising tool).\nWe then begin by introducing Gaussian noise to absorb the poisons and then\napply the model for denoising, resulting in a roughly purified dataset.\nFinally, to address the trade-off of the inconsistency in the assimilation\nsensitivity of different poisons by Gaussian noise, we propose a lightweight\ncorruption compensation module to effectively eliminate residual poisons,\nproviding a more universal defense approach. Extensive experiments demonstrate\nthat our defense approach outperforms 10 state-of-the-art defenses. We also\npropose an adaptive attack against ECLIPSE and verify the robustness of our\ndefense scheme. Our code is available at https://github.com/CGCL-codes/ECLIPSE.\n","authors":["Xianlong Wang","Shengshan Hu","Yechao Zhang","Ziqi Zhou","Leo Yu Zhang","Peng Xu","Wei Wan","Hai Jin"],"pdf_url":"https://arxiv.org/pdf/2406.15093v2.pdf","comment":"Accepted by ESORICS 2024"}]},"2024-06-24T00:00:00Z":{"Image and Video Processing":[{"id":"http://arxiv.org/abs/2307.03812v4","updated":"2024-06-24T23:08:29Z","published":"2023-07-07T19:36:24Z","title":"Coordinate-based neural representations for computational adaptive\n  optics in widefield microscopy","summary":"  Widefield microscopy is widely used for non-invasive imaging of biological\nstructures at subcellular resolution. When applied to complex specimen, its\nimage quality is degraded by sample-induced optical aberration. Adaptive optics\ncan correct wavefront distortion and restore diffraction-limited resolution but\nrequire wavefront sensing and corrective devices, increasing system complexity\nand cost. Here, we describe a self-supervised machine learning algorithm,\nCoCoA, that performs joint wavefront estimation and three-dimensional\nstructural information extraction from a single input 3D image stack without\nthe need for external training dataset. We implemented CoCoA for widefield\nimaging of mouse brain tissues and validated its performance with\ndirect-wavefront-sensing-based adaptive optics. Importantly, we systematically\nexplored and quantitatively characterized the limiting factors of CoCoA's\nperformance. Using CoCoA, we demonstrated the first in vivo widefield mouse\nbrain imaging using machine-learning-based adaptive optics. Incorporating\ncoordinate-based neural representations and a forward physics model, the\nself-supervised scheme of CoCoA should be applicable to microscopy modalities\nin general.\n","authors":["Iksung Kang","Qinrong Zhang","Stella X. Yu","Na Ji"],"pdf_url":"https://arxiv.org/pdf/2307.03812v4.pdf","comment":"60 pages, 20 figures, 2 tables. Nat Mach Intell (2024)"},{"id":"http://arxiv.org/abs/2310.09457v4","updated":"2024-06-24T20:29:38Z","published":"2023-10-14T00:32:11Z","title":"UCM-Net: A Lightweight and Efficient Solution for Skin Lesion\n  Segmentation using MLP and CNN","summary":"  Skin cancer poses a significant public health challenge, necessitating\nefficient diagnostic tools. We introduce UCM-Net, a novel skin lesion\nsegmentation model combining Multi-Layer Perceptrons (MLP) and Convolutional\nNeural Networks (CNN). This lightweight, efficient architecture, deviating from\ntraditional UNet designs, dramatically reduces computational demands, making it\nideal for mobile health applications. Evaluated on PH2, ISIC 2017, and ISIC\n2018 datasets, UCM-Net demonstrates robust performance with fewer than 50KB\nparameters and requires less than 0.05 Giga Operations Per Second (GLOPs).\nMoreover, its minimal memory requirement is just 1.19MB in CPU environment\npositions. It is a potential benchmark for efficiency in skin lesion\nsegmentation, suitable for deployment in resource-constrained settings. In\norder to facilitate accessibility and further research in the field, the\nUCM-Net source code is https://github.com/chunyuyuan/UCM-Net.\n","authors":["Chunyu Yuan","Dongfang Zhao","Sos S. Agaian"],"pdf_url":"https://arxiv.org/pdf/2310.09457v4.pdf","comment":"17 pages, accepted by Journal of Biomedical Signal Processing and\n  Control"},{"id":"http://arxiv.org/abs/2406.17080v1","updated":"2024-06-24T19:09:20Z","published":"2024-06-24T19:09:20Z","title":"Multi-Aperture Fusion of Transformer-Convolutional Network (MFTC-Net)\n  for 3D Medical Image Segmentation and Visualization","summary":"  Vision Transformers have shown superior performance to the traditional\nconvolutional-based frameworks in many vision applications, including but not\nlimited to the segmentation of 3D medical images. To further advance this area,\nthis study introduces the Multi-Aperture Fusion of Transformer-Convolutional\nNetwork (MFTC-Net), which integrates the output of Swin Transformers and their\ncorresponding convolutional blocks using 3D fusion blocks. The Multi-Aperture\nincorporates each image patch at its original resolutions with its pyramid\nrepresentation to better preserve minute details. The proposed architecture has\ndemonstrated a score of 89.73 and 7.31 for Dice and HD95, respectively, on the\nSynapse multi-organs dataset an improvement over the published results. The\nimproved performance also comes with the added benefits of the reduced\ncomplexity of approximately 40 million parameters. Our code is available at\nhttps://github.com/Siyavashshabani/MFTC-Net\n","authors":["Siyavash Shabani","Muhammad Sohaib","Sahar A. Mohammed","Bahram Parvin"],"pdf_url":"https://arxiv.org/pdf/2406.17080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05944v2","updated":"2024-06-24T18:05:06Z","published":"2024-05-09T17:33:09Z","title":"MRISegmentator-Abdomen: A Fully Automated Multi-Organ and Structure\n  Segmentation Tool for T1-weighted Abdominal MRI","summary":"  Background: Segmentation of organs and structures in abdominal MRI is useful\nfor many clinical applications, such as disease diagnosis and radiotherapy.\nCurrent approaches have focused on delineating a limited set of abdominal\nstructures (13 types). To date, there is no publicly available abdominal MRI\ndataset with voxel-level annotations of multiple organs and structures.\nConsequently, a segmentation tool for multi-structure segmentation is also\nunavailable. Methods: We curated a T1-weighted abdominal MRI dataset consisting\nof 195 patients who underwent imaging at National Institutes of Health (NIH)\nClinical Center. The dataset comprises of axial pre-contrast T1, arterial,\nvenous, and delayed phases for each patient, thereby amounting to a total of\n780 series (69,248 2D slices). Each series contains voxel-level annotations of\n62 abdominal organs and structures. A 3D nnUNet model, dubbed as\nMRISegmentator-Abdomen (MRISegmentator in short), was trained on this dataset,\nand evaluation was conducted on an internal test set and two large external\ndatasets: AMOS22 and Duke Liver. The predicted segmentations were compared\nagainst the ground-truth using the Dice Similarity Coefficient (DSC) and\nNormalized Surface Distance (NSD). Findings: MRISegmentator achieved an average\nDSC of 0.861$\\pm$0.170 and a NSD of 0.924$\\pm$0.163 in the internal test set.\nOn the AMOS22 dataset, MRISegmentator attained an average DSC of\n0.829$\\pm$0.133 and a NSD of 0.908$\\pm$0.067. For the Duke Liver dataset, an\naverage DSC of 0.933$\\pm$0.015 and a NSD of 0.929$\\pm$0.021 was obtained.\nInterpretation: The proposed MRISegmentator provides automatic, accurate, and\nrobust segmentations of 62 organs and structures in T1-weighted abdominal MRI\nsequences. The tool has the potential to accelerate research on various\nclinical topics, such as abnormality detection, radiotherapy, disease\nclassification among others.\n","authors":["Yan Zhuang","Tejas Sudharshan Mathai","Pritam Mukherjee","Brandon Khoury","Boah Kim","Benjamin Hou","Nusrat Rabbee","Abhinav Suri","Ronald M. Summers"],"pdf_url":"https://arxiv.org/pdf/2405.05944v2.pdf","comment":"We made the segmentation model publicly available"},{"id":"http://arxiv.org/abs/2406.16848v1","updated":"2024-06-24T17:55:02Z","published":"2024-06-24T17:55:02Z","title":"Unsupervised Domain Adaptation for Pediatric Brain Tumor Segmentation","summary":"  Significant advances have been made toward building accurate automatic\nsegmentation models for adult gliomas. However, the performance of these models\noften degrades when applied to pediatric glioma due to their imaging and\nclinical differences (domain shift). Obtaining sufficient annotated data for\npediatric glioma is typically difficult because of its rare nature. Also,\nmanual annotations are scarce and expensive. In this work, we propose\nDomain-Adapted nnU-Net (DA-nnUNet) to perform unsupervised domain adaptation\nfrom adult glioma (source domain) to pediatric glioma (target domain).\nSpecifically, we add a domain classifier connected with a gradient reversal\nlayer (GRL) to a backbone nnU-Net. Once the classifier reaches a very high\naccuracy, the GRL is activated with the goal of transferring domain-invariant\nfeatures from the classifier to the segmentation model while preserving\nsegmentation accuracy on the source domain. The accuracy of the classifier\nslowly degrades to chance levels. No annotations are used in the target domain.\nThe method is compared to 8 different supervised models using BraTS-Adult\nglioma (N=1251) and BraTS-PED glioma data (N=99). The proposed method shows\nnotable performance enhancements in the tumor core (TC) region compared to the\nmodel that only uses adult data: ~32% better Dice scores and ~20 better 95th\npercentile Hausdorff distances. Moreover, our unsupervised approach shows no\nstatistically significant difference compared to the practical upper bound\nmodel using manual annotations from both datasets in TC region. The code is\nshared at https://github.com/Fjr9516/DA_nnUNet.\n","authors":["Jingru Fu","Simone Bendazzoli","Örjan Smedby","Rodrigo Moreno"],"pdf_url":"https://arxiv.org/pdf/2406.16848v1.pdf","comment":"10 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2406.16754v1","updated":"2024-06-24T16:00:20Z","published":"2024-06-24T16:00:20Z","title":"The MRI Scanner as a Diagnostic: Image-less Active Sampling","summary":"  Despite the high diagnostic accuracy of Magnetic Resonance Imaging (MRI),\nusing MRI as a Point-of-Care (POC) disease identification tool poses\nsignificant accessibility challenges due to the use of high magnetic field\nstrength and lengthy acquisition times. We ask a simple question: Can we\ndynamically optimise acquired samples, at the patient level, according to an\n(automated) downstream decision task, while discounting image reconstruction?\nWe propose an ML-based framework that learns an active sampling strategy, via\nreinforcement learning, at a patient-level to directly infer disease from\nundersampled k-space. We validate our approach by inferring Meniscus Tear in\nundersampled knee MRI data, where we achieve diagnostic performance comparable\nwith ML-based diagnosis, using fully sampled k-space data. We analyse\ntask-specific sampling policies, showcasing the adaptability of our active\nsampling approach. The introduced frugal sampling strategies have the potential\nto reduce high field strength requirements that in turn strengthen the\nviability of MRI-based POC disease identification and associated preliminary\nscreening tools.\n","authors":["Yuning Du","Rohan Dharmakumar","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2406.16754v1.pdf","comment":"Accepted in MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.16724v1","updated":"2024-06-24T15:29:08Z","published":"2024-06-24T15:29:08Z","title":"μ-Net: A Deep Learning-Based Architecture for μ-CT Segmentation","summary":"  X-ray computed microtomography ({\\mu}-CT) is a non-destructive technique that\ncan generate high-resolution 3D images of the internal anatomy of medical and\nbiological samples. These images enable clinicians to examine internal anatomy\nand gain insights into the disease or anatomical morphology. However,\nextracting relevant information from 3D images requires semantic segmentation\nof the regions of interest, which is usually done manually and results\ntime-consuming and tedious. In this work, we propose a novel framework that\nuses a convolutional neural network (CNN) to automatically segment the full\nmorphology of the heart of Carassius auratus. The framework employs an\noptimized 2D CNN architecture that can infer a 3D segmentation of the sample,\navoiding the high computational cost of a 3D CNN architecture. We tackle the\nchallenges of handling large and high-resoluted image data (over a thousand\npixels in each dimension) and a small training database (only three samples) by\nproposing a standard protocol for data normalization and processing. Moreover,\nwe investigate how the noise, contrast, and spatial resolution of the sample\nand the training of the architecture are affected by the reconstruction\ntechnique, which depends on the number of input images. Experiments show that\nour framework significantly reduces the time required to segment new samples,\nallowing a faster microtomography analysis of the Carassius auratus heart\nshape. Furthermore, our framework can work with any bio-image (biological and\nmedical) from {\\mu}-CT with high-resolution and small dataset size\n","authors":["Pierangela Bruno","Edoardo De Rose","Carlo Adornetto","Francesco Calimeri","Sandro Donato","Raffaele Giuseppe Agostino","Daniela Amelio","Riccardo Barberi","Maria Carmela Cerra","Maria Caterina Crocco","Mariacristina Filice","Raffaele Filosa","Gianluigi Greco","Sandra Imbrogno","Vincenzo Formoso"],"pdf_url":"https://arxiv.org/pdf/2406.16724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18435v2","updated":"2024-06-24T15:07:34Z","published":"2024-03-19T17:57:24Z","title":"QUBIQ: Uncertainty Quantification for Biomedical Image Segmentation\n  Challenge","summary":"  Uncertainty in medical image segmentation tasks, especially inter-rater\nvariability, arising from differences in interpretations and annotations by\nvarious experts, presents a significant challenge in achieving consistent and\nreliable image segmentation. This variability not only reflects the inherent\ncomplexity and subjective nature of medical image interpretation but also\ndirectly impacts the development and evaluation of automated segmentation\nalgorithms. Accurately modeling and quantifying this variability is essential\nfor enhancing the robustness and clinical applicability of these algorithms. We\nreport the set-up and summarize the benchmark results of the Quantification of\nUncertainties in Biomedical Image Quantification Challenge (QUBIQ), which was\norganized in conjunction with International Conferences on Medical Image\nComputing and Computer-Assisted Intervention (MICCAI) 2020 and 2021. The\nchallenge focuses on the uncertainty quantification of medical image\nsegmentation which considers the omnipresence of inter-rater variability in\nimaging datasets. The large collection of images with multi-rater annotations\nfeatures various modalities such as MRI and CT; various organs such as the\nbrain, prostate, kidney, and pancreas; and different image dimensions 2D-vs-3D.\nA total of 24 teams submitted different solutions to the problem, combining\nvarious baseline models, Bayesian neural networks, and ensemble model\ntechniques. The obtained results indicate the importance of the ensemble\nmodels, as well as the need for further research to develop efficient 3D\nmethods for uncertainty quantification methods in 3D segmentation tasks.\n","authors":["Hongwei Bran Li","Fernando Navarro","Ivan Ezhov","Amirhossein Bayat","Dhritiman Das","Florian Kofler","Suprosanna Shit","Diana Waldmannstetter","Johannes C. Paetzold","Xiaobin Hu","Benedikt Wiestler","Lucas Zimmer","Tamaz Amiranashvili","Chinmay Prabhakar","Christoph Berger","Jonas Weidner","Michelle Alonso-Basant","Arif Rashid","Ujjwal Baid","Wesam Adel","Deniz Ali","Bhakti Baheti","Yingbin Bai","Ishaan Bhatt","Sabri Can Cetindag","Wenting Chen","Li Cheng","Prasad Dutand","Lara Dular","Mustafa A. Elattar","Ming Feng","Shengbo Gao","Henkjan Huisman","Weifeng Hu","Shubham Innani","Wei Jiat","Davood Karimi","Hugo J. Kuijf","Jin Tae Kwak","Hoang Long Le","Xiang Lia","Huiyan Lin","Tongliang Liu","Jun Ma","Kai Ma","Ting Ma","Ilkay Oksuz","Robbie Holland","Arlindo L. Oliveira","Jimut Bahan Pal","Xuan Pei","Maoying Qiao","Anindo Saha","Raghavendra Selvan","Linlin Shen","Joao Lourenco Silva","Ziga Spiclin","Sanjay Talbar","Dadong Wang","Wei Wang","Xiong Wang","Yin Wang","Ruiling Xia","Kele Xu","Yanwu Yan","Mert Yergin","Shuang Yu","Lingxi Zeng","YingLin Zhang","Jiachen Zhao","Yefeng Zheng","Martin Zukovec","Richard Do","Anton Becker","Amber Simpson","Ender Konukoglu","Andras Jakab","Spyridon Bakas","Leo Joskowicz","Bjoern Menze"],"pdf_url":"https://arxiv.org/pdf/2405.18435v2.pdf","comment":"initial technical report"},{"id":"http://arxiv.org/abs/2406.16701v1","updated":"2024-06-24T15:04:14Z","published":"2024-06-24T15:04:14Z","title":"Demystifying the Effect of Receptive Field Size in U-Net Models for\n  Medical Image Segmentation","summary":"  Medical image segmentation is a critical task in healthcare applications, and\nU-Nets have demonstrated promising results. This work delves into the\nunderstudied aspect of receptive field (RF) size and its impact on the U-Net\nand Attention U-Net architectures. This work explores several critical elements\nincluding the relationship between RF size, characteristics of the region of\ninterest, and model performance, as well as the balance between RF size and\ncomputational costs for U-Net and Attention U-Net methods for different\ndatasets. This work also proposes a mathematical notation for representing the\ntheoretical receptive field (TRF) of a given layer in a network and proposes\ntwo new metrics - effective receptive field (ERF) rate and the Object rate to\nquantify the fraction of significantly contributing pixels within the ERF\nagainst the TRF area and assessing the relative size of the segmentation object\ncompared to the TRF size respectively. The results demonstrate that there\nexists an optimal TRF size that successfully strikes a balance between\ncapturing a wider global context and maintaining computational efficiency,\nthereby optimizing model performance. Interestingly, a distinct correlation is\nobserved between the data complexity and the required TRF size; segmentation\nbased solely on contrast achieved peak performance even with smaller TRF sizes,\nwhereas more complex segmentation tasks necessitated larger TRFs. Attention\nU-Net models consistently outperformed their U-Net counterparts, highlighting\nthe value of attention mechanisms regardless of TRF size. These novel insights\npresent an invaluable resource for developing more efficient U-Net-based\narchitectures for medical imaging and pave the way for future exploration. A\ntool is also developed that calculates the TRF for a U-Net (and Attention\nU-Net) model, and also suggest an appropriate TRF size for a given model and\ndataset.\n","authors":["Vincent Loos","Rohit Pardasani","Navchetan Awasthi"],"pdf_url":"https://arxiv.org/pdf/2406.16701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08298v2","updated":"2024-06-24T09:48:01Z","published":"2024-03-13T07:10:24Z","title":"Physics-Informed Deep Learning for Motion-Corrected Reconstruction of\n  Quantitative Brain MRI","summary":"  We propose PHIMO, a physics-informed learning-based motion correction method\ntailored to quantitative MRI. PHIMO leverages information from the signal\nevolution to exclude motion-corrupted k-space lines from a data-consistent\nreconstruction. We demonstrate the potential of PHIMO for the application of\nT2* quantification from gradient echo MRI, which is particularly sensitive to\nmotion due to its sensitivity to magnetic field inhomogeneities. A\nstate-of-the-art technique for motion correction requires redundant acquisition\nof the k-space center, prolonging the acquisition. We show that PHIMO can\ndetect and exclude intra-scan motion events and, thus, correct for severe\nmotion artifacts. PHIMO approaches the performance of the state-of-the-art\nmotion correction method, while substantially reducing the acquisition time by\nover 40%, facilitating clinical applicability. Our code is available at\nhttps://github.com/HannahEichhorn/PHIMO.\n","authors":["Hannah Eichhorn","Veronika Spieker","Kerstin Hammernik","Elisa Saks","Kilian Weiss","Christine Preibisch","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2403.08298v2.pdf","comment":"Accepted at MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.14570v2","updated":"2024-06-24T09:25:33Z","published":"2024-06-06T09:46:14Z","title":"Deep-Learning Approach for Tissue Classification using Acoustic Waves\n  during Ablation with an Er:YAG Laser (Updated)","summary":"  Today's mechanical tools for bone cutting (osteotomy) cause mechanical trauma\nthat prolongs the healing process. Medical device manufacturers aim to minimize\nthis trauma, with minimally invasive surgery using laser cutting as one\ninnovation. This method ablates tissue using laser light instead of mechanical\ntools, reducing post-surgery healing time. A reliable feedback system is\ncrucial during laser surgery to prevent damage to surrounding tissues. We\npropose a tissue classification method analyzing acoustic waves generated\nduring laser ablation, demonstrating its applicability in an ex-vivo\nexperiment. The ablation process with a microsecond pulsed Er:YAG laser\nproduces acoustic waves, acquired with an air-coupled transducer. These waves\nwere used to classify five porcine tissue types: hard bone, soft bone, muscle,\nfat, and skin. For automated tissue classification, we compared five Neural\nNetwork (NN) approaches: a one-dimensional Convolutional Neural Network (CNN)\nwith time-dependent input, a Fully-connected Neural Network (FcNN) with either\nthe frequency spectrum or principal components of the frequency spectrum as\ninput, and a combination of a CNN and an FcNN with time-dependent data and its\nfrequency spectrum as input. Consecutive acoustic waves were used to improve\nclassification accuracy. Grad-Cam identified the activation map of the\nfrequencies, showing low frequencies as the most important for this task. Our\nresults indicated that combining time-dependent data with its frequency\nspectrum achieved the highest classification accuracy (65.5%-75.5%). We also\nfound that using the frequency spectrum alone was sufficient, with no\nadditional benefit from applying Principal Components Analysis (PCA).\n","authors":["Carlo Seppi","Philippe C. Cattin"],"pdf_url":"https://arxiv.org/pdf/2406.14570v2.pdf","comment":"This paper is an updated version of Deep-Learning Approach for Tissue\n  Classification using Acoustic Waves during Ablation with an Er:YAG Laser\n  originally published in DOI:10.1109/ACCESS.2021.3113055. This update\n  addresses several issues and incorporates corrections as outlined in\n  DOI:10.1109/ACCESS.2024.3395071. We provide here a detailed description of\n  our experiments and the new models we used"},{"id":"http://arxiv.org/abs/2406.16466v1","updated":"2024-06-24T09:16:17Z","published":"2024-06-24T09:16:17Z","title":"SLOctolyzer: Fully automatic analysis toolkit for segmentation and\n  feature extracting in scanning laser ophthalmoscopy images","summary":"  Purpose: To describe SLOctolyzer: an open-source analysis toolkit for en face\nretinal vessels appearing in infrared reflectance scanning laser ophthalmoscopy\n(SLO) images.\n  Methods: SLOctolyzer includes two main modules: segmentation and measurement.\nThe segmentation module use deep learning methods to delineate retinal anatomy,\nwhile the measurement module quantifies key retinal vascular features such as\nvessel complexity, density, tortuosity, and calibre. We evaluate the\nsegmentation module using unseen data and measure its reproducibility.\n  Results: SLOctolyzer's segmentation module performed well against unseen\ninternal test data (Dice for all-vessels, 0.9097; arteries, 0.8376; veins,\n0.8525; optic disc, 0.9430; fovea, 0.8837). External validation against severe\nretinal pathology showed decreased performance (Dice for arteries, 0.7180;\nveins, 0.7470; optic disc, 0.9032). SLOctolyzer had good reproducibility (mean\ndifference for fractal dimension, -0.0007; vessel density, -0.0003; vessel\ncalibre, -0.3154 $\\mu$m; tortuosity density, 0.0013). SLOctolyzer can process a\nmacula-centred SLO image in under 20 seconds and a disc-centred SLO image in\nunder 30 seconds using a standard laptop CPU.\n  Conclusions: To our knowledge, SLOctolyzer is the first open-source tool to\nconvert raw SLO images into reproducible and clinically meaningful retinal\nvascular parameters. SLO images are captured simultaneous to optical coherence\ntomography (OCT), and we believe our software will be useful for extracting\nretinal vascular measurements from large OCT image sets and linking them to\nocular or systemic diseases. It requires no specialist knowledge or proprietary\nsoftware, and allows manual correction of segmentations and re-computing of\nvascular metrics. SLOctolyzer is freely available at\nhttps://github.com/jaburke166/SLOctolyzer.\n","authors":["Jamie Burke","Samuel Gibbon","Justin Engelmann","Adam Threlfall","Ylenia Giarratano","Charlene Hamid","Stuart King","Ian J. C. MacCormick","Tom MacGillivray"],"pdf_url":"https://arxiv.org/pdf/2406.16466v1.pdf","comment":"10 pages, 5 figures, 6 tables + Supplementary (7 pages, 10 figures, 4\n  tables). Submitted for peer review at Translational Vision Science and\n  Technology"},{"id":"http://arxiv.org/abs/2406.16993v1","updated":"2024-06-24T08:01:05Z","published":"2024-06-24T08:01:05Z","title":"Are Vision xLSTM Embedded UNet More Reliable in Medical 3D Image\n  Segmentation?","summary":"  The advancement of developing efficient medical image segmentation has\nevolved from initial dependence on Convolutional Neural Networks (CNNs) to the\npresent investigation of hybrid models that combine CNNs with Vision\nTransformers. Furthermore, there is an increasing focus on creating\narchitectures that are both high-performing in medical image segmentation tasks\nand computationally efficient to be deployed on systems with limited resources.\nAlthough transformers have several advantages like capturing global\ndependencies in the input data, they face challenges such as high computational\nand memory complexity. This paper investigates the integration of CNNs and\nVision Extended Long Short-Term Memory (Vision-xLSTM) models by introducing a\nnovel approach called UVixLSTM. The Vision-xLSTM blocks captures temporal and\nglobal relationships within the patches extracted from the CNN feature maps.\nThe convolutional feature reconstruction path upsamples the output volume from\nthe Vision-xLSTM blocks to produce the segmentation output. Our primary\nobjective is to propose that Vision-xLSTM forms a reliable backbone for medical\nimage segmentation tasks, offering excellent segmentation performance and\nreduced computational complexity. UVixLSTM exhibits superior performance\ncompared to state-of-the-art networks on the publicly-available Synapse\ndataset. Code is available at: https://github.com/duttapallabi2907/UVixLSTM\n","authors":["Pallabi Dutta","Soham Bose","Swalpa Kumar Roy","Sushmita Mitra"],"pdf_url":"https://arxiv.org/pdf/2406.16993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.02088v3","updated":"2024-06-24T07:51:01Z","published":"2023-08-04T00:07:26Z","title":"Motion-robust free-running volumetric cardiovascular MRI","summary":"  PURPOSE: To present and assess an outlier mitigation method that makes\nfree-running volumetric cardiovascular MRI (CMR) more robust to motion.\n  METHODS: The proposed method, called compressive recovery with outlier\nrejection (CORe), models outliers in the measured data as an additive auxiliary\nvariable. We enforce MR physics-guided group sparsity on the auxiliary\nvariable, and jointly estimate it along with the image using an iterative\nalgorithm. For evaluation, CORe is first compared to traditional compressed\nsensing (CS), robust regression (RR), and an existing outlier rejection method\nusing two simulation studies. Then, CORe is compared to CS using seven\nthree-dimensional (3D) cine, 12 rest four-dimensional (4D) flow, and eight\nstress 4D flow imaging datasets.\n  RESULTS: Our simulation studies show that CORe outperforms CS, RR, and the\nexisting outlier rejection method in terms of normalized mean square error and\nstructural similarity index across 55 different realizations. The expert reader\nevaluation of 3D cine images demonstrates that CORe is more effective in\nsuppressing artifacts while maintaining or improving image sharpness. Finally,\n4D flow images show that CORe yields more reliable and consistent flow\nmeasurements, especially in the presence of involuntary subject motion or\nexercise stress.\n  CONCLUSION: An outlier rejection method is presented and tested using\nsimulated and measured data. This method can help suppress motion artifacts in\na wide range of free-running CMR applications.\n  CODE & DATA: Implementation code and datasets are available on GitHub at\nhttp://github.com/OSU-MR/motion-robust-CMR\n","authors":["Syed M. Arshad","Lee C. Potter","Chong Chen","Yingmin Liu","Preethi Chandrasekaran","Christopher Crabtree","Matthew S. Tong","Orlando P. Simonetti","Yuchi Han","Rizwan Ahmad"],"pdf_url":"https://arxiv.org/pdf/2308.02088v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16359v1","updated":"2024-06-24T06:57:51Z","published":"2024-06-24T06:57:51Z","title":"Improving Generative Adversarial Networks for Video Super-Resolution","summary":"  In this research, we explore different ways to improve generative adversarial\nnetworks for video super-resolution tasks from a base single image\nsuper-resolution GAN model. Our primary objective is to identify potential\ntechniques that enhance these models and to analyze which of these techniques\nyield the most significant improvements. We evaluate our results using Peak\nSignal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). Our\nfindings indicate that the most effective techniques include temporal\nsmoothing, long short-term memory (LSTM) layers, and a temporal loss function.\nThe integration of these methods results in an 11.97% improvement in PSNR and\nan 8% improvement in SSIM compared to the baseline video super-resolution\ngenerative adversarial network (GAN) model. This substantial improvement\nsuggests potential further applications to enhance current state-of-the-art\nmodels.\n","authors":["Daniel Wen"],"pdf_url":"https://arxiv.org/pdf/2406.16359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16358v1","updated":"2024-06-24T06:54:59Z","published":"2024-06-24T06:54:59Z","title":"Approximate DCT and Quantization Techniques for Energy-Constrained Image\n  Sensors","summary":"  Recent expansions in multimedia devices gather enormous amounts of real-time\nimages for processing and inference. The images are first compressed using\ncompression schemes, like JPEG, to reduce storage costs and power for\ntransmitting the captured data. Due to inherent error resilience and\nimperceptibility in images, JPEG can be approximated to reduce the required\ncomputation power and area. This work demonstrates the first end-to-end\napproximation computing-based optimization of JPEG hardware using i) an\napproximate division realized using bit-shift operators to reduce the\ncomplexity of the quantization block, ii) loop perforation, and iii) precision\nscaling on top of a multiplier-less fast DCT architecture to achieve an\nextremely energy-efficient JPEG compression unit which will be a perfect fit\nfor power/bandwidth-limited scenario. Furthermore, a gradient descent-based\nheuristic composed of two conventional approximation strategies, i.e.,\nPrecision Scaling and Loop Perforation, is implemented for tuning the degree of\napproximation to trade off energy consumption with the quality degradation of\nthe decoded image. The entire RTL design is coded in Verilog HDL, synthesized,\nmapped to TSMC 65nm CMOS technology, and simulated using Cadence Spectre\nSimulator under 25$^{\\circ}$\\textbf{C}, TT corner. The approximate division\napproach achieved around $\\textbf{28\\%}$ reduction in the active design area.\nThe heuristic-based approximation technique combined with accelerator\noptimization achieves a significant energy reduction of $\\textbf{36\\%}$ for a\nminimal image quality degradation of $\\textbf{2\\%}$ SAD. Simulation results\nalso show that the proposed architecture consumes 15uW at the DCT and\nquantization stages to compress a colored 480p image at 6fps.\n","authors":["Ming-Che Li","Archisman Ghosh","Shreyas Sen"],"pdf_url":"https://arxiv.org/pdf/2406.16358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16322v1","updated":"2024-06-24T05:15:15Z","published":"2024-06-24T05:15:15Z","title":"Lesion-Aware Cross-Phase Attention Network for Renal Tumor Subtype\n  Classification on Multi-Phase CT Scans","summary":"  Multi-phase computed tomography (CT) has been widely used for the\npreoperative diagnosis of kidney cancer due to its non-invasive nature and\nability to characterize renal lesions. However, since enhancement patterns of\nrenal lesions across CT phases are different even for the same lesion type, the\nvisual assessment by radiologists suffers from inter-observer variability in\nclinical practice. Although deep learning-based approaches have been recently\nexplored for differential diagnosis of kidney cancer, they do not explicitly\nmodel the relationships between CT phases in the network design, limiting the\ndiagnostic performance. In this paper, we propose a novel lesion-aware\ncross-phase attention network (LACPANet) that can effectively capture temporal\ndependencies of renal lesions across CT phases to accurately classify the\nlesions into five major pathological subtypes from time-series multi-phase CT\nimages. We introduce a 3D inter-phase lesion-aware attention mechanism to learn\neffective 3D lesion features that are used to estimate attention weights\ndescribing the inter-phase relations of the enhancement patterns. We also\npresent a multi-scale attention scheme to capture and aggregate temporal\npatterns of lesion features at different spatial scales for further\nimprovement. Extensive experiments on multi-phase CT scans of kidney cancer\npatients from the collected dataset demonstrate that our LACPANet outperforms\nstate-of-the-art approaches in diagnostic accuracy.\n","authors":["Kwang-Hyun Uhm","Seung-Won Jung","Sung-Hoo Hong","Sung-Jea Ko"],"pdf_url":"https://arxiv.org/pdf/2406.16322v1.pdf","comment":"This article has been accepted for publication in Computers in\n  Biology and Medicine"},{"id":"http://arxiv.org/abs/2403.18651v3","updated":"2024-06-24T04:04:35Z","published":"2024-03-27T14:59:19Z","title":"Do High-Performance Image-to-Image Translation Networks Enable the\n  Discovery of Radiomic Features? Application to MRI Synthesis from Ultrasound\n  in Prostate Cancer","summary":"  This study investigates the foundational characteristics of image-to-image\ntranslation networks, specifically examining their suitability and\ntransferability within the context of routine clinical environments, despite\nachieving high levels of performance, as indicated by a Structural Similarity\nIndex (SSIM) exceeding 0.95. The evaluation study was conducted using data from\n794 patients diagnosed with Prostate cancer. To synthesize MRI from Ultrasound\nimages, we employed five widely recognized image to image translation networks\nin medical imaging: 2DPix2Pix, 2DCycleGAN, 3DCycleGAN, 3DUNET, and\n3DAutoEncoder. For quantitative assessment, we report four prevalent evaluation\nmetrics Mean Absolute Error, Mean Square Error, Structural Similarity Index\n(SSIM), and Peak Signal to Noise Ratio. Moreover, a complementary analysis\nemploying Radiomic features (RF) via Spearman correlation coefficient was\nconducted to investigate, for the first time, whether networks achieving high\nperformance, SSIM greater than 0.85, could identify low-level RFs. The RF\nanalysis showed 75 features out of 186 RFs were discovered via just 2DPix2Pix\nalgorithm while half of RFs were lost in the translation process. Finally, a\ndetailed qualitative assessment by five medical doctors indicated a lack of low\nlevel feature discovery in image to image translation tasks.\n","authors":["Mohammad R. Salmanpour","Amin Mousavi","Yixi Xu","William B Weeks","Ilker Hacihaliloglu"],"pdf_url":"https://arxiv.org/pdf/2403.18651v3.pdf","comment":"Submitted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.16297v1","updated":"2024-06-24T03:49:52Z","published":"2024-06-24T03:49:52Z","title":"Priorformer: A UGC-VQA Method with content and distortion priors","summary":"  User Generated Content (UGC) videos are susceptible to complicated and\nvariant degradations and contents, which prevents the existing blind video\nquality assessment (BVQA) models from good performance since the lack of the\nadapability of distortions and contents. To mitigate this, we propose a novel\nprior-augmented perceptual vision transformer (PriorFormer) for the BVQA of\nUGC, which boots its adaptability and representation capability for divergent\ncontents and distortions. Concretely, we introduce two powerful priors, i.e.,\nthe content and distortion priors, by extracting the content and distortion\nembeddings from two pre-trained feature extractors. Then we adopt these two\npowerful embeddings as the adaptive prior tokens, which are transferred to the\nvision transformer backbone jointly with implicit quality features. Based on\nthe above strategy, the proposed PriorFormer achieves state-of-the-art\nperformance on three public UGC VQA datasets including KoNViD-1K, LIVE-VQC and\nYouTube-UGC.\n","authors":["Yajing Pei","Shiyu Huang","Yiting Lu","Xin Li","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2406.16297v1.pdf","comment":"7 pages"}]},"2024-06-23T00:00:00Z":{"Image and Video Processing":[{"id":"http://arxiv.org/abs/2406.16214v1","updated":"2024-06-23T20:45:20Z","published":"2024-06-23T20:45:20Z","title":"Reducing the Sampling Burden of Fourier Sensing with a Non-rectangular\n  Field-of-View","summary":"  With Fourier sensing, it is commonly the case that the field-of-view (FOV),\nthe area of space to be imaged, is known prior to reconstruction. To date,\nreconstruction algorithms have focused on FOVs with simple geometries: a\nrectangle or a hexagon. This yields sampling patterns that are more burdensome\nthan necessary. Due to the reduced area of imaging possible with an arbitrary\n(e.g., non-rectangular) FOV, the number of samples required for a high-quality\nimages is reduced. However, when an arbitrary FOV has been considered, the\nreconstruction algorithm is computationally expensive. In this manuscript, we\npresent a method to reduce the sampling pattern for an arbitrary FOV with an\naccompanying direct (non-iterative) reconstruction algorithm. We also present a\nmethod to decrease the computational cost of the (iterative) model-based\nreconstruction (MBR) algorithm. We present results using MRI data of an ankle,\na pineapple, and a brain.\n","authors":["Nicholas Dwork","Erin K. Englund","Alex J. Barker"],"pdf_url":"https://arxiv.org/pdf/2406.16214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16983v1","updated":"2024-06-23T19:44:00Z","published":"2024-06-23T19:44:00Z","title":"On Instabilities of Unsupervised Denoising Diffusion Models in Magnetic\n  Resonance Imaging Reconstruction","summary":"  Denoising diffusion models offer a promising approach to accelerating\nmagnetic resonance imaging (MRI) and producing diagnostic-level images in an\nunsupervised manner. However, our study demonstrates that even tiny worst-case\npotential perturbations transferred from a surrogate model can cause these\nmodels to generate fake tissue structures that may mislead clinicians. The\ntransferability of such worst-case perturbations indicates that the robustness\nof image reconstruction may be compromised due to MR system imperfections or\nother sources of noise. Moreover, at larger perturbation strengths, diffusion\nmodels exhibit Gaussian noise-like artifacts that are distinct from those\nobserved in supervised models and are more challenging to detect. Our results\nhighlight the vulnerability of current state-of-the-art diffusion-based\nreconstruction models to possible worst-case perturbations and underscore the\nneed for further research to improve their robustness and reliability in\nclinical settings.\n","authors":["Tianyu Han","Sven Nebelung","Firas Khader","Jakob Nikolas Kather","Daniel Truhn"],"pdf_url":"https://arxiv.org/pdf/2406.16983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16189v1","updated":"2024-06-23T18:47:51Z","published":"2024-06-23T18:47:51Z","title":"Fuzzy Attention-based Border Rendering Network for Lung Organ\n  Segmentation","summary":"  Automatic lung organ segmentation on CT images is crucial for lung disease\ndiagnosis. However, the unlimited voxel values and class imbalance of lung\norgans can lead to false-negative/positive and leakage issues in advanced\nmethods. Additionally, some slender lung organs are easily lost during the\nrecycled down/up-sample procedure, e.g., bronchioles & arterioles, causing\nsevere discontinuity issue. Inspired by these, this paper introduces an\neffective lung organ segmentation method called Fuzzy Attention-based Border\nRendering (FABR) network. Since fuzzy logic can handle the uncertainty in\nfeature extraction, hence the fusion of deep networks and fuzzy sets should be\na viable solution for better performance. Meanwhile, unlike prior top-tier\nmethods that operate on all regular dense points, our FABR depicts lung organ\nregions as cube-trees, focusing only on recycle-sampled border vulnerable\npoints, rendering the severely discontinuous, false-negative/positive organ\nregions with a novel Global-Local Cube-tree Fusion (GLCF) module. All\nexperimental results, on four challenging datasets of airway & artery,\ndemonstrate that our method can achieve the favorable performance\nsignificantly.\n","authors":["Sheng Zhang","Yang Nan","Yingying Fang","Shiyi Wang","Xiaodan Xing","Zhifan Gao","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2406.16189v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.16981v1","updated":"2024-06-23T18:41:43Z","published":"2024-06-23T18:41:43Z","title":"Research on Feature Extraction Data Processing System For MRI of Brain\n  Diseases Based on Computer Deep Learning","summary":"  Most of the existing wavelet image processing techniques are carried out in\nthe form of single-scale reconstruction and multiple iterations. However,\nprocessing high-quality fMRI data presents problems such as mixed noise and\nexcessive computation time. This project proposes the use of matrix operations\nby combining mixed noise elimination methods with wavelet analysis to replace\ntraditional iterative algorithms. Functional magnetic resonance imaging (fMRI)\nof the auditory cortex of a single subject is analyzed and compared to the\nwavelet domain signal processing technology based on repeated times and the\nworld's most influential SPM8. Experiments show that this algorithm is the\nfastest in computing time, and its detection effect is comparable to the\ntraditional iterative algorithm. However, this has a higher practical value for\nthe processing of FMRI data. In addition, the wavelet analysis method proposed\nsignal processing to speed up the calculation rate.\n","authors":["Lingxi Xiao","Jinxin Hu","Yutian Yang","Yinqiu Feng","Zichao Li","Zexi Chen"],"pdf_url":"https://arxiv.org/pdf/2406.16981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16150v1","updated":"2024-06-23T16:09:21Z","published":"2024-06-23T16:09:21Z","title":"Intensity Confusion Matters: An Intensity-Distance Guided Loss for\n  Bronchus Segmentation","summary":"  Automatic segmentation of the bronchial tree from CT imaging is important, as\nit provides structural information for disease diagnosis. Despite the merits of\nprevious automatic bronchus segmentation methods, they have paied less\nattention to the issue we term as \\textit{Intensity Confusion}, wherein the\nintensity values of certain background voxels approach those of the foreground\nvoxels within bronchi. Conversely, the intensity values of some foreground\nvoxels are nearly identical to those of background voxels. This proximity in\nintensity values introduces significant challenges to neural network\nmethodologies. To address the issue, we introduce a novel Intensity-Distance\nGuided loss function, which assigns adaptive weights to different image voxels\nfor mining hard samples that cause the intensity confusion. The proposed loss\nestimates the voxel-level hardness of samples, on the basis of the following\nintensity and distance priors. We regard a voxel as a hard sample if it is in:\n(1) the background and has an intensity value close to the bronchus region; (2)\nthe bronchus region and is of higher intensity than most voxels inside the\nbronchus; (3) the background region and at a short distance from the bronchus.\nExtensive experiments not only show the superiority of our method compared with\nthe state-of-the-art methods, but also verify that tackling the intensity\nconfusion issue helps to significantly improve bronchus segmentation. Project\npage: https://github.com/lhaof/ICM.\n","authors":["Haifan Gong","Wenhao Huang","Huan Zhang","Yu Wang","Xiang Wan","Hong Shen","Guanbin Li","Haofeng Li"],"pdf_url":"https://arxiv.org/pdf/2406.16150v1.pdf","comment":"IEEE International Conference on Multimedia & Expo (ICME) 2024"},{"id":"http://arxiv.org/abs/2404.06589v2","updated":"2024-06-23T14:54:11Z","published":"2024-04-09T19:33:05Z","title":"Leveraging Latents for Efficient Thermography Classification and\n  Segmentation","summary":"  Breast cancer is a prominent health concern worldwide, currently being the\nsecondmost common and second-deadliest type of cancer in women. While current\nbreast cancer diagnosis mainly relies on mammography imaging, in recent years\nthe use of thermography for breast cancer imaging has been garnering growing\npopularity. Thermographic imaging relies on infrared cameras to capture\nbody-emitted heat distributions. While these heat signatures have proven useful\nfor computer-vision systems for accurate breast cancer segmentation and\nclassification, prior work often relies on handcrafted feature engineering or\ncomplex architectures, potentially limiting the comparability and applicability\nof these methods. In this work, we present a novel algorithm for both breast\ncancer classification and segmentation. Rather than focusing efforts on manual\nfeature and architecture engineering, our algorithm focuses on leveraging an\ninformative, learned feature space, thus making our solution simpler to use and\nextend to other frameworks and downstream tasks, as well as more applicable to\ndata-scarce settings. Our classification produces SOTA results, while we are\nthe first work to produce segmentation regions studied in this paper.\n","authors":["Tamir Shor","Chaim Baskin","Alex Bronstein"],"pdf_url":"https://arxiv.org/pdf/2404.06589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06148v2","updated":"2024-06-23T13:24:58Z","published":"2022-07-13T12:16:33Z","title":"Multiview Contrastive Learning for Completely Blind Video Quality\n  Assessment of User Generated Content","summary":"  Completely blind video quality assessment (VQA) refers to a class of quality\nassessment methods that do not use any reference videos, human opinion scores\nor training videos from the target database to learn a quality model. The\ndesign of this class of methods is particularly important since it can allow\nfor superior generalization in performance across various datasets. We consider\nthe design of completely blind VQA for user generated content. While several\ndeep feature extraction methods have been considered in supervised and weakly\nsupervised settings, such approaches have not been studied in the context of\ncompletely blind VQA. We bridge this gap by presenting a self-supervised\nmultiview contrastive learning framework to learn spatio-temporal quality\nrepresentations. In particular, we capture the common information between frame\ndifferences and frames by treating them as a pair of views and similarly obtain\nthe shared representations between frame differences and optical flow. The\nresulting features are then compared with a corpus of pristine natural video\npatches to predict the quality of the distorted video. Detailed experiments on\nmultiple camera captured VQA datasets reveal the superior performance of our\nmethod over other features when evaluated without training on human scores.\n","authors":["Shankhanil Mitra","Rajiv Soundararajan"],"pdf_url":"https://arxiv.org/pdf/2207.06148v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16083v1","updated":"2024-06-23T11:28:08Z","published":"2024-06-23T11:28:08Z","title":"Mamba-based Light Field Super-Resolution with Efficient Subspace\n  Scanning","summary":"  Transformer-based methods have demonstrated impressive performance in 4D\nlight field (LF) super-resolution by effectively modeling long-range\nspatial-angular correlations, but their quadratic complexity hinders the\nefficient processing of high resolution 4D inputs, resulting in slow inference\nspeed and high memory cost. As a compromise, most prior work adopts a\npatch-based strategy, which fails to leverage the full information from the\nentire input LFs. The recently proposed selective state-space model, Mamba, has\ngained popularity for its efficient long-range sequence modeling. In this\npaper, we propose a Mamba-based Light Field Super-Resolution method, named\nMLFSR, by designing an efficient subspace scanning strategy. Specifically, we\ntokenize 4D LFs into subspace sequences and conduct bi-directional scanning on\neach subspace. Based on our scanning strategy, we then design the Mamba-based\nGlobal Interaction (MGI) module to capture global information and the local\nSpatial- Angular Modulator (SAM) to complement local details. Additionally, we\nintroduce a Transformer-to-Mamba (T2M) loss to further enhance overall\nperformance. Extensive experiments on public benchmarks demonstrate that MLFSR\nsurpasses CNN-based models and rivals Transformer-based methods in performance\nwhile maintaining higher efficiency. With quicker inference speed and reduced\nmemory demand, MLFSR facilitates full-image processing of high-resolution 4D\nLFs with enhanced performance.\n","authors":["Ruisheng Gao","Zeyu Xiao","Zhiwei Xiong"],"pdf_url":"https://arxiv.org/pdf/2406.16083v1.pdf","comment":"17 pages,7 figures"},{"id":"http://arxiv.org/abs/2406.16074v1","updated":"2024-06-23T10:50:22Z","published":"2024-06-23T10:50:22Z","title":"CAVM: Conditional Autoregressive Vision Model for Contrast-Enhanced\n  Brain Tumor MRI Synthesis","summary":"  Contrast-enhanced magnetic resonance imaging (MRI) is pivotal in the pipeline\nof brain tumor segmentation and analysis. Gadolinium-based contrast agents, as\nthe most commonly used contrast agents, are expensive and may have potential\nside effects, and it is desired to obtain contrast-enhanced brain tumor MRI\nscans without the actual use of contrast agents. Deep learning methods have\nbeen applied to synthesize virtual contrast-enhanced MRI scans from\nnon-contrast images. However, as this synthesis problem is inherently\nill-posed, these methods fall short in producing high-quality results. In this\nwork, we propose Conditional Autoregressive Vision Model (CAVM) for improving\nthe synthesis of contrast-enhanced brain tumor MRI. As the enhancement of image\nintensity grows with a higher dose of contrast agents, we assume that it is\nless challenging to synthesize a virtual image with a lower dose, where the\ndifference between the contrast-enhanced and non-contrast images is smaller.\nThus, CAVM gradually increases the contrast agent dosage and produces\nhigher-dose images based on previous lower-dose ones until the final desired\ndose is achieved. Inspired by the resemblance between the gradual dose increase\nand the Chain-of-Thought approach in natural language processing, CAVM uses an\nautoregressive strategy with a decomposition tokenizer and a decoder.\nSpecifically, the tokenizer is applied to obtain a more compact image\nrepresentation for computational efficiency, and it decomposes the image into\ndose-variant and dose-invariant tokens. Then, a masked self-attention mechanism\nis developed for autoregression that gradually increases the dose of the\nvirtual image based on the dose-variant tokens. Finally, the updated\ndose-variant tokens corresponding to the desired dose are decoded together with\ndose-invariant tokens to produce the final contrast-enhanced MRI.\n","authors":["Lujun Gui","Chuyang Ye","Tianyi Yan"],"pdf_url":"https://arxiv.org/pdf/2406.16074v1.pdf","comment":"The work has been accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2402.16581v2","updated":"2024-06-23T09:32:06Z","published":"2024-02-26T14:04:54Z","title":"Rate Splitting Multiple Access-Enabled Adaptive Panoramic Video Semantic\n  Transmission","summary":"  In this paper, we propose an adaptive panoramic video semantic transmission\n(APVST) framework enabled by rate splitting multiple access (RSMA). The APVST\nframework consists of a semantic transmitter and receiver, utilizing a deep\njoint source-channel coding structure to adaptively extract and encode semantic\nfeatures from panoramic frames. To achieve higher spectral efficiency and\nconserve bandwidth, APVST employs an entropy model and a dimension-adaptive\nmodule to control the transmission rate. Additionally, we take\nweighted-to-spherically-uniform peak signal-to-noise ratio (WS-PSNR) and\nweighted-to-spherically-uniform structural similarity (WS-SSIM) as distortion\nevaluation metrics for panoramic videos and design a weighted self-attention\nmodule for APVST. This module integrates weights and feature maps to enhance\nthe quality of the immersive experience. Considering the overlap in the field\nof view when users watch panoramic videos, we further utilize RSMA to split the\nrequired panoramic video semantic streams into common and private messages for\ntransmission. We propose an RSMA-enabled semantic stream transmission scheme\nand formulate a joint problem of latency and immersive experience quality by\noptimizing the allocation ratios of power, common rate, and channel bandwidth,\naiming to maximize the quality of service (QoS) scores for users. To address\nthe above problem, we propose a deep reinforcement learning algorithm based on\nproximal policy optimization (PPO) with high efficiency to handle dynamically\nchanging environments. Simulation results demonstrate that our proposed APVST\nframework saves up to 20% and 50% of channel bandwidth compared to other\nsemantic and traditional video transmission schemes, respectively. Moreover,\nour study confirms the efficiency of RSMA in panoramic video transmission,\nachieving performance gains of 13% and 20% compared to NOMA and OFDMA.\n","authors":["Haixiao Gao","Mengying Sun","Xiaodong Xu","Shujun Han","Bizhu Wang","Jingxuan Zhang","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.16581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16012v1","updated":"2024-06-23T05:01:51Z","published":"2024-06-23T05:01:51Z","title":"Wound Tissue Segmentation in Diabetic Foot Ulcer Images Using Deep\n  Learning: A Pilot Study","summary":"  Identifying individual tissues, so-called tissue segmentation, in diabetic\nfoot ulcer (DFU) images is a challenging task and little work has been\npublished, largely due to the limited availability of a clinical image dataset.\nTo address this gap, we have created a DFUTissue dataset for the research\ncommunity to evaluate wound tissue segmentation algorithms. The dataset\ncontains 110 images with tissues labeled by wound experts and 600 unlabeled\nimages. Additionally, we conducted a pilot study on segmenting wound\ncharacteristics including fibrin, granulation, and callus using deep learning.\nDue to the limited amount of annotated data, our framework consists of both\nsupervised learning (SL) and semi-supervised learning (SSL) phases. In the SL\nphase, we propose a hybrid model featuring a Mix Transformer (MiT-b3) in the\nencoder and a CNN in the decoder, enhanced by the integration of a parallel\nspatial and channel squeeze-and-excitation (P-scSE) module known for its\nefficacy in improving boundary accuracy. The SSL phase employs a\npseudo-labeling-based approach, iteratively identifying and incorporating\nvaluable unlabeled images to enhance overall segmentation performance.\nComparative evaluations with state-of-the-art methods are conducted for both SL\nand SSL phases. The SL achieves a Dice Similarity Coefficient (DSC) of 84.89%,\nwhich has been improved to 87.64% in the SSL phase. Furthermore, the results\nare benchmarked against two widely used SSL approaches: Generative Adversarial\nNetworks and Cross-Consistency Training. Additionally, our hybrid model\noutperforms the state-of-the-art methods with a 92.99% DSC in performing binary\nsegmentation of DFU wound areas when tested on the Chronic Wound dataset. Codes\nand data are available at https://github.com/uwm-bigdata/DFUTissueSegNet.\n","authors":["Mrinal Kanti Dhar","Chuanbo Wang","Yash Patel","Taiyu Zhang","Jeffrey Niezgoda","Sandeep Gopalakrishnan","Keke Chen","Zeyun Yu"],"pdf_url":"https://arxiv.org/pdf/2406.16012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15979v1","updated":"2024-06-23T01:32:53Z","published":"2024-06-23T01:32:53Z","title":"Deep Learning Segmentation of Ascites on Abdominal CT Scans for\n  Automatic Volume Quantification","summary":"  Purpose: To evaluate the performance of an automated deep learning method in\ndetecting ascites and subsequently quantifying its volume in patients with\nliver cirrhosis and ovarian cancer.\n  Materials and Methods: This retrospective study included contrast-enhanced\nand non-contrast abdominal-pelvic CT scans of patients with cirrhotic ascites\nand patients with ovarian cancer from two institutions, National Institutes of\nHealth (NIH) and University of Wisconsin (UofW). The model, trained on The\nCancer Genome Atlas Ovarian Cancer dataset (mean age, 60 years +/- 11 [s.d.];\n143 female), was tested on two internal (NIH-LC and NIH-OV) and one external\ndataset (UofW-LC). Its performance was measured by the Dice coefficient,\nstandard deviations, and 95% confidence intervals, focusing on ascites volume\nin the peritoneal cavity.\n  Results: On NIH-LC (25 patients; mean age, 59 years +/- 14 [s.d.]; 14 male)\nand NIH-OV (166 patients; mean age, 65 years +/- 9 [s.d.]; all female), the\nmodel achieved Dice scores of 0.855 +/- 0.061 (CI: 0.831-0.878) and 0.826 +/-\n0.153 (CI: 0.764-0.887), with median volume estimation errors of 19.6% (IQR:\n13.2-29.0) and 5.3% (IQR: 2.4-9.7) respectively. On UofW-LC (124 patients; mean\nage, 46 years +/- 12 [s.d.]; 73 female), the model had a Dice score of 0.830\n+/- 0.107 (CI: 0.798-0.863) and median volume estimation error of 9.7% (IQR:\n4.5-15.1). The model showed strong agreement with expert assessments, with r^2\nvalues of 0.79, 0.98, and 0.97 across the test sets.\n  Conclusion: The proposed deep learning method performed well in segmenting\nand quantifying the volume of ascites in concordance with expert radiologist\nassessments.\n","authors":["Benjamin Hou","Sung-Won Lee","Jung-Min Lee","Christopher Koh","Jing Xiao","Perry J. Pickhardt","Ronald M. Summers"],"pdf_url":"https://arxiv.org/pdf/2406.15979v1.pdf","comment":null}]}}